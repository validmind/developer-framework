{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customize test outputs using output templates\n",
        "\n",
        "Run individual tests and tests for experimentation and when exploring and building output templates to create custom results.\n",
        "\n",
        "The ValidMind Developer Framework provides a suite of tests to help you evaluate the performance of your machine learning models. The out-of-the-box results are designed to be informative and easy to understand, but you may want to customize the look and feel of the results to better suit your needs. This might include things like removing or adding columns from the results, changing the formatting or structure of a table, or adding entirely new tables to the results. Output templates allow you to do all of these things and more. \n",
        "\n",
        "Output templates currently can create and cutomize tables. They are written in HTML and use the Jinja2 templating language.\n",
        "\n",
        "As part of the notebook, you will build on the simple quickstart_customer_churn notebook and learn how to:\n",
        "\n",
        "- Create an output template to customize the look and feel of the results produced by the ValidMind tests\n",
        "- Use output templates in your code to create one-off cusomized results\n",
        "- Add output templates to your documentation templates to save and share your customizations\n",
        "\n",
        "This interactive notebook uses the [Bank Customer Churn Prediction](https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data) sample dataset from Kaggle to train a simple classification model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contents\n",
        "- [About ValidMind](#toc1_)    \n",
        "  - [Before you begin](#toc1_1_)    \n",
        "  - [New to ValidMind?](#toc1_2_)    \n",
        "  - [Key Concepts](#toc1_3_)    \n",
        "  - [How documentation template work with output templates](#toc1_4_)    \n",
        "    - [Example documentation template](#toc1_4_1_)    \n",
        "- [Install the client library](#toc2_)    \n",
        "- [Initialize the client library](#toc3_)    \n",
        "- [Initialize the Python environment](#toc4_)    \n",
        "- [Load the sample dataset](#toc5_)    \n",
        "- [Document the model](#toc6_)    \n",
        "  - [Prepocess the raw dataset](#toc6_1_)    \n",
        "  - [Initialize the ValidMind datasets](#toc6_2_)    \n",
        "  - [Initialize a model object](#toc6_3_)    \n",
        "  - [Assign predictions to the datasets](#toc6_4_)    \n",
        "  - [Run individual tests and customize the results](#toc6_5_)    \n",
        "  - [Run the full suite of tests with output templates](#toc6_6_)    \n",
        "    - [Add the output template to the documentation template](#toc6_6_1_)    \n",
        "    - [Run the full suite of tests](#toc6_6_2_)    \n",
        "- [Next steps](#toc7_)    \n",
        "  - [Work with your model documentation](#toc7_1_)    \n",
        "  - [Discover more learning resources](#toc7_2_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=2\n",
        "\tmaxLevel=4\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc1_'></a>\n",
        "\n",
        "## About ValidMind\n",
        "\n",
        "ValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\n",
        "\n",
        "You use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
        "\n",
        "<a id='toc1_1_'></a>\n",
        "\n",
        "### Before you begin\n",
        "\n",
        "This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language. \n",
        "\n",
        "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n",
        "\n",
        "<a id='toc1_2_'></a>\n",
        "\n",
        "### New to ValidMind?\n",
        "\n",
        "If you haven't already seen our [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/developer/get-started-developer-framework.html), we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"background-color: #f7e4ee; color: #222425; border: 1px solid #222425;\">For access to all features available in this notebook, create a free ValidMind account.\n",
        "\n",
        "Signing up is FREE — <a href=\"https://app.prod.validmind.ai\"><b>Sign up now</b></a></div>\n",
        "\n",
        "<a id='toc1_3_'></a>\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- **Output Templates**: Customizable HTML templates that define the look and feel of the results produced by the ValidMind tests. They are written in HTML and use the Jinja2 templating language.\n",
        "- **Jinja2 Templating Language**: A powerful templating language for Python that allows you to embed expressions and control structures in your HTML templates.\n",
        "- **Customizing Tables**: Output templates allow you to customize the look and feel of the tables produced by the ValidMind tests. This includes things like adding or removing columns, changing the formatting or structure of the table, and adding entirely new tables to the results.\n",
        "- **Documentation Templates**: Documentation templates are covered in the quickstart notebook and are the base for all model documentation. They are written in YAML and define the entire structure and content of a model's documentation. Output templates are not part of the documentation template, but they are defined in and shared via a field in the documentation template.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc1_4_'></a>\n",
        "\n",
        "### How documentation template work with output templates\n",
        "\n",
        "Below is a section of the standard Binary Classification Documentation Template that comes pre-installed with the ValidMind Developer Framework. \n",
        "\n",
        "The top-level `model_evaluation` section contains a list of sub sections which contain content blocks. These blocks can be either editable text blocks or test-driven blocks where the `content_id` identifies the test within the Developer Framework whose results will be displayed in that block. Now the key thing here is that each of these tests produces a specific output that is not directly editable from the ValidMind platform. This is where output templates come in, both figuraively and literally. They can be added as an optional field in the content block and will use the raw test output data in an HTML template to produce a custom table that can be displayed in the documentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc1_4_1_'></a>\n",
        "\n",
        "#### Example documentation template\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```yaml\n",
        "- id: model_development\n",
        "  title: Model Development\n",
        "  index_only: true\n",
        "  sections:\n",
        "    - id: model_training\n",
        "      title: Model Training\n",
        "      guidelines:\n",
        "        - Describe the model training process, including the algorithm used, any\n",
        "          hyperparameters or settings, and the optimization techniques employed\n",
        "          to minimize the loss function or maximize the objective function.\n",
        "        - ... (additional guidelines)\n",
        "      contents:\n",
        "        - content_type: metric\n",
        "          content_id: validmind.model_validation.ModelMetadata\n",
        "        - ... (additional content blocks)\n",
        "      parent_section: model_development\n",
        "    - id: model_evaluation\n",
        "      title: Model Evaluation\n",
        "      guidelines:\n",
        "        - Describe the process used to evaluate the model's performance on a\n",
        "          test or validation dataset that was not used during training, to\n",
        "          assess its generalizability and robustness.\n",
        "        - ... (additional guidelines)\n",
        "      contents:\n",
        "        - content_type: metric\n",
        "          content_id: validmind.model_validation.sklearn.ConfusionMatrix\n",
        "        - content_type: metric\n",
        "          content_id: validmind.model_validation.sklearn.ClassifierPerformance\n",
        "        - ... (additional content blocks)\n",
        "      parent_section: model_development\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above example, the `validmind.model_validation.sklearn.ClassifierPerformance` produces two tables like this:\n",
        "\n",
        "![Alt text](<Screenshot 2024-02-15 at 2.48.03 PM.png>)\n",
        "\n",
        "But with output templates, you can customize the look and feel of the output to produce a much simpler/clearer version like this:\n",
        "\n",
        "![Alt text](<Screenshot 2024-02-15 at 4.51.56 PM.png>)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How this is accomplished is with the following output template:\n",
        "\n",
        "```yaml\n",
        "- content_type: metric\n",
        "  content_id: validmind.model_validation.sklearn.ClassifierPerformance:with_template\n",
        "  output_template: |\n",
        "    <table>\n",
        "        <thead>\n",
        "            <tr>\n",
        "                <th>Accuracy</th>\n",
        "                <th>Precision</th>\n",
        "                <th>Recall</th>\n",
        "                <th>F1 Score</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "            <tr>\n",
        "                <td>{{ value[\"accuracy\"] }}</td>\n",
        "                <td>{{ value[\"weighted avg\"][\"precision\"] }}</td>\n",
        "                <td>{{ value[\"weighted avg\"][\"recall\"] }}</td>\n",
        "                <td>{{ value[\"weighted avg\"][\"f1-score\"] }}</td>\n",
        "            </tr>\n",
        "        </tbody>\n",
        "    </table>\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the output template is a simple HTML table that uses the Jinja2 templating language to embed expressions that reference the raw test output data. The `{{ value[\"accuracy\"] }}` expression, for example, references the `accuracy` key in the raw test output data. This is how you can customize the look and feel of the results produced by the ValidMind tests.\n",
        "\n",
        "Now that you understand the basics of output templates, the following sections will guide you through the process of creating and using them in your code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc2_'></a>\n",
        "\n",
        "## Install the client library\n",
        "\n",
        "The client library provides Python support for the ValidMind Developer Framework. To install it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc3_'></a>\n",
        "\n",
        "## Initialize the client library\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "Get your code snippet:\n",
        "\n",
        "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
        "\n",
        "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
        "\n",
        "3. Enter the model details and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/register-models-in-inventory.html))\n",
        "\n",
        "   For example, to register a model for use with this notebook, select:\n",
        "\n",
        "   - Documentation template: `Binary classification`\n",
        "   - Use case: `Marketing/Sales - Attrition/Churn Management`\n",
        "\n",
        "   You can fill in other options according to your preference.\n",
        "\n",
        "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
        "\n",
        "Next, replace this placeholder with your own code snippet:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace with your code snippet\n",
        "\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    project=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc4_'></a>\n",
        "\n",
        "## Initialize the Python environment\n",
        "\n",
        "Next, let's import the necessary libraries and set up your Python environment for data analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc5_'></a>\n",
        "\n",
        "## Load the sample dataset\n",
        "\n",
        "The sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), a two-dimensional tabular data structure that makes use of rows and columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the sample dataset from the library\n",
        "\n",
        "from validmind.datasets.classification import customer_churn as demo_dataset\n",
        "\n",
        "print(\n",
        "    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}' \\n\\t• Class labels: {demo_dataset.class_labels}\"\n",
        ")\n",
        "\n",
        "raw_df = demo_dataset.load_data()\n",
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6_'></a>\n",
        "\n",
        "## Document the model\n",
        "\n",
        "As part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6_1_'></a>\n",
        "\n",
        "### Prepocess the raw dataset\n",
        "\n",
        "Preprocessing performs a number of operations to get ready for the subsequent steps:\n",
        "\n",
        "- Preprocess the data: Splits the DataFrame (`df`) into multiple datasets (`train_df`, `validation_df`, and `test_df`) using `demo_dataset.preprocess` to simplify preprocessing.\n",
        "- Separate features and targets: Drops the target column to create feature sets (`x_train`, `x_val`) and target sets (`y_train`, `y_val`).\n",
        "- Initialize XGBoost classifier: Creates an `XGBClassifier` object with early stopping rounds set to 10.\n",
        "- Set evaluation metrics: Specifies metrics for model evaluation as \"error,\" \"logloss,\" and \"auc.\"\n",
        "- Fit the model: Trains the model on `x_train` and `y_train` using the validation set `(x_val, y_val)`. Verbose output is disabled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, validation_df, test_df = demo_dataset.preprocess(raw_df)\n",
        "\n",
        "x_train = train_df.drop(demo_dataset.target_column, axis=1)\n",
        "y_train = train_df[demo_dataset.target_column]\n",
        "x_val = validation_df.drop(demo_dataset.target_column, axis=1)\n",
        "y_val = validation_df[demo_dataset.target_column]\n",
        "\n",
        "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
        "model.set_params(\n",
        "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
        ")\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    eval_set=[(x_val, y_val)],\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6_2_'></a>\n",
        "\n",
        "### Initialize the ValidMind datasets\n",
        "\n",
        "Before you can run tests, you must first initialize a ValidMind dataset object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\n",
        "\n",
        "This function takes a number of arguments:\n",
        "\n",
        "- `dataset` — the raw dataset that you want to provide as input to tests\n",
        "- `input_id` - a unique identifier that allows tracking what inputs are used when running each individual test\n",
        "- `target_column` — a required argument if tests require access to true values. This is the name of the target column in the dataset\n",
        "- `class_labels` — an optional value to map predicted classes to class labels\n",
        "\n",
        "With all datasets ready, you can now initialize the raw, training and test datasets (`raw_df`, `train_df` and `test_df`) created earlier into their own dataset objects using [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import validmind as vm\n",
        "\n",
        "vm_raw_dataset = vm.init_dataset(\n",
        "    dataset=raw_df,\n",
        "    input_id=\"raw_dataset\",\n",
        "    target_column=demo_dataset.target_column,\n",
        "    class_labels=demo_dataset.class_labels,\n",
        ")\n",
        "\n",
        "vm_train_ds = vm.init_dataset(\n",
        "    dataset=train_df, input_id=\"train_dataset\", target_column=demo_dataset.target_column\n",
        ")\n",
        "\n",
        "vm_test_ds = vm.init_dataset(\n",
        "    dataset=test_df, input_id=\"test_dataset\", target_column=demo_dataset.target_column\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6_3_'></a>\n",
        "\n",
        "### Initialize a model object\n",
        "\n",
        "Additionally, you need to initialize a ValidMind model object (`vm_model`) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm_model = vm.init_model(\n",
        "    model,\n",
        "    input_id=\"model\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6_4_'></a>\n",
        "\n",
        "### Assign predictions to the datasets\n",
        "\n",
        "We can now use the `assign_predictions()` method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm_train_ds.assign_predictions(model=vm_model)\n",
        "vm_test_ds.assign_predictions(model=vm_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6_5_'></a>\n",
        "\n",
        "### Run individual tests and customize the results\n",
        "\n",
        "Instead of running the full suite of tests, you can run individual tests. This is useful for experimentation and when exploring and building output templates to create custom results. Lets go ahead and run a single test, `ClassifierInSamplePerformance`, and see how we can create fully customized results from the output using output templates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.tests import run_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's run the test as normal and see the standard output:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = run_test(\n",
        "    test_id=\"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
        "    inputs={\n",
        "        \"dataset\": vm_train_ds,\n",
        "        \"model\": vm_model,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also take a look at the result object that is returned when running the test and see how we can grab the raw metric value from it to start developing our output template:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "print(\"In Sample Performance Raw Value:\")\n",
        "print(json.dumps(result.metric.value, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the raw `value` object that will get passed into the output template and accessible just with the `value` variable name. Now let's go ahead and create a simple output template like in the example and then see how we can test it directly against the result object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_template = \"\"\"\n",
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th>Accuracy</th>\n",
        "            <th>Precision</th>\n",
        "            <th>Recall</th>\n",
        "            <th>F1 Score</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td>{{ value[\"accuracy\"] }}</td>\n",
        "            <td>{{ value[\"weighted avg\"][\"precision\"] }}</td>\n",
        "            <td>{{ value[\"weighted avg\"][\"recall\"] | number }}</td>\n",
        "            <td>{{ value[\"weighted avg\"][\"f1-score\"] | number }}</td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\"\"\"\n",
        "# specifically notice how the values from the result are being accessed inside the template\n",
        "# also notice that we can use filters to format the values e.g. `| number` to format the number to 4 decimal places"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we can immediately re-render while trying different output templates\n",
        "result.render(output_template=output_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And, there you go, you have successfully created and used a custom output template. Try making some changes to the template html and see how it affects the output. You can also add more complex logic and control structures to the template using the Jinja2 templating language [here](https://jinja.palletsprojects.com/en/2.10.x/).\n",
        "\n",
        "Now that you have a working output template, it can also be passed right into the `run_test` function to produce the same results as before:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = run_test(\n",
        "    test_id=\"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
        "    inputs={\n",
        "        \"dataset\": vm_train_ds,\n",
        "        \"model\": vm_model,\n",
        "    },\n",
        "    output_template=output_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome! So you have seen how to create and use output templates when running individual tests. In a real-world scenario though, you would want to add the output template to the documentation template so that it can live there as a permanent customization. This is what we will cover next.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6_6_'></a>\n",
        "\n",
        "### Run the full suite of tests with output templates\n",
        "\n",
        "Now that you've seen how to run an individual test and customize the output, let's see how you can apply that concept to model documentation by adding the output template to the documentation template and running the full suite of tests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6_6_1_'></a>\n",
        "\n",
        "#### Add the output template to the documentation template\n",
        "\n",
        "Head to the ValidMind platform UI to [customize your documentation template](https://docs.validmind.ai/guide/model-documentation/customize-documentation-templates.html). \n",
        "\n",
        "- Find the Binary Classification Template that is used by the Customer Churn model\n",
        "- Add the following content block below the existing `validmind.model_validation.sklearn.ClassifierPerformance` test:\n",
        "\n",
        "```yaml\n",
        "- content_type: metric\n",
        "  content_id: validmind.model_validation.sklearn.ClassifierPerformance:with_template\n",
        "  output_template: |\n",
        "    <table>\n",
        "        <thead>\n",
        "            <tr>\n",
        "                <th>Accuracy</th>\n",
        "                <th>Precision</th>\n",
        "                <th>Recall</th>\n",
        "                <th>F1 Score</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "            <tr>\n",
        "                <td>{{ value[\"accuracy\"] }}</td>\n",
        "                <td>{{ value[\"weighted avg\"][\"precision\"] }}</td>\n",
        "                <td>{{ value[\"weighted avg\"][\"recall\"] }}</td>\n",
        "                <td>{{ value[\"weighted avg\"][\"f1-score\"] }}</td>\n",
        "            </tr>\n",
        "        </tbody>\n",
        "    </table>\n",
        "```\n",
        "\n",
        "This will add a second version of the `ClassifierPerformance` test so we can compare the standard output with the custom output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6_6_2_'></a>\n",
        "\n",
        "#### Run the full suite of tests\n",
        "\n",
        "Now that you've added the output template to the documentation template, you can run the following code cells to initialize the client which retrieves the template. Then you can run the full suite of tests to see the custom output in the documentation and on the ValidMind UI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_suite = vm.run_documentation_tests(\n",
        "    section=[\"model_development\"],\n",
        "    inputs={\n",
        "        \"dataset\": vm_test_ds,\n",
        "        \"datasets\": (vm_train_ds, vm_test_ds),\n",
        "        \"model\": vm_model,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7_'></a>\n",
        "\n",
        "## Next steps\n",
        "\n",
        "You can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n",
        "\n",
        "<a id='toc7_1_'></a>\n",
        "\n",
        "### Work with your model documentation\n",
        "\n",
        "1. From the [**Model Inventory**](https://app.prod.validmind.ai/model-inventory) in the ValidMind Platform UI, go to the model you registered earlier.\n",
        "\n",
        "2. Click and expand the **Model Development** section.\n",
        "\n",
        "What you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it's ready. [Learn more ...](https://docs.validmind.ai/guide/model-documentation/working-with-model-documentation.html)\n",
        "\n",
        "<a id='toc7_2_'></a>\n",
        "\n",
        "### Discover more learning resources\n",
        "\n",
        "We offer many interactive notebooks to help you document models:\n",
        "\n",
        "- [Run tests & test suites](https://docs.validmind.ai/developer/model-testing/testing-overview.html)\n",
        "- [Code samples](https://docs.validmind.ai/developer/samples-jupyter-notebooks.html)\n",
        "\n",
        "Or, visit our [documentation](https://docs.validmind.ai/) to learn more about ValidMind."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
