{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Validation for Large Language Models (LLMs)\n",
        "\n",
        "This notebook guides model developers through using ValidMind for running and documenting prompt validation tests for a large language model (LLM) specialized in sentiment analysis for financial news. It shows you how to set up the ValidMind Developer Framework, initialize the client library, and use a specific prompt template for analyzing the sentiment of given sentences. The prompt validation covers the initialization of a test dataset and the creation of a foundational model using ValidMind's framework, followed by the execution of a test suite specifically designed for prompt validation. The notebook also includes example data to test the model's ability to correctly identify sentiment as positive, negative, or neutral."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ValidMind at a glance\n",
        "\n",
        "ValidMind's platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
        "\n",
        "If this is your first time trying out ValidMind, we recommend going through the following resources first:\n",
        "\n",
        "- [Get started](https://docs.validmind.ai/guide/get-started.html) — The basics, including key concepts, and how our products work\n",
        "- [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/guide/get-started-developer-framework.html) —  The path for developers, more code samples, and our developer reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you begin\n",
        "\n",
        "::: {.callout-tip}\n",
        "### New to ValidMind? \n",
        "For access to all features available in this notebook, create a free ValidMind account. \n",
        "\n",
        "Signing up is FREE — [**Sign up now**](https://app.prod.validmind.ai)\n",
        ":::\n",
        "\n",
        "\n",
        "This notebook requires an OpenAI API secret key to run. If you don't have one, visit [API keys](https://platform.openai.com/account/api-keys) on OpenAI's site to create a new key for yourself. Note that API usage charges may apply.\n",
        "\n",
        "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install the client library\n",
        "\n",
        "The client library provides Python support for the ValidMind Developer Framework. To install it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the client library\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "Get your code snippet:\n",
        "\n",
        "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
        "\n",
        "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
        "\n",
        "3. Enter the model details, making sure to select **LLM-based Text Classification** as the template and **Marketing/Sales - Analytics** as the use case, and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
        "\n",
        "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
        "\n",
        "Next, replace this placeholder with your own code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Replace with your code snippet\n",
        "\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    project=\"...\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preview the documentation template\n",
        "\n",
        "A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\n",
        "\n",
        "You will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the `vm.preview_template()` function from the ValidMind library and note the empty sections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm.preview_template()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get ready to run the analysis\n",
        "\n",
        "Import the ValidMind `FoundationModel` and `Prompt` classes needed for the sentiment analysis later on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.models import FoundationModel, Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check your access to the OpenAI API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "    raise Exception(\"OPENAI_API_KEY not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "model = OpenAI()\n",
        "\n",
        "\n",
        "def call_model(prompt):\n",
        "    return model.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "    ).choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the prompt guidelines for the sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an AI with expertise in sentiment analysis, particularly in the context of financial news.\n",
        "Your task is to analyze the sentiment of a specific sentence provided below.\n",
        "Before proceeding, take a moment to understand the context and nuances of the financial terminology used in the sentence.\n",
        "\n",
        "Sentence to Analyze:\n",
        "```\n",
        "{Sentence}\n",
        "```\n",
        "\n",
        "Please respond with the sentiment of the sentence denoted by one of either 'positive', 'negative', or 'neutral'.\n",
        "Please respond only with the sentiment enum value. Do not include any other text in your response.\n",
        "\n",
        "Note: Ensure that your analysis is based on the content of the sentence and not on external information or assumptions.\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_variables = [\"Sentence\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get your sample dataset ready for analysis\n",
        "\n",
        "To perform the sentiment analysis for financial news we're going to load a local copy of this dataset: https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news.\n",
        "\n",
        "This dataset contains two columns, `Sentiment` and `Sentence`. The sentiment can be `negative`, `neutral` or `positive`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('./datasets/sentiments.csv')\n",
        "\n",
        "df_test = df[:10].reset_index(drop=True)\n",
        "df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform the prompt validation\n",
        "\n",
        "First, use the ValidMind Developer Framework to initialize the dataset and model objects necessary for documentation. The ValidMind `predict_fn` function allows the model to be tested and evaluated in a standardized manner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm_test_ds = vm.init_dataset(\n",
        "    dataset=df_test,\n",
        "    input_id=\"test_dataset\",\n",
        "    text_column=\"Sentence\",\n",
        "    target_column=\"Sentiment\",\n",
        ")\n",
        "\n",
        "vm_model = FoundationModel(\n",
        "    predict_fn=call_model,\n",
        "    prompt=Prompt(\n",
        "        template=prompt_template,\n",
        "        variables=prompt_variables,\n",
        "    ),\n",
        "    test_ds=vm_test_ds,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, use the ValidMind Developer Framework to run validation tests on the model. These tests evaluate various aspects of the prompts, including bias, clarity, conciseness, delimitation, negative instruction, and specificity. \n",
        "\n",
        "Each test is explained in detail, highlighting its purpose, test mechanism, and the importance of the specific aspect being evaluated. The tests are graded on a scale from 1 to 10, with a predetermined threshold, and the explanations for each test include a score, threshold, and a pass/fail determination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_suite_results = vm.run_test_suite(\n",
        "    \"prompt_validation\",\n",
        "    inputs={\"model\": vm_model}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, most of the tests pass but the test for _conciseness_ needs further attention, as it fails the threshold. This test is designed to evaluate the brevity and succinctness of prompts provided to a large language model (LLM).\n",
        "\n",
        "The test matters, because a concise prompt strikes a balance between offering clear instructions and eliminating redundant or unnecessary information, ensuring that the LLM receives relevant input without being overwhelmed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "You can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation right in the ValidMind Platform UI: \n",
        "\n",
        "1. In the [Platform UI](https://app.prod.validmind.ai), go to the **Documentation** page for the model you registered earlier.\n",
        "\n",
        "2. Expand **3. Model Development** > **3.2. Prompt Evaluation**.\n",
        "\n",
        "What you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your model documentation that still need to be completed. \n",
        "\n",
        "If you want to learn more about where you are in the model documentation process, take a look at [How do I use the framework?](https://docs.validmind.ai/guide/get-started-developer-framework.html#how-do-i-use-the-framework)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Dev Framework 3.9.16",
      "language": "python",
      "name": "dev-framework-3.9"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
