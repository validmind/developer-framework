{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integrate external test providers\n",
        "\n",
        "Register a custom test provider with the Validmind Developer Framework to run your own tests.\n",
        "\n",
        "The ValidMind Developer framework offers the ability to extend the built-in library of tests (metrics) with custom tests. If you've followed along in the [implementing_custom_tests.ipynb](./implementing_custom_tests.ipynb), you will be familiar with the process of creating custom tests and running them for use in your model documentation. In that notebook, the tests were defined inline, as a notebook cell, using the `validmind.metric` decorator. This works very well when you just want to create one-off, ad-hoc tests or for experimenting. But for more complex, reusable and shareable tests, it is recommended to use a more structured approach.\n",
        "\n",
        "This is where the concept of External Test Providers come in. A test \"Provider\" is a Python class that gets registered with the ValidMind Framework and loads tests based on a test ID, for example `my_test_provider.my_test_id`. The built-in suite of tests that ValidMind offers is technically its own test provider. You can use one the built-in test provider offered by ValidMind (`validmind.tests.test_providers.LocalTestProvider`) or you can create your own. More than likely, you'll want to use the `LocalTestProvider` to add a directory of custom tests but there's flexibility to be able to load tests from any source.\n",
        "\n",
        "In this notebook, we'll take you through the process of creating a folder of custom tests from existing, inline tests. We'll then show you how to defined and register a `LocalTestProvider` that points to that folder. Finally, we'll show how the test ID works when using test providers and how you can run and use custom tests from your provider."    
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contents\n",
        "- [About ValidMind](#toc1_)    \n",
        "  - [Before you begin](#toc1_1_)    \n",
        "  - [New to ValidMind?](#toc1_2_)    \n",
        "  - [Key concepts](#toc1_3_)    \n",
        "- [Install the client library](#toc2_)    \n",
        "- [Update the customer churn template template](#toc3_)    \n",
        "- [Initialize the client library](#toc4_)    \n",
        "  - [Preview the documentation template and validate that it contains the new test blocks](#toc4_1_)    \n",
        "  - [Register external test providers](#toc4_2_)    \n",
        "  - [Verify that `preview_template()` now loads the tests from the test providers](#toc4_3_)    \n",
        "  - [Running the template](#toc4_4_)    \n",
        "- [Next steps](#toc5_)    \n",
        "  - [Work with your model documentation](#toc5_1_)    \n",
        "  - [Discover more learning resources](#toc5_2_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=2\n",
        "\tmaxLevel=4\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc1_'></a>\n",
        "\n",
        "## About ValidMind\n",
        "\n",
        "ValidMind is a platform for managing model risk, including risk associated with AI and statistical models. You use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
        "\n",
        "<a id='toc1_1_'></a>\n",
        "\n",
        "### Before you begin\n",
        "\n",
        "This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language. \n",
        "\n",
        "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n",
        "\n",
        "<a id='toc1_2_'></a>\n",
        "\n",
        "### New to ValidMind?\n",
        "\n",
        "If you haven't already seen our [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/guide/get-started-developer-framework.html), we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n",
        "\n",
        "::: {.callout-tip}\n",
        "\n",
        "For access to all features available in this notebook, create a free ValidMind account.\n",
        "\n",
        "Signing up is FREE — [**Sign up now!**](https://app.prod.validmind.ai)\n",
        "\n",
        ":::\n",
        "\n",
        "<a id='toc1_3_'></a>\n",
        "\n",
        "### Key concepts\n",
        "\n",
        "**Model documentation**: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n",
        "\n",
        "**Documentation template**: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\n",
        "\n",
        "**Tests**: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\n",
        "\n",
        "**Metrics**: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\n",
        "\n",
        "**Custom metrics**: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\n",
        "\n",
        "**Inputs**: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n",
        "\n",
        "  - **model**: A single model that has been initialized in ValidMind with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model).\n",
        "  - **dataset**: Single dataset that has been initialized in ValidMind with [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset).\n",
        "  - **models**: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\n",
        "  - **datasets**: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this [example](https://docs.validmind.ai/notebooks/how_to/run_tests_that_require_multiple_datasets.html) for more information.\n",
        "\n",
        "**Parameters**: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\n",
        "\n",
        "**Outputs**: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\n",
        "\n",
        "**Test suites**: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\n",
        "\n",
        "Example: the [`classifier_full_suite`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html#ClassifierFullSuite) test suite runs tests from the [`tabular_dataset`](https://docs.validmind.ai/validmind/validmind/test_suites/tabular_datasets.html) and [`classifier`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html) test suites to fully document the data and model sections for binary classification model use-cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc2_'></a>\n",
        "\n",
        "## Install the client library\n",
        "\n",
        "The client library provides Python support for the ValidMind Developer Framework. To install it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc3_'></a>\n",
        "\n",
        "## Update the customer churn template template\n",
        "\n",
        "First, let's edit the **Binary classification** template and register test blocks for the demo test providers we will implement below.\n",
        "\n",
        "- Go to **Settings > Templates** and click on the **Binary classification** template. Let's add a new top level section called `test_providers_demo` with some test driven content blocks like below:\n",
        "\n",
        "```yaml\n",
        "- id: test_providers_demo\n",
        "  title: Test providers demo\n",
        "  contents:\n",
        "    - content_type: metric\n",
        "      content_id: my_local_provider.tests.MyCustomTest\n",
        "    - content_type: metric\n",
        "      content_id: my_inline_provider.tests.MyCustomTest\n",
        "```\n",
        "\n",
        "- Click on **Prepare new version**, provide some version notes and click con **Save new version** to save a new version of this template\n",
        "- Now we need to swap our model documentation to this new version of the template. Follow the steps on this guide to swap the template of our customer churn model: https://docs.validmind.ai/guide/swap-documentation-templates.html\n",
        "\n",
        "In the sections below we provide more context on how these `content_id`s above get mapped to the actual test providers and tests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc4_'></a>\n",
        "\n",
        "## Initialize the client library\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "Get your code snippet:\n",
        "\n",
        "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
        "\n",
        "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
        "\n",
        "3. Enter the model details, making sure to select the `Attrition/Churn Management` use case, and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
        "\n",
        "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
        "\n",
        "Next, replace this placeholder with your own code snippet:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace with your code snippet\n",
        "\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    project=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc4_1_'></a>\n",
        "\n",
        "### Preview the documentation template and validate that it contains the new test blocks\n",
        "\n",
        "We should see two custom content blocks in the template whose IDs are under the namespaces registered below (`my_inline_provider` and `my_local_provider`).\n",
        "\n",
        "::: {.callout-tip}\n",
        "\n",
        "`preview_template()` will show an error when loading the new tests since we haven't registered our new test providers yet. This is expected.\n",
        "\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm.preview_template()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc4_2_'></a>\n",
        "\n",
        "### Register external test providers\n",
        "\n",
        "We now instantiate and register test provider classes that include the tests we included in the template. Instantiating creates a new context based on a pre-existing model or scheme. We will build an inline test provider that allows creating tests directly in the notebook and a local filesystem test provider that allows loading tests from a local folder.\n",
        "\n",
        "For the local filesystem provider, we just need to specify the root folder under which the provider class will look for tests. For this demo, it is the `./tests/` directory.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Import the Local File System Test Provider from the `validmind.tests` module**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.tests import LocalTestProvider"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Create an inline TestProvider Class that just returns a single test**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "class MySecondCustomTest(vm.vm_models.Metric):\n",
        "    # The test name should match the content ID on the template\n",
        "    name = \"my_inline_provider.tests.MyCustomTest\"\n",
        "\n",
        "    def description(self):\n",
        "        return \"This is a custom test from an external test provider.\"\n",
        "\n",
        "    def run(self):\n",
        "        return self.cache_results([{\"foo\": \"bar\"}])\n",
        "\n",
        "    def summary(self, results):\n",
        "        return vm.vm_models.ResultSummary(\n",
        "            results=[\n",
        "                vm.vm_models.ResultTable(\n",
        "                    data=pd.DataFrame(results),\n",
        "                    metadata=vm.vm_models.ResultTableMetadata(\n",
        "                        title=\"Results from Test Provider Inside Notebook\"\n",
        "                    ),\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "class TestProviderInline:\n",
        "    def load_test(self, test_id):\n",
        "        # ignore the test_id and just return the single test above\n",
        "        return MySecondCustomTest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instantiate the test provider\n",
        "inline_test_provider = TestProviderInline()\n",
        "local_test_provider = LocalTestProvider(root_folder=\".\")\n",
        "\n",
        "# register the test providers\n",
        "vm.tests.register_test_provider(\n",
        "    namespace=\"my_inline_provider\",\n",
        "    test_provider=inline_test_provider,\n",
        ")  # validmind will now call the `TestProviderInline.load_test` method whenever it encounters a test ID that starts with `my_inline_provider`\n",
        "\n",
        "vm.tests.register_test_provider(\n",
        "    namespace=\"my_local_provider\",\n",
        "    test_provider=local_test_provider,\n",
        ")  # validmind will now call the `LocalTestProvider.load_test` method whenever it encounters a test ID that starts with `my_local_provider`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc4_3_'></a>\n",
        "\n",
        "### Verify that `preview_template()` now loads the tests from the test providers\n",
        "\n",
        "After registering the test providers with `vm.tests.register_test_provider()`, the developer framework can now locate the code that will execute the tests when we run the documentation tests on the template. We can verify this by running `preview_template()` again and seeing that the tests are now loaded correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm.preview_template()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc4_4_'></a>\n",
        "\n",
        "### Running the template\n",
        "\n",
        "Now we can run the template as usual and it will use the external test providers to load the appropriate tests. Note that we're not passing any inputs such as `dataset` and `model` to `run_documentation_tests()`. This is because our demo test providers do not have any required inputs and we're scoping the template execution to the `test_providers_demo` section by using the `section` keyword argument.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "suite_results = vm.run_documentation_tests(section=\"test_providers_demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc5_'></a>\n",
        "\n",
        "## Next steps\n",
        "\n",
        "You can look at the output produced by the ValidMind Developer Framework right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n",
        "\n",
        "<a id='toc5_1_'></a>\n",
        "\n",
        "### Work with your model documentation\n",
        "\n",
        "1. From the [**Model Inventory**](https://app.prod.validmind.ai/model-inventory) in the ValidMind Platform UI, go to the model you registered earlier.\n",
        "\n",
        "2. Click and expand the **Model Development** section.\n",
        "\n",
        "3. Expand the **Test Providers Demo** section as defined in the template.\n",
        "\n",
        "What you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it's ready. [Learn more ...](https://docs.validmind.ai/guide/working-with-model-documentation.html)\n",
        "\n",
        "<a id='toc5_2_'></a>\n",
        "\n",
        "### Discover more learning resources\n",
        "\n",
        "We offer many interactive notebooks to help you document models:\n",
        "\n",
        "- [Run tests & test suites](https://docs.validmind.ai/guide/testing-overview.html)\n",
        "- [Code samples](https://docs.validmind.ai/guide/samples-jupyter-notebooks.html)\n",
        "\n",
        "Or, visit our [documentation](https://docs.validmind.ai/) to learn more about ValidMind.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "validmind-mI3jzOkk-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Requisites\n",
    "\n",
    "As mentioned above, its recommended that you have gone through the [implementing_custom_tests.ipynb](./implementing_custom_tests.ipynb) notebook to understand the basics of creating custom tests. We will be using the same tests defined in that notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "- **Test Provider**: A Python class that gets registered with the ValidMind Framework and loads tests based on a test ID e.g. `my_test_provider.my_test_id`.\n",
    "- **LocalTestProvider**: A built-in test provider that loads tests from a local directory.\n",
    "- **Test ID**: A unique identifier for a test that is used to load the test from a test provider. Test IDs in ValidMind have multiple parts, separated by dots (`.`), that have special meaning/functions - `namespace.path.test_name:result_id` (e.g. `validmind.data_validation.ClassImbalance:1`):\n",
    "    - `namespace`: The namespace of the test - built-in tests use the `validmind` namespace, custom test providers specify a custom namespace when they are registered\n",
    "    - `path`: The path to the test within the test provider. For internal tests, this is relative to the `validmind/tests` directory. When using a `LocalTestProvider` this is relative to the root directory you specify when creating the provider class instance. **Note: This path-based approach is a convention and not a strict requirement when implementing your own test provider.**\n",
    "    - `test_name`: The name of the test module (file) as well as the name of the function or class that is the \"test\" itself. Again, this is a convention for internal and `LocalTestProvider` tests, but test files should be named the same as the test function or class they contain.\n",
    "    - `result_id`: An optional identifier for the test. Especially useful, and required, for running tests multiple times in one section. This is separated from the previous parts by a colon `:` and can be effectively ignored when loading the test itself as its only used to uniquely identify the test result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Level Steps\n",
    "\n",
    "1. Create a folder of custom tests from existing, inline tests.\n",
    "2. Define and register a `LocalTestProvider` that points to that folder.\n",
    "3. Use the test ID to run, view and log custom tests from your provider.\n",
    "4. Add the test results to your documentation\n",
    "5. `run_documentation_tests()` to run custom tests as part of your documentation template suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValidMind at a glance\n",
    "\n",
    "ValidMind's platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
    "\n",
    "If this is your first time trying out ValidMind, we recommend going through the following resources first:\n",
    "\n",
    "- [Get started](https://docs.validmind.ai/guide/get-started.html) — The basics, including key concepts, and how our products work\n",
    "- [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/guide/get-started-developer-framework.html) — The path for developers, more code samples, and our developer reference\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "::: {.callout-tip}\n",
    "\n",
    "### New to ValidMind?\n",
    "\n",
    "For access to all features available in this notebook, create a free ValidMind account.\n",
    "\n",
    "Signing up is FREE — [**Sign up now**](https://app.prod.validmind.ai)\n",
    ":::\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library\n",
    "\n",
    "The client library provides Python support for the ValidMind Developer Framework. To install it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "Get your code snippet:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
    "\n",
    "3. Enter the model details, making sure to select the `Attrition/Churn Management` use case, and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
    "\n",
    "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"...\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    project=\"...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Custom Tests\n",
    "\n",
    "If you've gone through the `implementing_custom_tests.ipynb` notebook, you should have a good understanding of how custom tests are implemented. If you haven't, we recommend going through that notebook first. In this notebook, we will take those custom tests and move them into separate modules in a folder. This is the logical progression from the previous notebook, as it allows you to take one-off tests and move them into an organized structure that makes it easier to manage, maintain and share them.\n",
    "\n",
    "First let's set the path to our custom tests folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_folder = \"my_tests\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create this folder if it doesn't exist. If it does, let's empty it to start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create tests folder\n",
    "os.makedirs(tests_folder, exist_ok=True)\n",
    "\n",
    "# remove existing tests\n",
    "for f in os.listdir(tests_folder):\n",
    "    # remove files and pycache\n",
    "    if f.endswith(\".py\") or f == \"__pycache__\":\n",
    "        os.system(f\"rm -rf {tests_folder}/{f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and redefine the custom tests we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from validmind import metric\n",
    "\n",
    "\n",
    "@metric(\"my_custom_metrics.ConfusionMatrix\")\n",
    "def confusion_matrix(dataset, model):\n",
    "    \"\"\"The confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known.\n",
    "\n",
    "    The confusion matrix is a 2x2 table that contains 4 values:\n",
    "\n",
    "    - True Positive (TP): the number of correct positive predictions\n",
    "    - True Negative (TN): the number of correct negative predictions\n",
    "    - False Positive (FP): the number of incorrect positive predictions\n",
    "    - False Negative (FN): the number of incorrect negative predictions\n",
    "\n",
    "    The confusion matrix can be used to assess the holistic performance of a classification model by showing the accuracy, precision, recall, and F1 score of the model on a single figure.\n",
    "    \"\"\"\n",
    "    confusion_matrix = metrics.confusion_matrix(dataset.y, dataset.y_pred(model))\n",
    "\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(\n",
    "        confusion_matrix=confusion_matrix,\n",
    "        display_labels=[False, True]\n",
    "    )\n",
    "    cm_display.plot()\n",
    "\n",
    "    plt.close()  # close the plot to avoid displaying it\n",
    "\n",
    "    return cm_display.figure_  # return the figure object itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator thats used to register these as one-off custom tests adds a convenience method to the function object that allows us to simply call `<func_name>.save()` to save it to a file. This will save the function to a Python file in the current directory. You can also pass a path to a folder to save it to a different location. In our case, we can pass the variable `tests_folder` to save it to the custom tests folder we created.\n",
    "\n",
    "Normally, this will get you started by creating the file and saving the function code with the correct name. But it won't automatically add any import or other functions/variables outside of the function that are needed for the test to run. In that case, you would need to manually add those to the file.\n",
    "\n",
    "But for the sake of this demo, we will pass a list of imports into the save method to automatically add them to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix.save(tests_folder, imports=[\"import matplotlib.pyplot as plt\", \"from sklearn import metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and do this for the rest of the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@metric(\"my_custom_metrics.Hyperparameters\")\n",
    "def hyperparameters(model):\n",
    "    \"\"\"The hyperparameters of a machine learning model are the settings that control the learning process.\n",
    "    These settings are specified before the learning process begins and can have a significant impact on the\n",
    "    performance of the model.\n",
    "\n",
    "    The hyperparameters of a model can be used to tune the model to achieve the best possible performance\n",
    "    on a given dataset. By examining the hyperparameters of a model, you can gain insight into how the model\n",
    "    was trained and how it might be improved.\n",
    "    \"\"\"\n",
    "    hyperparameters = model.model.get_xgb_params() # dictionary of hyperparameters\n",
    "\n",
    "    # turn the dictionary into a table where each row contains a hyperparameter and its value\n",
    "    return [{\"Hyperparam\": k, \"Value\": v} for k, v in hyperparameters.items() if v]\n",
    "\n",
    "\n",
    "hyperparameters.save(tests_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "@metric(\"my_custom_metrics.ExternalAPI\")\n",
    "def external_api():\n",
    "    \"\"\"This metric calls an external API to get the current BTC price. It then creates\n",
    "    a table with the relevant data so it can be displayed in the documentation.\n",
    "\n",
    "    The purpose of this metric is to demonstrate how to call an external API and use the\n",
    "    data in a metric. A metric like this could even be setup to run in a scheduled\n",
    "    pipeline to keep your documentation in-sync with an external data source.\n",
    "    \"\"\"\n",
    "    url = \"https://api.coindesk.com/v1/bpi/currentprice.json\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # extract the time and the current BTC price in USD\n",
    "    return [\n",
    "        {\n",
    "            \"Time\": data[\"time\"][\"updated\"],\n",
    "            \"Price (USD)\": data[\"bpi\"][\"USD\"][\"rate\"],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "external_api.save(tests_folder, imports=[\"import requests\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly_express as px\n",
    "\n",
    "\n",
    "@metric(\"my_custom_metrics.ParameterExample\")\n",
    "def parameter_example(plot_title=\"Default Plot Title\", x_col=\"sepal_width\", y_col=\"sepal_length\"):\n",
    "    \"\"\"This metric takes two parameters and creates a scatter plot based on them.\n",
    "\n",
    "    The purpose of this metric is to demonstrate how to create a metric that takes\n",
    "    parameters and uses them to generate a plot. This can be useful for creating\n",
    "    metrics that are more flexible and can be used in a variety of scenarios.\n",
    "    \"\"\"\n",
    "    # return px.scatter(px.data.iris(), x=x_col, y=y_col, color=\"species\")\n",
    "    return px.scatter(px.data.iris(), x=x_col, y=y_col, color=\"species\", title=plot_title)\n",
    "\n",
    "\n",
    "parameter_example.save(tests_folder, imports=[\"import plotly_express as px\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly_express as px\n",
    "\n",
    "@metric(\"my_custom_metrics.ComplexOutput\")\n",
    "def complex_output():\n",
    "    \"\"\"This metric demonstrates how to return many tables and figures in a single metric\"\"\"\n",
    "    # create a couple tables\n",
    "    table = [{\"A\": 1, \"B\": 2}, {\"A\": 3, \"B\": 4}]\n",
    "    table2 = [{\"C\": 5, \"D\": 6}, {\"C\": 7, \"D\": 8}]\n",
    "\n",
    "    # create a few figures showing some random data\n",
    "    fig1 = px.line(x=np.arange(10), y=np.random.rand(10), title=\"Random Line Plot\")\n",
    "    fig2 = px.bar(x=[\"A\", \"B\", \"C\"], y=np.random.rand(3), title=\"Random Bar Plot\")\n",
    "    fig3 = px.scatter(x=np.random.rand(10), y=np.random.rand(10), title=\"Random Scatter Plot\")\n",
    "\n",
    "    return {\n",
    "        \"My Cool Table\": table,\n",
    "        \"Another Table\": table2,\n",
    "    }, fig1, fig2, fig3\n",
    "\n",
    "\n",
    "complex_output.save(tests_folder, imports=[\"import numpy as np\", \"import plotly_express as px\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "@metric(\"my_custom_metrics.Image\")\n",
    "def image():\n",
    "    \"\"\"This metric demonstrates how to return an image in a metric\"\"\"\n",
    "\n",
    "    # create a simple plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([1, 2, 3, 4])\n",
    "    ax.set_title(\"Simple Line Plot\")\n",
    "\n",
    "    # save the plot as a PNG image (in-memory buffer)\n",
    "    img_data = io.BytesIO()\n",
    "    fig.savefig(img_data, format=\"png\")\n",
    "    img_data.seek(0)\n",
    "\n",
    "    plt.close()  # close the plot to avoid displaying it\n",
    "\n",
    "    return img_data.read()\n",
    "\n",
    "\n",
    "image.save(tests_folder, imports=[\"import io\", \"import matplotlib.pyplot as plt\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Test Provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Providers Overview\n",
    "\n",
    "Now that we have a folder with our custom tests, we can create a test provider that will tell the ValidMind Developer Framework where to find these tests. ValidMind offers out-of-the-box test providers for local tests (i.e. tests in a folder) or a Github provider for tests in a Github repository. You can also create your own test provider by creating a class that has a `load_test` method that takes a test ID and returns the test function matching that ID. The protocol for test providers is below:\n",
    "\n",
    "```python\n",
    "class ExternalTestProvider(Protocol):\n",
    "    \"\"\"Protocol for user-defined test providers\"\"\"\n",
    "\n",
    "    def load_test(self, test_id: str) -> callable:\n",
    "        \"\"\"Load the test by test ID\n",
    "\n",
    "        Args:\n",
    "            test_id (str): The test ID (does not contain the namespace under which\n",
    "                the test is registered)\n",
    "\n",
    "        Returns:\n",
    "            callable: The test object\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the test is not found\n",
    "        \"\"\"\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Test Provider\n",
    "\n",
    "For most use-cases, the local test provider should be sufficient. Let's go ahead and see how we can do this with our custom tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import LocalTestProvider\n",
    "\n",
    "# initialize the test provider with the tests folder we created earlier\n",
    "my_test_provider = LocalTestProvider(tests_folder)\n",
    "\n",
    "vm.tests.register_test_provider(\n",
    "    namespace=\"my_test_provider\",\n",
    "    test_provider=my_test_provider,\n",
    ")\n",
    "# `my_test_provider.load_test()` will be called for any test ID that starts with `my_test_provider`\n",
    "# e.g. `my_test_provider.ConfusionMatrix` will look for a function named `ConfusionMatrix` in `my_tests/ConfusionMatrix.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Test Provider Tests\n",
    "\n",
    "Now that we have our test provider set up, we can run any test that's located in our tests folder by using the `run_test()` method. This function is your entry point to running single tests in the ValidMind Developer Framework. It takes a test ID and runs the test associated with that ID. For our custom tests, the test ID will be the `namespace` specified when registering the provider, followed by the path to the test file relative to the tests folder. For example, the Confusion Matrix test we created earlier will have the test ID `my_test_provider.ConfusionMatrix`. You could organize the tests in subfolders, say `classification` and `regression`, and the test ID for the Confusion Matrix test would then be `my_test_provider.classification.ConfusionMatrix`.\n",
    "\n",
    "Let's go ahead and run some of our tests. But first, let's setup a dataset and model to run against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the Model and Dataset\n",
    "\n",
    "First let's setup a an example model and dataset to run our custom metic against. Since this is a Confusion Matrix, we will use the Customer Churn dataset that ValidMind provides and train a simple XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from validmind.datasets.classification import customer_churn\n",
    "\n",
    "raw_df = customer_churn.load_data()\n",
    "train_df, validation_df, test_df = customer_churn.preprocess(raw_df)\n",
    "\n",
    "x_train = train_df.drop(customer_churn.target_column, axis=1)\n",
    "y_train = train_df[customer_churn.target_column]\n",
    "x_val = validation_df.drop(customer_churn.target_column, axis=1)\n",
    "y_val = validation_df[customer_churn.target_column]\n",
    "\n",
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_val, y_val)],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy enough! Now we have a model and dataset setup and trained. One last thing to do is bring the dataset and model into the ValidMind framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now, we'll just use the test dataset\n",
    "vm_raw_ds = vm.init_dataset(\n",
    "    dataset=raw_df,\n",
    "    target_column=customer_churn.target_column,\n",
    "    input_id=\"raw_dataset\",\n",
    ")\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    target_column=customer_churn.target_column,\n",
    "    input_id=\"train_dataset\",\n",
    ")\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df,\n",
    "    target_column=customer_churn.target_column,\n",
    "    input_id=\"test_dataset\",\n",
    ")\n",
    "\n",
    "vm_model = vm.init_model(model, input_id=\"model\")\n",
    "# link the model to the datasets\n",
    "vm_train_ds.assign_predictions(model=vm_model)\n",
    "vm_test_ds.assign_predictions(model=vm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Tests\n",
    "\n",
    "Now that we have our model and dataset setup, we can run our custom tests against them. Let's go ahead and run the Confusion Matrix test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\"my_test_provider.ConfusionMatrix\", inputs={\"dataset\": vm_test_ds, \"model\": vm_model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the output of the test above. If you want to learn more about how test functions are run and get turned into test results, check out the `implementing_custom_tests.ipynb` notebook.\n",
    "\n",
    "Let's go ahead and log the result to the ValidMind platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Custom Metrics to Model Documentation\n",
    "\n",
    "Now that the result has been logged to the platform, you can add it to your model documentation. This will add the result where you specify but it also will add the test to the template so it gets run anytime you `run_documentation_tests()`. To do this, go to the documentation page of the model you connected to above and navigate to the `Model Development` -> `Model Evaluation` section. Then hover between any existing content block to reveal the `+` button as shown in the screenshot below.\n",
    "\n",
    "![screenshot showing insert button for test-driven blocks](../../images/insert-test-driven-block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now click on the `+` button and select the `Test-Driven Block` option. This will open a dialog where you can select `Metric` as the type of test and the `My Test Provider Confusion Matrix` from the list of available metrics. You can preview the result and then click `Insert Block` to add it to the documentation.\n",
    "\n",
    "![screenshot showing how to insert a test-driven block](../../images/insert-test-driven-block-test-provider.png)\n",
    "\n",
    "The test should match the result you see above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run, Save and Add Other Tests\n",
    "\n",
    "Let's take the same steps for all of our other custom tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\"my_test_provider.ComplexOutput\", inputs={\"dataset\": vm_test_ds, \"model\": vm_model}).log()\n",
    "run_test(\"my_test_provider.ExternalAPI\").log()\n",
    "run_test(\"my_test_provider.Hyperparameters\", inputs={\"model\": vm_model}).log()\n",
    "run_test(\"my_test_provider.Image\").log()\n",
    "run_test(\"my_test_provider.ParameterExample\", params={\n",
    "    \"plot_title\": \"Test Provider Plot\",\n",
    "    \"x_col\": \"petal_width\",\n",
    "    \"y_col\": \"petal_length\",\n",
    "}).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that they are all saved, follow the same steps above to add them to the model documentation. Once thats done, we will run the documentation tests to see all of them run automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that `preview_template()` now loads the tests from the test providers\n",
    "\n",
    "Now that we have added the tests to the model documentation, we can run the `preview_template()` method to see the tests run automatically.\n",
    "\n",
    "First, let's reload the connection to the model to get the updated documentation template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run `preview_template()` and verify that the tests you added are included in the proper section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.preview_template()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Documentation Tests\n",
    "\n",
    "Now we can run the documentation tests as normal. This should include all of our custom tests which will be loaded from our test provider folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = customer_churn.get_demo_test_config()\n",
    "suite_results = vm.run_documentation_tests(config=test_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we showed how to integrate custom tests into the ValidMind Developer Framework. We created a custom test provider that loads tests from a folder and then ran the tests against a model and dataset. We then added the tests to the model documentation and ran the documentation tests to see all of the tests run automatically. This is a powerful concept that allows you to create, organize and, most importantly, share custom tests with other model developers.\n",
    "\n",
    "To see more documentation on how to use the ValidMind Developer Framework, check out the [ValidMind Developer Framework documentation](https://docs.validmind.ai/guide/get-started-developer-framework.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
