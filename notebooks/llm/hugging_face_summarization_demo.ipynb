{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization of financial data using Hugging Face LLM models\n",
    "This notebook aims to provide an introduction to documenting an LLM model using the ValidMind Developer Framework. The use case presented is a summarization of financial data (https://huggingface.co/datasets/financial_phrasebank).\n",
    "\n",
    "- Initializing the ValidMind Developer Framework\n",
    "- Running a test various tests to quickly generate document about the data and model\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "To use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\n",
    "\n",
    "If you don't already have one, you should also [create a documentation project](https://docs.validmind.ai/guide/create-your-first-documentation-project.html) on the ValidMind platform. You will use this project to upload your documentation and test results.\n",
    "\n",
    "## Install the client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "In a browser, go to the **Client Integration** page of your documentation project and click **Copy to clipboard** next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n",
    "\n",
    "::: {.column-margin}\n",
    "::: {.callout-tip}\n",
    "This step requires a documentation project. [Learn how you can create one](https://docs.validmind.ai/guide/create-your-first-documentation-project.html).\n",
    ":::\n",
    ":::\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace the code below with the code snippet from your project ## \n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"cllnq0ckr000273y6ev40pmb5\"\n",
    ")\n",
    "\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import textwrap\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "from rouge import Rouge\n",
    "import plotly.graph_objects as go\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import string\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_cell_text(text, width=50):  \n",
    "    \"\"\"Private function to format a cell's text.\"\"\"\n",
    "    return '\\n'.join([textwrap.fill(line, width=width) for line in text.split('\\n')])\n",
    "\n",
    "def _format_dataframe_for_tabulate(df):\n",
    "    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    # Format all string columns\n",
    "    for column in df_out.columns:\n",
    "        if df_out[column].dtype == object:  # Check if column is of type object (likely strings)\n",
    "            df_out[column] = df_out[column].apply(_format_cell_text)\n",
    "    return df_out\n",
    "\n",
    "def _dataframe_to_html_table(df):\n",
    "    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n",
    "    headers = df.columns.tolist()\n",
    "    table_data = df.values.tolist()\n",
    "    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n",
    "\n",
    "def display_formatted_dataframe(df, num_rows=None):\n",
    "    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n",
    "    if num_rows is not None:\n",
    "        df = df.head(num_rows)\n",
    "    formatted_df = _format_dataframe_for_tabulate(df)\n",
    "    html_table = _dataframe_to_html_table(formatted_df)\n",
    "    display(HTML(html_table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_summaries_to_df(df, summaries):\n",
    "    \"\"\"\n",
    "    Adds a new column 'summary_X' to the dataframe df that contains the given summaries, where X is an incremental number.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The original pandas DataFrame.\n",
    "    - summaries: List/array of summarized texts.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with an additional summary column, with 'labels' being the first column followed by the original 'text'.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()  # Make an explicit copy of the DataFrame\n",
    "\n",
    "    # Check if the length of summaries matches the number of rows in the DataFrame\n",
    "    if len(summaries) != len(df):\n",
    "        raise ValueError(f\"The number of summaries ({len(summaries)}) does not match the number of rows in the DataFrame ({len(df)}).\")\n",
    "\n",
    "    # Determine the name for the new summary column\n",
    "    col_index = 1\n",
    "    col_name = 'summary_1'\n",
    "    while col_name in df.columns:\n",
    "        col_index += 1\n",
    "        col_name = f'summary_{col_index}'\n",
    "\n",
    "    # Add the summaries to the DataFrame\n",
    "    df[col_name] = summaries\n",
    "\n",
    "    # Rearrange the DataFrame columns to have 'topic' first, then the original 'input', followed by summary columns\n",
    "    summary_columns = [col for col in df.columns if col.startswith('summary')]\n",
    "    other_columns = [col for col in df.columns if col not in summary_columns + ['topic', 'input', 'reference_summary']]\n",
    "    \n",
    "    columns_order = ['topic', 'input', 'reference_summary'] + sorted(summary_columns) + other_columns\n",
    "    df = df[columns_order]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(df, ref_column, gen_column, metric=\"rouge-2\"):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores for each row in the DataFrame.\n",
    "\n",
    "    :param df: DataFrame containing the summaries\n",
    "    :param ref_column: Column name for the reference summaries\n",
    "    :param gen_column: Column name for the generated summaries\n",
    "    :param metric: Type of ROUGE metric (\"rouge-1\", \"rouge-2\", \"rouge-l\", \"rouge-s\")\n",
    "    :return: DataFrame with ROUGE scores for each row\n",
    "    \"\"\"\n",
    "    if metric not in [\"rouge-1\", \"rouge-2\", \"rouge-l\", \"rouge-s\"]:\n",
    "        raise ValueError(\"Invalid metric. Choose from 'rouge-1', 'rouge-2', 'rouge-l', 'rouge-s'.\")\n",
    "    \n",
    "    rouge = Rouge(metrics=[metric])\n",
    "    score_list = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        scores = rouge.get_scores(row[gen_column], row[ref_column], avg=True)[metric]\n",
    "        score_list.append(scores)\n",
    "    \n",
    "    return pd.DataFrame(score_list)\n",
    "\n",
    "def visualize_rouge_scores(df_scores):\n",
    "    \"\"\"\n",
    "    Visualize ROUGE scores using Plotly line plots for each row.\n",
    "\n",
    "    :param df_scores: DataFrame of ROUGE scores.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Adding the line plots\n",
    "    fig.add_trace(go.Scatter(x=df_scores.index, y=df_scores['p'], mode='lines+markers', name='Precision'))\n",
    "    fig.add_trace(go.Scatter(x=df_scores.index, y=df_scores['r'], mode='lines+markers', name='Recall'))\n",
    "    fig.add_trace(go.Scatter(x=df_scores.index, y=df_scores['f'], mode='lines+markers', name='F1 Score'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"ROUGE Scores for Each Row\",\n",
    "        xaxis_title=\"Row Index\",\n",
    "        yaxis_title=\"Score\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First function\n",
    "def general_text_metrics(df, text_column):\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for text in df[text_column]:\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "        total_words = len(words)\n",
    "        total_sentences = len(sentences)\n",
    "        avg_sentence_length = round(sum(len(sentence.split()) for sentence in sentences) / total_sentences if total_sentences else 0, 1)\n",
    "        total_paragraphs = len(paragraphs)\n",
    "\n",
    "        results.append([total_words, total_sentences, avg_sentence_length, total_paragraphs])\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"Total Words\", \"Total Sentences\", \"Avg Sentence Length\", \"Total Paragraphs\"])\n",
    "\n",
    "# Second function\n",
    "def vocabulary_structure_metrics(df, text_column, unwanted_tokens, num_top_words, lang):\n",
    "    stop_words = set(word.lower() for word in stopwords.words(lang))\n",
    "    unwanted_tokens = set(token.lower() for token in unwanted_tokens)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for text in df[text_column]:\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words and word.lower() not in unwanted_tokens and word not in string.punctuation]\n",
    "\n",
    "        total_unique_words = len(set(filtered_words))\n",
    "        total_punctuations = sum(1 for word in words if word in string.punctuation)\n",
    "        lexical_diversity = round(total_unique_words / len(filtered_words) if filtered_words else 0, 1)\n",
    "\n",
    "        results.append([total_unique_words, total_punctuations, lexical_diversity])\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"Total Unique Words\", \"Total Punctuations\", \"Lexical Diversity\"])\n",
    "\n",
    "# Wrapper function that combines the outputs\n",
    "def text_description_table(df, params):\n",
    "    text_column = params[\"text_column\"]\n",
    "    unwanted_tokens = params[\"unwanted_tokens\"]\n",
    "    num_top_words = params[\"num_top_words\"]\n",
    "    lang = params[\"lang\"]\n",
    "    \n",
    "    gen_metrics_df = general_text_metrics(df, text_column)\n",
    "    vocab_metrics_df = vocabulary_structure_metrics(df, text_column, unwanted_tokens, num_top_words, lang)\n",
    "    \n",
    "    combined_df = pd.concat([gen_metrics_df, vocab_metrics_df], axis=1)\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_description_histograms(df, params):\n",
    "    \n",
    "    text_column = params[\"text_column\"]\n",
    "    num_docs_to_plot = params[\"num_docs_to_plot\"]\n",
    "    \n",
    "    \n",
    "    # Ensure the nltk punkt tokenizer is downloaded\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Decide on the number of documents to plot\n",
    "    if not num_docs_to_plot or num_docs_to_plot > len(df):\n",
    "        num_docs_to_plot = len(df)\n",
    "\n",
    "    # Colors for each subplot\n",
    "    colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "    # Axis titles for clarity\n",
    "    x_titles = [\n",
    "        \"Word Frequencies\",\n",
    "        \"Sentence Position in Document\",\n",
    "        \"Sentence Lengths (Words)\",\n",
    "        \"Word Lengths (Characters)\"\n",
    "    ]\n",
    "    y_titles = [\n",
    "        \"Number of Words\",\n",
    "        \"Sentence Length (Words)\",\n",
    "        \"Number of Sentences\",\n",
    "        \"Number of Words\"\n",
    "    ]\n",
    "\n",
    "    # Iterate over each document in the DataFrame up to the user-specified limit\n",
    "    for index, (idx, row) in enumerate(df.head(num_docs_to_plot).iterrows()):\n",
    "        # Create subplots with a 2x2 grid for each metric\n",
    "        fig = sp.make_subplots(\n",
    "            rows=2, cols=2, \n",
    "            subplot_titles=[\n",
    "                \"Word Frequencies\", \n",
    "                \"Sentence Positions\",\n",
    "                \"Sentence Lengths\", \n",
    "                \"Word Lengths\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Tokenize document into sentences and words\n",
    "        sentences = nltk.sent_tokenize(row[text_column])\n",
    "        words = nltk.word_tokenize(row[text_column])\n",
    "        \n",
    "        # Metrics computation\n",
    "        word_freq = Counter(words)\n",
    "        freq_counts = Counter(word_freq.values())\n",
    "        word_frequencies = list(freq_counts.keys())\n",
    "        word_frequency_counts = list(freq_counts.values())\n",
    "        \n",
    "        sentence_positions = list(range(1, len(sentences) + 1))\n",
    "        sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "        word_lengths = [len(word) for word in words]\n",
    "        \n",
    "        # Adding data to subplots\n",
    "        fig.add_trace(go.Bar(x=word_frequencies, y=word_frequency_counts, marker_color=colors[0], showlegend=False), row=1, col=1)\n",
    "        fig.add_trace(go.Bar(x=sentence_positions, y=sentence_lengths, marker_color=colors[1], showlegend=False), row=1, col=2)\n",
    "        fig.add_trace(go.Histogram(x=sentence_lengths, nbinsx=50, opacity=0.75, marker_color=colors[2], showlegend=False), row=2, col=1)\n",
    "        fig.add_trace(go.Histogram(x=word_lengths, nbinsx=50, opacity=0.75, marker_color=colors[3], showlegend=False), row=2, col=2)\n",
    "\n",
    "        # Update x and y axis titles\n",
    "        for i, (x_title, y_title) in enumerate(zip(x_titles, y_titles)):\n",
    "            fig['layout'][f'xaxis{i+1}'].update(title=x_title, titlefont=dict(size=10))\n",
    "            fig['layout'][f'yaxis{i+1}'].update(title=y_title, titlefont=dict(size=10))\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f\"Text Description for Document {index+1}\",\n",
    "            barmode='overlay',\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot scatter plots for specified combinations using Plotly\n",
    "def text_description_scatter_plot(df, combinations_to_plot):\n",
    "\n",
    "    combinations_to_plot = params[\"combinations_to_plot\"]\n",
    "\n",
    "    for metric1, metric2 in combinations_to_plot:\n",
    "        fig = px.scatter(df, x=metric1, y=metric2, title=f\"Scatter Plot: {metric1} vs {metric2}\")\n",
    "        fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face summarisation wrappers\n",
    "\n",
    "The following code template showcases how to wrap a Hugging Face model for compatibility with the ValidMind Developer Framework. We will load an example model using the transformers API and then run some predictions on our test dataset.\n",
    "\n",
    "The ValidMind developer framework provides support for Hugging Face transformers out of the box, so in the following section we will show how to initialize multiple transformers models with the `init_model` function, removing the need for a custom wrapper. In cases where you need extra pre-processing or post-processing steps, you can use the following code template as a starting point to wrap your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "@dataclass\n",
    "class AbstractSummarization_HuggingFace:\n",
    "    \"\"\"\n",
    "    A VM Model instance wrapper for abstract summarization using HuggingFace Transformers.\n",
    "    \"\"\"\n",
    "    model: any\n",
    "    tokenizer: any\n",
    "    predicted_prob_values: list = None\n",
    "\n",
    "    def __init__(self, model_name=None, model=None, tokenizer=None):\n",
    "        pipeline_task = \"summarization\"\n",
    "        self.model_name = model_name\n",
    "        self.pipeline_task = pipeline_task\n",
    "        self.model = pipeline(pipeline_task, model=model, tokenizer=tokenizer)\n",
    "\n",
    "    def predict(self, texts, params):\n",
    "        \"\"\"\n",
    "        Generates summaries for the given texts.\n",
    "        \n",
    "        Parameters:\n",
    "        - texts (list): List of texts to be summarized.\n",
    "        - min_length (int, optional): Minimum length of the produced summary.\n",
    "        - max_length (int, optional): Maximum length of the produced summary.\n",
    "        \n",
    "        Returns:\n",
    "        - List of summaries.\n",
    "        \"\"\"\n",
    "        \n",
    "        min_length = params.get(\"min_length\")\n",
    "        max_length = params.get(\"max_length\")\n",
    "        \n",
    "        # If either value is None, don't pass it to the model function\n",
    "        model_args = {}\n",
    "        if min_length is not None:\n",
    "            model_args[\"min_length\"] = min_length\n",
    "        if max_length is not None:\n",
    "            model_args[\"max_length\"] = max_length\n",
    "\n",
    "        summaries = []\n",
    "\n",
    "        for text in texts:\n",
    "            data = [str(text)]\n",
    "            results = self.model(data, **model_args)  # Using ** unpacking to pass arguments conditionally\n",
    "            results_df = pd.DataFrame(results)\n",
    "            summary = results_df[\"summary_text\"].values[0] if \"summary_text\" in results_df.columns else results_df[\"label\"].values[0]\n",
    "            summaries.append(summary)\n",
    "\n",
    "        return summaries\n",
    "\n",
    "\n",
    "    def predict_proba(self):\n",
    "        \"\"\"\n",
    "        Retrieves predicted probabilities after prediction. \n",
    "        Note: Not all models provide predicted probabilities.\n",
    "        \"\"\"\n",
    "        if self.predicted_prob_values is None:\n",
    "            raise ValueError(\"First run predict method to retrieve predicted probabilities\")\n",
    "        return self.predicted_prob_values\n",
    "\n",
    "    def description(self):\n",
    "        \"\"\"\n",
    "        Describes the methods available in the class.\n",
    "\n",
    "        Returns:\n",
    "        - A string describing the methods.\n",
    "        \"\"\"\n",
    "        desc = (\n",
    "            \"This class provides methods for abstract summarization using HuggingFace Transformers.\\n\"\n",
    "            \"1. predict: Generates summaries for given texts. Accepts optional min_length and max_length parameters.\\n\"\n",
    "            \"2. predict_proba: Retrieves predicted probabilities after prediction (if available).\\n\"\n",
    "        )\n",
    "        return desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "@dataclass\n",
    "class ExtractiveSummarization_BERT:\n",
    "    model: any\n",
    "    tokenizer: any\n",
    "\n",
    "    def _get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding='max_length')\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**inputs)\n",
    "        return output['last_hidden_state'].mean(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "    def predict(self, texts, params):\n",
    "        summaries = []\n",
    "        \n",
    "        for text in texts:\n",
    "            sentences = text.split('. ') # Changed to splitting at period for general sentence delineation.\n",
    "            \n",
    "            document_embedding = self._get_embedding(' '.join(sentences))\n",
    "            sentence_embeddings = [self._get_embedding(sentence) for sentence in sentences]\n",
    "            similarities = cosine_similarity(sentence_embeddings, [document_embedding])\n",
    "            sorted_indices = np.argsort(similarities, axis=0)[::-1].squeeze()\n",
    "\n",
    "            if params[\"method\"] == \"percentage\":\n",
    "                top_k = int(len(sentences) * params[\"value\"])\n",
    "                selected_sentences = [sentences[i] for i in sorted_indices[:top_k]]\n",
    "\n",
    "            elif params[\"method\"] == \"fixed_sentences\":\n",
    "                top_k = params[\"value\"]\n",
    "                selected_sentences = [sentences[i] for i in sorted_indices[:top_k]]\n",
    "\n",
    "            elif params[\"method\"] == \"word_count\":\n",
    "                selected_sentences = []\n",
    "                total_words = 0\n",
    "                for index in sorted_indices:\n",
    "                    current_sentence_words = len(sentences[index].split())\n",
    "                    # If adding the current sentence doesn't exceed the word limit, add it.\n",
    "                    if total_words + current_sentence_words <= params[\"value\"]:\n",
    "                        total_words += current_sentence_words\n",
    "                        selected_sentences.append(sentences[index])\n",
    "                    # Once the word limit is reached or exceeded, stop adding more sentences.\n",
    "                    if total_words >= params[\"value\"]:\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid method specified.\")\n",
    "            \n",
    "            summaries.append(' '.join(selected_sentences))\n",
    "        \n",
    "        return summaries\n",
    "\n",
    "    def description(self):\n",
    "        \"\"\" \n",
    "        Provides a description of the methods available for extractive summarization.\n",
    "        \n",
    "        1. Percentage: Extracts a given percentage of the total sentences from the original text.\n",
    "        2. Fixed Sentences: Extracts a fixed number of sentences from the original text.\n",
    "        3. Word Count: Extracts sentences up to a given word count.\n",
    "        \n",
    "        For all methods, sentences are ranked based on their similarity to the overall document meaning, as determined by BERT embeddings.\n",
    "        \"\"\"\n",
    "        return self.description.__doc__\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll load the financial dataset, which will be the foundation for our summarization analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/bbc_text_cls_reference.csv')\n",
    "df = df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extractive Summarization: Hugging Face-BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")  \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "extractive_model = ExtractiveSummarization_BERT(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df.copy()\n",
    "data = df_results.input.values.tolist()\n",
    "\n",
    "params = {\n",
    "    \"method\": \"percentage\",\n",
    "    \"value\": 0.4\n",
    "}\n",
    "\n",
    "list_extractive_summary = extractive_model.predict(data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = add_summaries_to_df(df_results, list_extractive_summary)\n",
    "display_formatted_dataframe(df_results, num_rows=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Abstract Summarization: Hugging Face-T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "abstract_model = AbstractSummarization_HuggingFace(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_results.summary_1.values.tolist()\n",
    "\n",
    "params = {\n",
    "    \"min_length\": None,\n",
    "    \"max_length\": 50\n",
    "}\n",
    "\n",
    "list_abstract_summary = abstract_model.predict(data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = add_summaries_to_df(df_results, list_abstract_summary)\n",
    "\n",
    "display_formatted_dataframe(df_results, num_rows=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Input text\n",
    "input_text = (\"Ad sales boost Time Warner profit Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) \"\n",
    "              \"for the three months to December, from $639m year-earlier. The firm, which is now one of the biggest investors \"\n",
    "              \"in Google, benefited from sales of high- speed internet connections and higher advert sales For 2005, TimeWarner \"\n",
    "              \"is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins. \"\n",
    "              \"TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators \"\n",
    "              \"TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), \"\n",
    "              \"which is close to concluding. Time Warner's fourth quarter profits were slightly better than analysts' expectations \"\n",
    "              \"For the full- year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn \"\n",
    "              \"Its profits were buoyed by one- off gains which offset a profit dip at Warner Bros, and less users for AOL. \"\n",
    "              \"Time Warner said on Friday that it now owns 8% of search-engine Google However, the company said AOL's underlying profit before \"\n",
    "              \"exceptional items rose 8% on the back of stronger internet advertising revenues.\")\n",
    "\n",
    "# Summary\n",
    "summary = (\"the dollar has hit its highest level against the euro in almost three months . the federal reserve head said the US trade deficit is set to stabilise . he highlighted the government's willingness to curb spending and rising household savings \")\n",
    "\n",
    "# Tokenize the texts\n",
    "input_tokens = tokenizer.tokenize(input_text)\n",
    "summary_tokens = tokenizer.tokenize(summary)\n",
    "\n",
    "# Print token counts\n",
    "print(f\"Number of tokens in input text: {len(input_tokens)}\")\n",
    "print(f\"Number of tokens in the summary: {len(summary_tokens)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEXT DESCRIPTION TABLE**\n",
    "\n",
    "- Total Words: Assess the length and complexity of the input text. Longer documents might require more sophisticated summarization techniques, while shorter ones may need more concise summaries.\n",
    "\n",
    "- Total Sentences: Understand the structural makeup of the content. Longer texts with numerous sentences might require the model to generate longer summaries to capture essential information.\n",
    "\n",
    "- Avg Sentence Length: Determine the average length of sentences in the text. This can help the model decide on the appropriate length for generated summaries, ensuring they are coherent and readable.\n",
    "\n",
    "- Total Paragraphs: Analyze how the content is organized into paragraphs. The model should be able to maintain the logical structure of the content when producing summaries.\n",
    "\n",
    "- Total Unique Words: Measure the diversity of vocabulary in the text. A higher count of unique words could indicate more complex content, which the model needs to capture accurately.\n",
    "\n",
    "- Most Common Words: Identify frequently occurring words that likely represent key themes. The model should pay special attention to including these words and concepts in its summaries.\n",
    "\n",
    "- Total Punctuations: Evaluate the usage of punctuation marks, which contribute to the tone and structure of the content. The model should be able to maintain appropriate punctuation in summaries.\n",
    "\n",
    "- Lexical Diversity: Calculate the richness of vocabulary in relation to the overall text length. A higher lexical diversity suggests a broader range of ideas and concepts that the model needs to capture in its summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"text_column\": \"raw_text\",\n",
    "    \"unwanted_tokens\": {'s', 's\\'', 'mr', 'ms', 'mrs', 'dr', '\\'s', ' ', \"''\", 'dollar', 'us', '``'},\n",
    "    \"num_top_words\": 3,\n",
    "    \"lang\": \"english\"\n",
    "}\n",
    "\n",
    "df_text_description = text_description_table(df_summaries, params)\n",
    "display(df_text_description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEXT DESCRIPTION SCATTER PLOT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the combinations you want to plot\n",
    "combinations_to_plot = [\n",
    "    (\"Total Words\", \"Total Sentences\"),\n",
    "    (\"Total Words\", \"Total Unique Words\"),\n",
    "    (\"Total Sentences\", \"Avg Sentence Length\"),\n",
    "    (\"Total Unique Words\", \"Lexical Diversity\")\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"combinations_to_plot\": combinations_to_plot\n",
    "}\n",
    "\n",
    "text_description_scatter_plot(df_text_description, params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEXT DESCRIPTION HISTOGRAM**\n",
    "\n",
    "- Word Frequencies: This metric provides a histogram of how often words appear with a given frequency. For example, if a lot of words appear only once in a document, it might be indicative of a text rich in unique words. On the other hand, a small set of words appearing very frequently might indicate repetitive content or a certain theme or pattern in the text.\n",
    "\n",
    "- Sentence Positions vs. Sentence Lengths: This bar chart showcases the length of each sentence (in terms of word count) in their order of appearance in the document. This can give insights into the flow of information in a text, highlighting any long, detailed sections or brief, potentially superficial areas.\n",
    "\n",
    "- Sentence Lengths Distribution: A histogram showing the frequency of sentence lengths across the document. Long sentences might contain a lot of information but could be harder for summarization models to digest and for readers to comprehend. Conversely, many short sentences might indicate fragmented information.\n",
    "\n",
    "- Word Lengths Distribution: A histogram of the lengths of words in the document. Extremely long words might be anomalies, technical terms, or potential errors in the corpus. Conversely, a majority of very short words might denote lack of depth or specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"text_column\": 'raw_text',\n",
    "    \"num_docs_to_plot\": 2\n",
    "}\n",
    "\n",
    "text_description_histograms(df_summaries, params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Score \n",
    "\n",
    "The ROUGE score ((Recall-Oriented Understudy for Gisting Evaluation) is a widely adopted set of metrics used for evaluating automatic summarization and machine translation. It fundamentally measures the overlap between the n-grams in the generated summary and those in the reference summary.\n",
    "\n",
    "- **ROUGE-N**: This evaluates the overlap of n-grams between the produced summary and reference summary. It calculates precision (the proportion of n-grams in the generated summary that are also present in the reference summary), recall (the proportion of n-grams in the reference summary that are also present in the generated summary), and F1 score (the harmonic mean of precision and recall).\n",
    "\n",
    "- **ROUGE-L**: This metric is based on the Longest Common Subsequence (LCS) between the generated and reference summaries. LCS measures the longest sequence of tokens in the generated summary that matches the reference, without considering the order. It's beneficial because it can identify and reward longer coherent matching sequences.\n",
    "\n",
    "- **ROUGE-S**: This measures the skip-bigram overlap, considering the pair of words in order as \"bigrams\" while allowing arbitrary gaps or \"skips\". This can be valuable to capture sentence-level structure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"rouge-l\"\n",
    "df_scores = calculate_rouge_scores(\n",
    "    df_all_summaries, \n",
    "    ref_column=\"reference_summary\", \n",
    "    gen_column=\"summary_1\", \n",
    "    metric=metric)\n",
    "visualize_rouge_scores(df_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "This is a statistical measure used to evaluate the importance of a word in a document relative to a corpus. Words with high TF-IDF scores are considered more important. By examining words with the highest scores, you can potentially identify key topics or themes in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'text': [\n",
    "        'OpenAI develops artificial general intelligence.',\n",
    "        'Artificial intelligence has many applications.',\n",
    "        'OpenAI is leading in AI research and development.'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the text column\n",
    "tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Create a DataFrame for TF-IDF vectors\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get the top N words with highest TF-IDF score for each document\n",
    "N = 3  # Change N as per your requirement\n",
    "\n",
    "top_words = {}\n",
    "for i, row in tfidf_df.iterrows():\n",
    "    top_words[i] = row.nlargest(N).index.tolist()\n",
    "\n",
    "print(top_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
