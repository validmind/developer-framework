{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Scorecard Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "To use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\n",
    "\n",
    "If you don't already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "In a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n",
    "\n",
    "This step requires a documentation project. Learn how you can create one.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 15:56:18,021 - INFO(validmind.api_client): Connected to ValidMind. Project: [6] Credit Risk Scorecard - Initial Validation (clk00h0u800x9qjy67gduf5om)\n"
     ]
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"clk00h0u800x9qjy67gduf5om\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Credit risk Scorecard** model created from the Lending Club dataset is instrumental in computing the Probability of Default (PD), a key factor in ECL calculations. This scorecard assesses several credit characteristics of potential borrowers, like their credit history, income, outstanding debts, and more, each of which is assigned a specific score. By combining these scores, we derive a total score for each borrower, which translates into an estimated Point-in-Time (PiT) PD. The PiT PD reflects the borrower's likelihood of default at a specific point in time, accounting for both current and foreseeable future conditions.\n",
    "\n",
    "Additionally, for a holistic view of credit risk, it's essential to estimate the Lifetime PD. The Lifetime PD, as the name suggests, predicts the borrower's likelihood of default throughout the life of the exposure, taking into account potential future changes in the economic and financial conditions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "# Standard library imports\n",
    "import re\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "# Data handling and analysis imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import inspect\n",
    "\n",
    "# Visualization imports\n",
    "%matplotlib inline\n",
    "\n",
    "# Scorecard development\n",
    "import scorecardpy as sc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, df, base_filename):\n",
    "    \"\"\"Save a model and a dataframe with a timestamp in the filename\"\"\"\n",
    "    # Get current date and time\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # Convert the current date and time to string\n",
    "    timestamp_str = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    filename = f'{base_filename}_{timestamp_str}.pkl'\n",
    "\n",
    "    # Save the model and dataframe\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump((model, df), file)\n",
    "        \n",
    "    print(f\"Model and dataframe saved as {filename}\")\n",
    "\n",
    "def get_numerical_columns(df):\n",
    "        numerical_columns = df.select_dtypes(\n",
    "            include=[\"int\", \"float\", \"uint\"]\n",
    "        ).columns.tolist()\n",
    "        return numerical_columns\n",
    "\n",
    "def get_categorical_columns(df):\n",
    "        categorical_columns = df.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns.tolist()\n",
    "        return categorical_columns\n",
    "\n",
    "def compute_outliers(series, threshold=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    return series[(series < lower_bound) | (series > upper_bound)]\n",
    "\n",
    "def transform_woe_df(woe_df):\n",
    "    # Select and rename columns\n",
    "    transformed_df = woe_df[['variable', 'bin', 'count', 'count_distr', 'good', 'bad', 'badprob', 'woe', 'bin_iv', 'total_iv']].copy()\n",
    "    transformed_df.rename(columns={\n",
    "        'bin_iv': 'total_iv'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Create 'is_special_values' column (assuming there are no special values)\n",
    "    transformed_df['is_special_values'] = False\n",
    "    \n",
    "    # Transform 'bin' column into interval format and store it in 'breaks' column\n",
    "    transformed_df['breaks'] = transformed_df['bin'].apply(lambda x: '[-inf, %s)' % x if isinstance(x, float) else '[%s, inf)' % x)\n",
    "    \n",
    "    # Group by 'variable' to create bins dictionary\n",
    "    bins = {}\n",
    "    for variable, group in transformed_df.groupby('variable'):\n",
    "        bins[variable] = group\n",
    "    \n",
    "    return bins\n",
    "\n",
    "def get_features_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    # Also add any columns where all values are missing\n",
    "    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n",
    "\n",
    "    # Remove duplicates (if any)\n",
    "    variables_to_drop = list(set(variables_to_drop))\n",
    "\n",
    "    return variables_to_drop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developer Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_raw_data(source): \n",
    "    print(\"Importing raw data from:\", source)\n",
    "    df_out = pd.read_csv(source)\n",
    "    print(f\"Data imported successfully with {df_out.shape[0]} rows and {df_out.shape[1]} columns.\")\n",
    "    return df_out\n",
    "\n",
    "def drop_features(df, to_drop):\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Before dropping\n",
    "    initial_cols = df_out.shape[1]\n",
    "    \n",
    "    df_out.drop(columns=to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # After dropping\n",
    "    after_drop_cols = df_out.shape[1]\n",
    "    \n",
    "    print(f\"Dropped {initial_cols - after_drop_cols} columns.\")\n",
    "    print(f\"Columns remaining after dropping: {after_drop_cols}\")\n",
    "\n",
    "    return df_out \n",
    "\n",
    "def add_default_definition(df, default_column):\n",
    "    \n",
    "    # Check if 'loan_status' is in the DataFrame\n",
    "    if 'loan_status' not in df.columns:\n",
    "        raise ValueError(\"'loan_status' column not found in the DataFrame.\")\n",
    "\n",
    "    print(\"Converting 'loan_status' to target column...\")\n",
    "    # Assuming the column name is 'loan_status'\n",
    "    df[default_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n",
    "\n",
    "    initial_row_count = df.shape[0]\n",
    "    # Remove rows where the target column is NaN\n",
    "    df = df.dropna(subset=[default_column])\n",
    "    removed_rows = initial_row_count - df.shape[0]\n",
    "    print(f\"Removed {removed_rows} rows with undefined 'loan_status' values.\")\n",
    "\n",
    "    # Convert target column to integer\n",
    "    df[default_column] = df[default_column].astype(int)\n",
    "    print(f\"Converted 'loan_status' to '{default_column}' and set its data type to integer.\")\n",
    "    \n",
    "    # Remove the 'loan_status' column from the DataFrame\n",
    "    df.drop(columns=['loan_status'], inplace=True)\n",
    "    print(\"'loan_status' column has been removed from the DataFrame.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_term_column(df):\n",
    "    \"\"\"\n",
    "    Function to remove 'months' string from the 'term' column and convert it to categorical\n",
    "    \"\"\"\n",
    "    \n",
    "    column = \"term\"\n",
    "    \n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].str.replace(' months', '')\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_emp_length_column(df):\n",
    "    \"\"\"\n",
    "    Function to clean 'emp_length' column and convert it to categorical.\n",
    "    \"\"\"\n",
    "    \n",
    "    column = \"emp_length\"\n",
    "    \n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].replace('n/a', np.nan)\n",
    "    df[column] = df[column].str.replace('< 1 year', str(0))\n",
    "    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n",
    "    df[column].fillna(value = 0, inplace=True)\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "    return df \n",
    "\n",
    "def convert_inq_last_6mths_column(df):\n",
    "    \"\"\"\n",
    "    Function to convert 'inq_last_6mths' column into categorical.\n",
    "    \"\"\"\n",
    "    column = \"inq_last_6mths\"\n",
    "\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_iqr_outliers(df, target_column, threshold=1.5):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols.remove(target_column)  # Exclude target_column from numerical columns\n",
    "    for col in num_cols:\n",
    "        outliers = compute_outliers(df[col], threshold)\n",
    "        df = df[~df[col].isin(outliers)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_split(df, target_column):\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Split data into train and test \n",
    "    X = df_out.drop(target_column, axis=1)\n",
    "    y = df_out[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(f\"Training data has {X_train.shape[0]} rows and {X_train.shape[1]} columns.\")\n",
    "    print(f\"Test data has {X_test.shape[0]} rows and {X_test.shape[1]} columns.\")\n",
    "\n",
    "    # Concatenate X_train with y_train to form df_train\n",
    "    df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # Concatenate X_test with y_test to form df_test\n",
    "    df_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def remove_features_missing_values(df, min_missing_percentage):\n",
    "    \"\"\"Drop columns with missing values exceeding a certain percentage.\"\"\"\n",
    "    \n",
    "    def get_features_with_min_missing(data, threshold_percentage):\n",
    "        \"\"\"Get features with missing values above the given threshold.\"\"\"\n",
    "        missing_percent = data.isnull().mean() * 100\n",
    "        return missing_percent[missing_percent > threshold_percentage].index.tolist()\n",
    "\n",
    "    print(\"Analyzing missing values in the dataset...\")\n",
    "    vars_to_drop = get_features_with_min_missing(df, min_missing_percentage)\n",
    "    \n",
    "    if vars_to_drop:\n",
    "        print(f\"Found {len(vars_to_drop)} features with more than {min_missing_percentage}% missing values.\")\n",
    "        print(\"Dropping the following columns:\", ', '.join(vars_to_drop))\n",
    "        return df.drop(columns=vars_to_drop)\n",
    "    else:\n",
    "        print(f\"No features found with more than {min_missing_percentage}% missing values.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "def drop_categories(df):\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Initial count\n",
    "    initial_count = df_out.shape[0]\n",
    "\n",
    "    # Select rows where purpose is 'debt_consolidation' or 'credit_card'\n",
    "    df_out = df_out[df_out['purpose'].isin(['debt_consolidation', 'credit_card'])]\n",
    "    print(f\"Rows retained with purpose 'debt_consolidation' or 'credit_card': {df_out.shape[0]}\")\n",
    "\n",
    "    # Remove rows where grade is 'F' or 'G'\n",
    "    df_out = df_out[~df_out['grade'].isin(['F', 'G'])]\n",
    "    print(f\"Rows after removing grades 'F' or 'G': {df_out.shape[0]}\")\n",
    "\n",
    "    # Remove rows where sub_grade starts with 'F' or 'G'\n",
    "    df_out = df_out[~df_out['sub_grade'].str.startswith(('F', 'G'))]\n",
    "    print(f\"Rows after removing sub_grades starting with 'F' or 'G': {df_out.shape[0]}\")\n",
    "\n",
    "    # Remove rows where home_ownership is 'OTHER', 'NONE', or 'ANY'\n",
    "    df_out = df_out[~df_out['home_ownership'].isin(['OTHER', 'NONE', 'ANY'])]\n",
    "    print(f\"Rows after removing home_ownership values 'OTHER', 'NONE', or 'ANY': {df_out.shape[0]}\")\n",
    "\n",
    "    print(f\"Total rows dropped: {initial_count - df_out.shape[0]}\")\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def convert_to_woe(df, woe_df, target_col):\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    # Placeholder for the transformation function - you'll need to define or import it\n",
    "    bins = transform_woe_df(woe_df)\n",
    "    \n",
    "    # Print how many features are getting transformed\n",
    "    print(f\"Converting {len(bins)} features to WoE values.\")\n",
    "    \n",
    "    # Make sure we don't transform the target column\n",
    "    if target_col in bins:\n",
    "        del bins[target_col]\n",
    "        print(f\"Excluded {target_col} from WoE transformation.\")\n",
    "    \n",
    "    # Apply the WoE transformation\n",
    "    df_out = sc.woebin_ply(df_out, bins=bins)\n",
    "    \n",
    "    print(f\"Successfully converted features to WoE values.\")\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def add_constant(df):\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Before adding constant\n",
    "    initial_cols = df_out.shape[1]\n",
    "\n",
    "    # Add constant\n",
    "    df_out = sm.add_constant(df_out)\n",
    "\n",
    "    # After adding constant\n",
    "    after_add_cols = df_out.shape[1]\n",
    "\n",
    "    print(f\"Added constant to dataframe. Number of columns went from {initial_cols} to {after_add_cols}.\")\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "def train_model(df, target_column):\n",
    "    \n",
    "    # Ensure that the target column is in the DataFrame\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"'{target_column}' not found in DataFrame.\")\n",
    "\n",
    "    # Get X (features) and y (target) from df\n",
    "    X = df.drop(target_column, axis=1)  # Drop the target column to get features\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Define the model\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "\n",
    "    print(f\"Training the model with {X.shape[1]} features and {X.shape[0]} data points.\")\n",
    "\n",
    "    # Fit the model\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    print(\"Model trained successfully.\")\n",
    "\n",
    "    return model_fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import inspect\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Set up the logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='INFO: %(message)s')\n",
    "\n",
    "class Developer:\n",
    "    def __init__(self):\n",
    "        self.tasks_log = []\n",
    "        self.tasks_details = []\n",
    "        self.validation_log = []  # Log for validation tests\n",
    "        self.tasks = {}  # Dictionary to store tasks\n",
    "\n",
    "    def add_task(self, task_id, task):\n",
    "        \"\"\"Register a task.\"\"\"\n",
    "        if task_id in self.tasks:\n",
    "            raise ValueError(f\"Task ID '{task_id}' already exists!\")\n",
    "        self.tasks[task_id] = {'task': task}\n",
    "        return task_id\n",
    "\n",
    "    def get_caller_info(self, frame):\n",
    "        \"\"\"Fetch the calling line of code and the variable names.\"\"\"\n",
    "        code_context = inspect.getframeinfo(frame).code_context\n",
    "        line_of_code = code_context[0].strip() if code_context else \"\"\n",
    "        input_vars = {id(var): name for name, var in frame.f_locals.items()}\n",
    "        return line_of_code, input_vars\n",
    "\n",
    "    def get_task(self, task_id):\n",
    "        \"\"\"Retrieve task entry based on the task ID.\"\"\"\n",
    "        task_entry = self.tasks.get(task_id)\n",
    "        if not task_entry:\n",
    "            raise ValueError(f\"No task found for ID {task_id}\")\n",
    "        return task_entry\n",
    "\n",
    "    def execute_task(self, task_id, inputs=None, area_id=None, validation_tests=None):\n",
    "        if inputs is None:\n",
    "            inputs = []\n",
    "        \n",
    "        logging.info(f\"Executing task '{task_id}'...\\n\")\n",
    "        \n",
    "        frame = inspect.currentframe().f_back\n",
    "        line_of_code, input_vars = self.get_caller_info(frame)\n",
    "        input_var_names = [input_vars.get(id(inp), \"N/A\") for inp in inputs]\n",
    "        task_entry = self.get_task(task_id)\n",
    "        \n",
    "        result = task_entry['task'](*inputs)\n",
    "        \n",
    "        # Extract the variable name to which the result is assigned\n",
    "        output_match = re.search(r'^\\s*([\\w\\s,]+?)\\s*=', line_of_code)\n",
    "        output_var_name = output_match.group(1).replace(\" \", \"\") if output_match else \"N/A\"\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        end_time = datetime.datetime.now()\n",
    "        duration = (end_time - start_time).seconds\n",
    "        \n",
    "        # Log the task details internally\n",
    "        self.tasks_log.append(task_id)\n",
    "        self.tasks_details.append({\n",
    "            'Time': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'Area ID': area_id,\n",
    "            'Task ID': task_id,\n",
    "            'Input': \", \".join(input_var_names),\n",
    "            'Output': output_var_name,\n",
    "            'Duration': f\"{duration} seconds\"\n",
    "        })\n",
    "\n",
    "        # Log the validation tests\n",
    "        self.validation_log.append({\n",
    "            'Area ID': area_id,\n",
    "            'Task ID': task_id,\n",
    "            'Input': \", \".join(input_var_names),\n",
    "            'Output': output_var_name,\n",
    "            'Validation Tests': \", \".join(validation_tests) if validation_tests else \"N/A\"\n",
    "        })\n",
    "\n",
    "        return result\n",
    "\n",
    "    def show_validation_plan(self):\n",
    "        \"\"\"Return the validation plan details in a tabular format.\"\"\"\n",
    "        df = pd.DataFrame(self.validation_log)\n",
    "\n",
    "        # Use HTML line breaks for Jupyter Notebook rendering\n",
    "        separator = \"<br>\"\n",
    "        df['Validation Tests'] = df['Validation Tests'].apply(lambda x: separator.join(x.split(\", \")) if x != \"none\" else \"none\")\n",
    "\n",
    "        # Replace \"N/A\" with \"none\"\n",
    "        df.replace({\"N/A\": \"none\"}, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    \n",
    "    def show_lifecycle(self):\n",
    "        \"\"\"Display the model lifecycle details in a tabular format.\"\"\"\n",
    "        df = pd.DataFrame(self.tasks_details)\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Development Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_column = \"default\"\n",
    "\n",
    "lending_club_url = \"https://vmai.s3.us-west-1.amazonaws.com/datasets/lending_club_loan_data_2007_2014.csv\"\n",
    "\n",
    "preliminary_features_to_drop = [\n",
    "    \"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n",
    "    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n",
    "    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\", \"pub_rec\",\n",
    "    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n",
    "    \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"last_pymnt_d\", \"last_credit_pull_d\",\n",
    "    'earliest_cr_line', 'issue_d']\n",
    "\n",
    "final_features_to_drop = ['addr_state', 'total_rec_int', 'loan_amnt',\n",
    "                    'funded_amnt_inv', 'dti', 'revol_util', 'total_pymnt', \n",
    "                    'total_pymnt_inv', 'last_pymnt_amnt', \"inq_last_6mths\"]\n",
    "\n",
    "min_missing_percentage = 80"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register Developer Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove_features_missing_values'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Developer class\n",
    "developer = Developer()\n",
    "\n",
    "# Register developer tasks\n",
    "developer.add_task(\n",
    "    task_id=\"import_raw_data\", \n",
    "    task=import_raw_data,\n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"drop_features\",\n",
    "    task=drop_features,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"add_default_definition\",\n",
    "    task=add_default_definition,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"convert_term_column\",\n",
    "    task=convert_term_column,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"convert_emp_length_column\",\n",
    "    task=convert_emp_length_column,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"convert_inq_last_6mths_column\",\n",
    "    task=convert_inq_last_6mths_column,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"data_split\",\n",
    "    task=data_split,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"drop_categories\",\n",
    "    task=drop_categories,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"convert_to_woe\",\n",
    "    task=convert_to_woe,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"add_constant\",\n",
    "    task=add_constant,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"train_model\",\n",
    "    task=train_model,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"remove_features_missing_values\",\n",
    "    task=remove_features_missing_values,  \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'import_raw_data'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing raw data from: https://vmai.s3.us-west-1.amazonaws.com/datasets/lending_club_loan_data_2007_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_61175/927216944.py:3: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_out = pd.read_csv(source)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully with 466285 rows and 75 columns.\n"
     ]
    }
   ],
   "source": [
    "df_1 = developer.execute_task(\n",
    "    area_id = \"data_description\",\n",
    "    task_id = \"import_raw_data\", \n",
    "    inputs = [lending_club_url],\n",
    "    validation_tests = [\"descriptive_statistics\", \"missing_values_bar_plot\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'drop_features'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 33 columns.\n",
      "Columns remaining after dropping: 42\n"
     ]
    }
   ],
   "source": [
    "df_2 = developer.execute_task(\n",
    "    area_id = \"data_preparation\",\n",
    "    task_id = \"drop_features\", \n",
    "    inputs = [df_1, preliminary_features_to_drop],\n",
    "    validation_tests = []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'add_default_definition'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 'loan_status' to target column...\n",
      "Removed 239071 rows with undefined 'loan_status' values.\n",
      "Converted 'loan_status' to 'default' and set its data type to integer.\n",
      "'loan_status' column has been removed from the DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_61175/927216944.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[default_column] = df[default_column].astype(int)\n",
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_61175/927216944.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=['loan_status'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_3 = developer.execute_task(\n",
    "    area_id = \"data_preparation\",\n",
    "    task_id = \"add_default_definition\", \n",
    "    inputs = [df_2, default_column],\n",
    "    validation_tests = [\"missing_values_bar_plot\",\n",
    "                        \"class_imbalance\", \n",
    "                        \"iqr_outliers_table\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'remove_features_missing_values'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing missing values in the dataset...\n",
      "Found 18 features with more than 80% missing values.\n",
      "Dropping the following columns: mths_since_last_major_derog, annual_inc_joint, dti_joint, verification_status_joint, open_acc_6m, open_il_6m, open_il_12m, open_il_24m, mths_since_rcnt_il, total_bal_il, il_util, open_rv_12m, open_rv_24m, max_bal_bc, all_util, inq_fi, total_cu_tl, inq_last_12m\n"
     ]
    }
   ],
   "source": [
    "df_4 = developer.execute_task(\n",
    "    area_id=\"data_preparation\",\n",
    "    task_id=\"remove_features_missing_values\", \n",
    "    inputs=[df_3, min_missing_percentage],\n",
    "    validation_tests=[\"missing_values_bar_plot\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'convert_term_column'...\n",
      "\n",
      "INFO: Executing task 'convert_emp_length_column'...\n",
      "\n",
      "INFO: Executing task 'convert_inq_last_6mths_column'...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5 = developer.execute_task(\n",
    "    area_id=\"data_preparation\",\n",
    "    task_id=\"convert_term_column\", \n",
    "    inputs=[df_4]\n",
    ")\n",
    "\n",
    "df_6 = developer.execute_task(\n",
    "    area_id=\"data_preparation\",\n",
    "    task_id=\"convert_emp_length_column\", \n",
    "    inputs=[df_5]\n",
    ")\n",
    "\n",
    "df_7 = developer.execute_task(\n",
    "    area_id=\"data_preparation\",\n",
    "    task_id=\"convert_inq_last_6mths_column\", \n",
    "    inputs=[df_6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'data_split'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 181771 rows and 23 columns.\n",
      "Test data has 45443 rows and 23 columns.\n"
     ]
    }
   ],
   "source": [
    "df_train_1, df_test_1 = developer.execute_task(\n",
    "    area_id=\"data_sampling\",\n",
    "    task_id=\"data_split\", \n",
    "    inputs=[df_7, default_column],\n",
    "    validation_tests=[\"tabular_numerical_histograms\", \n",
    "                      \"high_cardinality\", \n",
    "                      \"tabular_categorical_bar_plots\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'drop_categories'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows retained with purpose 'debt_consolidation' or 'credit_card': 142293\n",
      "Rows after removing grades 'F' or 'G': 137816\n",
      "Rows after removing sub_grades starting with 'F' or 'G': 137816\n",
      "Rows after removing home_ownership values 'OTHER', 'NONE', or 'ANY': 137723\n",
      "Total rows dropped: 44048\n",
      "Rows retained with purpose 'debt_consolidation' or 'credit_card': 35532\n",
      "Rows after removing grades 'F' or 'G': 34349\n",
      "Rows after removing sub_grades starting with 'F' or 'G': 34349\n",
      "Rows after removing home_ownership values 'OTHER', 'NONE', or 'ANY': 34322\n",
      "Total rows dropped: 11121\n"
     ]
    }
   ],
   "source": [
    "df_train_2 = developer.execute_task(\n",
    "    area_id=\"exploratory_data_analysis\",\n",
    "    task_id=\"drop_categories\", \n",
    "    inputs=[df_train_1],\n",
    "    validation_tests=[\"target_rate_bar_plots\"]\n",
    ")\n",
    "\n",
    "df_test_2 = drop_categories(df_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'drop_features'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 10 columns.\n",
      "Columns remaining after dropping: 14\n",
      "Dropped 10 columns.\n",
      "Columns remaining after dropping: 14\n"
     ]
    }
   ],
   "source": [
    "df_train_3 = developer.execute_task(\n",
    "    area_id=\"exploratory_data_analysis\",\n",
    "    task_id=\"drop_features\", \n",
    "    inputs=[df_train_2, final_features_to_drop],\n",
    "    validation_tests=[\"chi_squared_features_table\", \n",
    "                      \"anova_one_way_table\", \n",
    "                      \"pearson_correlation_matrix\", \n",
    "                      \"feature_target_correlation_plot\",\n",
    "                      \"woe_bin_table\",\n",
    "                      \"woe_bin_table\",   # with different parameters\n",
    "                      \"woe_bin_plots\"]\n",
    ")\n",
    "\n",
    "df_test_3 = drop_features(df_test_2, final_features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 15:57:14,626 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with breaks_adj: {'int_rate': [5, 10, 15]}\n",
      "Performing binning with breaks_adj: {'int_rate': [5, 10, 15]}\n",
      "[INFO] creating woe binning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Library/Caches/pypoetry/virtualenvs/validmind-X_uvMH0R-py3.10/lib/python3.10/site-packages/scorecardpy/condition_fun.py:79: UserWarning: There are blank strings in 1 columns, which are replaced with NaN. \n",
      " (ColumnNames: emp_length)\n",
      "  warnings.warn('There are blank strings in {} columns, which are replaced with NaN. \\n (ColumnNames: {})'.format(\n"
     ]
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.WOEBinTable import WOEBinTable\n",
    "\n",
    "params = {\n",
    "    \"breaks_adj\": {\n",
    "        \"int_rate\": [5,10,15]}  \n",
    "     }\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train_3, target_column=default_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = WOEBinTable(test_context, params=params)\n",
    "metric.run()\n",
    "woe_dic = metric.result.metric.value['woe_iv']\n",
    "woe_df = pd.DataFrame(woe_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'convert_to_woe'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 13 features to WoE values.\n",
      "[INFO] converting into woe values ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Library/Caches/pypoetry/virtualenvs/validmind-X_uvMH0R-py3.10/lib/python3.10/site-packages/scorecardpy/condition_fun.py:79: UserWarning: There are blank strings in 1 columns, which are replaced with NaN. \n",
      " (ColumnNames: emp_length)\n",
      "  warnings.warn('There are blank strings in {} columns, which are replaced with NaN. \\n (ColumnNames: {})'.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted features to WoE values.\n",
      "Converting 13 features to WoE values.\n",
      "[INFO] converting into woe values ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Library/Caches/pypoetry/virtualenvs/validmind-X_uvMH0R-py3.10/lib/python3.10/site-packages/scorecardpy/condition_fun.py:79: UserWarning: There are blank strings in 1 columns, which are replaced with NaN. \n",
      " (ColumnNames: emp_length)\n",
      "  warnings.warn('There are blank strings in {} columns, which are replaced with NaN. \\n (ColumnNames: {})'.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted features to WoE values.\n"
     ]
    }
   ],
   "source": [
    "df_train_4 = developer.execute_task(\n",
    "    area_id=\"feature_engineering\",\n",
    "    task_id=\"convert_to_woe\", \n",
    "    inputs=[df_train_3, woe_df, default_column],\n",
    ")     \n",
    "\n",
    "df_test_4 = convert_to_woe(df_test_3, woe_df, default_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'add_constant'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added constant to dataframe. Number of columns went from 14 to 15.\n",
      "Added constant to dataframe. Number of columns went from 14 to 15.\n"
     ]
    }
   ],
   "source": [
    "df_train_5 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"add_constant\", \n",
    "    inputs=[df_train_4]\n",
    ")\n",
    "\n",
    "df_test_5 = add_constant(df_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'train_model'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with 14 features and 137723 data points.\n",
      "Model trained successfully.\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:               137723\n",
      "Model:                            GLM   Df Residuals:                   137709\n",
      "Model Family:                Binomial   Df Model:                           13\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -59975.\n",
      "Date:                Wed, 09 Aug 2023   Deviance:                   1.1995e+05\n",
      "Time:                        15:57:26   Pearson chi2:                 1.38e+05\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):            0.06635\n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "const                      -1.5257      0.007   -204.231      0.000      -1.540      -1.511\n",
      "Unnamed: 0_woe              0.7117      0.045     15.917      0.000       0.624       0.799\n",
      "term_woe                    0.5146      0.022     23.195      0.000       0.471       0.558\n",
      "int_rate_woe                0.1799      0.028      6.455      0.000       0.125       0.234\n",
      "installment_woe             0.7332      0.073     10.022      0.000       0.590       0.877\n",
      "grade_woe                   0.2865      0.049      5.878      0.000       0.191       0.382\n",
      "open_acc_woe                2.5244      0.220     11.458      0.000       2.093       2.956\n",
      "total_acc_woe               0.9871      0.128      7.713      0.000       0.736       1.238\n",
      "emp_length_woe              0.7761      0.060     12.916      0.000       0.658       0.894\n",
      "verification_status_woe     0.2869      0.044      6.584      0.000       0.201       0.372\n",
      "purpose_woe                 0.2789      0.090      3.095      0.002       0.102       0.455\n",
      "home_ownership_woe          0.6607      0.066      9.999      0.000       0.531       0.790\n",
      "sub_grade_woe               0.3735      0.051      7.386      0.000       0.274       0.473\n",
      "annual_inc_woe              1.1000      0.033     33.221      0.000       1.035       1.165\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_fit_1 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"train_model\", \n",
    "    inputs=[df_train_5, default_column]\n",
    ")\n",
    "\n",
    "print(model_fit_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'drop_features'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 columns.\n",
      "Columns remaining after dropping: 15\n",
      "Dropped 0 columns.\n",
      "Columns remaining after dropping: 15\n"
     ]
    }
   ],
   "source": [
    "model_features_to_drop = []\n",
    "\n",
    "df_train_6 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"drop_features\", \n",
    "    inputs=[df_train_5, model_features_to_drop]\n",
    ")\n",
    "\n",
    "df_test_6 = drop_features(df_test_5, model_features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'train_model'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with 14 features and 137723 data points.\n",
      "Model trained successfully.\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:               137723\n",
      "Model:                            GLM   Df Residuals:                   137709\n",
      "Model Family:                Binomial   Df Model:                           13\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -59975.\n",
      "Date:                Wed, 09 Aug 2023   Deviance:                   1.1995e+05\n",
      "Time:                        15:57:26   Pearson chi2:                 1.38e+05\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):            0.06635\n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "const                      -1.5257      0.007   -204.231      0.000      -1.540      -1.511\n",
      "Unnamed: 0_woe              0.7117      0.045     15.917      0.000       0.624       0.799\n",
      "term_woe                    0.5146      0.022     23.195      0.000       0.471       0.558\n",
      "int_rate_woe                0.1799      0.028      6.455      0.000       0.125       0.234\n",
      "installment_woe             0.7332      0.073     10.022      0.000       0.590       0.877\n",
      "grade_woe                   0.2865      0.049      5.878      0.000       0.191       0.382\n",
      "open_acc_woe                2.5244      0.220     11.458      0.000       2.093       2.956\n",
      "total_acc_woe               0.9871      0.128      7.713      0.000       0.736       1.238\n",
      "emp_length_woe              0.7761      0.060     12.916      0.000       0.658       0.894\n",
      "verification_status_woe     0.2869      0.044      6.584      0.000       0.201       0.372\n",
      "purpose_woe                 0.2789      0.090      3.095      0.002       0.102       0.455\n",
      "home_ownership_woe          0.6607      0.066      9.999      0.000       0.531       0.790\n",
      "sub_grade_woe               0.3735      0.051      7.386      0.000       0.274       0.473\n",
      "annual_inc_woe              1.1000      0.033     33.221      0.000       1.035       1.165\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_fit_2 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"train_model\", \n",
    "    inputs=[df_train_6, default_column],\n",
    "    validation_tests = [\"regression_coeffs_plot\", \n",
    "                        \"regression_models_coeffs\", \n",
    "                        \"log_regression_confusion_matrix\", \n",
    "                        \"regression_roc_curve\", \"gini_table\", \n",
    "                        \"logistic_reg_prediction_histogram\", \n",
    "                        \"logistic_reg_cumulative_prob\", \n",
    "                        \"scorecard_histogram\"]\n",
    ")\n",
    "\n",
    "print(model_fit_2.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area ID</th>\n",
       "      <th>Task ID</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "      <th>Validation Tests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_description</td>\n",
       "      <td>import_raw_data</td>\n",
       "      <td>lending_club_url</td>\n",
       "      <td>df_1</td>\n",
       "      <td>descriptive_statistics<br>missing_values_bar_plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>drop_features</td>\n",
       "      <td>df_1, preliminary_features_to_drop</td>\n",
       "      <td>df_2</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>add_default_definition</td>\n",
       "      <td>df_2, default_column</td>\n",
       "      <td>df_3</td>\n",
       "      <td>missing_values_bar_plot<br>class_imbalance<br>iqr_outliers_table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>remove_features_missing_values</td>\n",
       "      <td>df_3, min_missing_percentage</td>\n",
       "      <td>df_4</td>\n",
       "      <td>missing_values_bar_plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>convert_term_column</td>\n",
       "      <td>df_4</td>\n",
       "      <td>df_5</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>convert_emp_length_column</td>\n",
       "      <td>df_5</td>\n",
       "      <td>df_6</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>convert_inq_last_6mths_column</td>\n",
       "      <td>df_6</td>\n",
       "      <td>df_7</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data_sampling</td>\n",
       "      <td>data_split</td>\n",
       "      <td>df_7, default_column</td>\n",
       "      <td>df_train_1,df_test_1</td>\n",
       "      <td>tabular_numerical_histograms<br>high_cardinality<br>tabular_categorical_bar_plots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>exploratory_data_analysis</td>\n",
       "      <td>drop_categories</td>\n",
       "      <td>df_train_1</td>\n",
       "      <td>df_train_2</td>\n",
       "      <td>target_rate_bar_plots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>exploratory_data_analysis</td>\n",
       "      <td>drop_features</td>\n",
       "      <td>df_train_2, final_features_to_drop</td>\n",
       "      <td>df_train_3</td>\n",
       "      <td>chi_squared_features_table<br>anova_one_way_table<br>pearson_correlation_matrix<br>feature_target_correlation_plot<br>woe_bin_table<br>woe_bin_table<br>woe_bin_plots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>feature_engineering</td>\n",
       "      <td>convert_to_woe</td>\n",
       "      <td>df_train_3, woe_df, default_column</td>\n",
       "      <td>df_train_4</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model_training</td>\n",
       "      <td>add_constant</td>\n",
       "      <td>df_train_4</td>\n",
       "      <td>df_train_5</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model_training</td>\n",
       "      <td>train_model</td>\n",
       "      <td>df_train_5, default_column</td>\n",
       "      <td>model_fit_1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model_training</td>\n",
       "      <td>drop_features</td>\n",
       "      <td>df_train_5, model_features_to_drop</td>\n",
       "      <td>df_train_6</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model_training</td>\n",
       "      <td>train_model</td>\n",
       "      <td>df_train_6, default_column</td>\n",
       "      <td>model_fit_2</td>\n",
       "      <td>regression_coeffs_plot<br>regression_models_coeffs<br>log_regression_confusion_matrix<br>regression_roc_curve<br>gini_table<br>logistic_reg_prediction_histogram<br>logistic_reg_cumulative_prob<br>scorecard_histogram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_validation = developer.show_validation_plan()\n",
    "display(HTML(df_validation.to_html(escape=False)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ValidMind Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 15:57:26,948 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-09 15:57:33,263 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-09 15:57:34,386 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-09 15:57:35,456 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-09 15:57:36,797 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-09 15:57:37,404 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    }
   ],
   "source": [
    "vm_df_1 = vm.init_dataset(dataset=df_1, target_column=default_column)\n",
    "vm_df_3 = vm.init_dataset(dataset=df_3, target_column=default_column)\n",
    "vm_df_4 = vm.init_dataset(dataset=df_4, target_column=default_column)\n",
    "vm_df_train_1 = vm.init_dataset(dataset=df_train_1, target_column=default_column)\n",
    "vm_df_train_2 = vm.init_dataset(dataset=df_train_2, target_column=default_column)\n",
    "vm_df_train_3 = vm.init_dataset(dataset=df_train_3, target_column=default_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ValidMind Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 15:57:38,211 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-09 15:57:38,945 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    }
   ],
   "source": [
    "vm_df_train = vm.init_dataset(dataset=df_train_6, target_column=default_column)\n",
    "vm_df_test = vm.init_dataset(dataset=df_test_6, target_column=default_column)\n",
    "\n",
    "vm_model_fit_2 = vm.init_model(\n",
    "    model = model_fit_2, \n",
    "    train_ds=vm_df_train, \n",
    "    test_ds=vm_df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run All Validation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15de4004ce444338fb3ea7a34fdd073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>This section provides descriptive statistics for numerical and categorical varia"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.DescriptiveStatistics import DescriptiveStatistics\n",
    "\n",
    "test_context_1 = TestContext(dataset=vm_df_1)\n",
    "\n",
    "metric = DescriptiveStatistics(test_context_1)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bdbfb42e0845c08f8de3ecded92317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of missing values by plotting horizontal bar plots w"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n",
    "\n",
    "params = {\"threshold\": 80,\n",
    "          \"fig_height\": 1100}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context_1, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6dc38d0dda4d6791501e00aadc9ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of missing values by plotting horizontal bar plots w"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_context_3 = TestContext(dataset=vm_df_3)\n",
    "\n",
    "params = {\"threshold\": 80,\n",
    "          \"fig_height\": 1100}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context_3, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a3efe2bdd942288250c0b1f300b574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h2>Class Imbalance </h2>\\n            <p>The class imbalance test m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "\n",
    "metric = ClassImbalance(test_context_3)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667e323956ac4035a0681ce17c53b232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Analyzes the distribution of outliers in numerical features using the Interquart"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.IQROutliersTable import IQROutliersTable\n",
    "\n",
    "num_features = get_numerical_columns(df_3)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"threshold\": 1.5\n",
    "        }\n",
    "\n",
    "metric = IQROutliersTable(test_context_3, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7851db39ff146ab8214ccd618c5ee73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of the outliers for numeric variables based on perce"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.IQROutliersBarPlot import IQROutliersBarPlot\n",
    "\n",
    "num_features = get_numerical_columns(df_3)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"threshold\": 1.5,\n",
    "          \"fig_width\": 500}\n",
    "\n",
    "metric = IQROutliersBarPlot(test_context_3, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6ba60dd8004c2ebd102648a0e3c43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of numerical data by plotting the histogram. The inp"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n",
    "\n",
    "test_context_train_1 = TestContext(dataset=vm_df_train_1)\n",
    "\n",
    "metric = TabularNumericalHistograms(test_context_train_1)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11772d01e7004d7c9509e50a5781fd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h2>Cardinality </h2>\\n            <p>The high cardinality test meas"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context_train_1)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd5c92aa4e7459b8498914b33acbbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of categorical data by plotting bar plots. The input"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context_train_1)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column default is correct and contains only 1 and 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9e061ce54c41f49de3bda7485845ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of target ratios by plotting bar plots. The input da"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TargetRateBarPlots import TargetRateBarPlots\n",
    "\n",
    "test_context_train_2 = TestContext(dataset=vm_df_train_2)\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"default_column\": default_column,\n",
    "    \"columns\": None\n",
    "}\n",
    "\n",
    "metric = TargetRateBarPlots(test_context_train_2, params=params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46728c61e7484c66a5e9b369720e0131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Perform a Chi-Squared test of independence for each categorical variable with th"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ChiSquaredFeaturesTable import ChiSquaredFeaturesTable\n",
    "\n",
    "test_context_train_3 = TestContext(dataset=vm_df_train_3)\n",
    "\n",
    "cat_features = get_categorical_columns(df_train_3)\n",
    "params = {\"cat_features\": cat_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ChiSquaredFeaturesTable(test_context_train_3, params)\n",
    "metric.run()\n",
    "await metric.result.log() \n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99887cfbba164e97b4bdd89d2630995e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Perform an ANOVA F-test for each numerical variable with the target. The input d"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ANOVAOneWayTable import ANOVAOneWayTable\n",
    "\n",
    "num_features = get_numerical_columns(df_train_3)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ANOVAOneWayTable(test_context_train_3, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e3184eb9d84ec997432c5f3652b502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Extracts the Pearson correlation coefficient for all pairs of numerical variable"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context_train_3, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af021c3a045546a0981e2703e934949a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of correlations between features and target by plott"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n",
    "\n",
    "params = {\"features\": None}\n",
    "\n",
    "metric = FeatureTargetCorrelationPlot(test_context_train_3, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with breaks_adj: None\n",
      "Performing binning with breaks_adj: None\n",
      "[INFO] creating woe binning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Library/Caches/pypoetry/virtualenvs/validmind-X_uvMH0R-py3.10/lib/python3.10/site-packages/scorecardpy/condition_fun.py:79: UserWarning:\n",
      "\n",
      "There are blank strings in 1 columns, which are replaced with NaN. \n",
      " (ColumnNames: emp_length)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710b95a265a947f4915e2e718e5c6da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<p>Implements WoE-based automatic binning for features in a dataset and calculates "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.WOEBinTable import WOEBinTable\n",
    "\n",
    "metric = WOEBinTable(test_context_train_3)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with breaks_adj: {'int_rate': [5, 10, 15]}\n",
      "Performing binning with breaks_adj: {'int_rate': [5, 10, 15]}\n",
      "[INFO] creating woe binning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Library/Caches/pypoetry/virtualenvs/validmind-X_uvMH0R-py3.10/lib/python3.10/site-packages/scorecardpy/condition_fun.py:79: UserWarning:\n",
      "\n",
      "There are blank strings in 1 columns, which are replaced with NaN. \n",
      " (ColumnNames: emp_length)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf1864058c3499b80d9959a0c7b0a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<p>Implements WoE-based automatic binning for features in a dataset and calculates "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"breaks_adj\": {\n",
    "        \"int_rate\": [5,10,15]}  \n",
    "     }\n",
    "\n",
    "metric = WOEBinTable(test_context_train_3, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating woe binning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Library/Caches/pypoetry/virtualenvs/validmind-X_uvMH0R-py3.10/lib/python3.10/site-packages/scorecardpy/condition_fun.py:79: UserWarning:\n",
      "\n",
      "There are blank strings in 1 columns, which are replaced with NaN. \n",
      " (ColumnNames: emp_length)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352c0677a1bb43378ccbd40ff216b289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of the WoE and IV values distribution for categorica"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.WOEBinPlots import WOEBinPlots\n",
    "\n",
    "params = {\n",
    "    \"breaks_adj\": {\"int_rate\": [5,10,15]},\n",
    "    \"fig_height\": 500,\n",
    "}\n",
    "\n",
    "metric = WOEBinPlots(test_context_train_3, params=params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbd6f297f9f49b89ca1bebf037bb343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<p>Regression Coefficients with Confidence Intervals Plot</p>\\n<p>This class is use"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionCoeffsPlot import RegressionCoeffsPlot\n",
    "\n",
    "test_context_models_fit_2 = TestContext(models = [vm_model_fit_2])\n",
    "\n",
    "metric = RegressionCoeffsPlot(test_context_models_fit_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac26447b15b428e88853b27a58af64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>This section shows the coefficients of different regression models that were tra"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs import RegressionModelsCoeffs\n",
    "\n",
    "metric = RegressionModelsCoeffs(test_context_models_fit_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a219296603649718f8ee798569bc4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>A confusion matrix is a table that is used to describe the performance of a clas"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogRegressionConfusionMatrix import LogRegressionConfusionMatrix\n",
    "\n",
    "test_context_model_fit_2 = TestContext(model= vm_model_fit_2)\n",
    "\n",
    "#Configure test parameters\n",
    "params = {\n",
    "    \"cut_off_threshold\": 0.5,\n",
    "}\n",
    "\n",
    "metric = LogRegressionConfusionMatrix(test_context_model_fit_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc07bf77ca6447bf91839ac4991b854a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>A receiver operating characteristic (ROC), or simply ROC curve, is a graphical p"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionROCCurve import RegressionROCCurve\n",
    "\n",
    "metric = RegressionROCCurve(test_context_model_fit_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted scores obtained...\n",
      "Computing AUC...\n",
      "Computing GINI...\n",
      "Computing AUC...\n",
      "Computing KS...\n",
      "Predicted scores obtained...\n",
      "Computing AUC...\n",
      "Computing GINI...\n",
      "Computing AUC...\n",
      "Computing KS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4b3aa43bc94dbf9821adc2807185fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Compute and display the AUC, GINI, and KS for train and test sets.</p>'), HTML(v"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.GINITable import GINITable\n",
    "\n",
    "metric = GINITable(test_context_model_fit_2)\n",
    "metric.run()\n",
    "await metric.result.log() \n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_61175/4005571062.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegPredictionHistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_context_model_fit_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/github/validmind-python/validmind/tests/model_validation/statsmodels/LogisticRegPredictionHistogram.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/github/validmind-python/validmind/tests/model_validation/statsmodels/LogisticRegPredictionHistogram.py\u001b[0m in \u001b[0;36mcompute_probabilities\u001b[0;34m(model, X)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"probabilities\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogisticRegPredictionHistogram import LogisticRegPredictionHistogram\n",
    "\n",
    "#Configure test parameters\n",
    "params = {\n",
    "    \"title\": \"Histogram of Probability of Default\",\n",
    "}\n",
    "\n",
    "metric = LogisticRegPredictionHistogram(test_context_model_fit_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogisticRegCumulativeProb import LogisticRegCumulativeProb\n",
    "\n",
    "#Configure test parameters\n",
    "params = {\n",
    "    \"title\": \"Cumulative Probability of Default\",\n",
    "}\n",
    "\n",
    "metric = LogisticRegCumulativeProb(test_context_model_fit_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.ScorecardHistogram import ScorecardHistogram\n",
    "\n",
    "#Configure test parameters\n",
    "params = {\n",
    "    \"target_score\": 600,\n",
    "    \"target_odds\": 50,\n",
    "    \"pdo\": 20,\n",
    "    \"title\": \"Histogram of Credit Scores\",\n",
    "}\n",
    "\n",
    "metric = ScorecardHistogram(test_context_model_fit_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
