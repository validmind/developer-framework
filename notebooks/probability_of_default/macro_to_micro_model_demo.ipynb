{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro to Micro Model Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to ValidMind Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to ValidMind Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"clk2jf1yy0005o5y6u8a30v6l\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Available Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.test_plans.describe_plan(\"time_series_data_quality\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.datasets.regression import fred as fred\n",
    "\n",
    "# Define target and feature columns\n",
    "target_column = 'DRSFRMACBS'\n",
    "feature_columns = ['GDPC1', 'CSUSHPISA', 'UNRATE', 'CPIAUCSL', 'FEDFUNDS']\n",
    "\n",
    "# Load FRED data\n",
    "df = fred.load_all_data()\n",
    "\n",
    "# Select columns for analysis\n",
    "df = df[[target_column] + feature_columns]\n",
    "\n",
    "df.tail(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.TimeSeriesMissingValues import TimeSeriesMissingValues\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"min_threshold\": 2}\n",
    "\n",
    "metric = TimeSeriesMissingValues(test_context, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesOutliers import TimeSeriesOutliers\n",
    "\n",
    "params = {\"zscore_threshold\": 3}\n",
    "\n",
    "metric = TimeSeriesOutliers(test_context, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesFrequency import TimeSeriesFrequency\n",
    "\n",
    "metric = TimeSeriesFrequency(test_context)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to quarterly data (end of October)\n",
    "df = df.resample('QS-OCT').mean()\n",
    "\n",
    "# Remove all missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Take the first difference across all variables\n",
    "df = df.diff().dropna()\n",
    "\n",
    "# Remove data from 2020 onwards\n",
    "df = df[df.index.year < 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models import TestPlan\n",
    "\n",
    "class DemoPlan(TestPlan):\n",
    "\n",
    "    name = \"data_description\"\n",
    "    required_context = [\"dataset\"]\n",
    "    tests = [\n",
    "        TimeSeriesMissingValues,\n",
    "        TimeSeriesOutliers,\n",
    "        TimeSeriesFrequency,\n",
    "    ]\n",
    "\n",
    "config = {TimeSeriesMissingValues}\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "test_plan = DemoPlan(test_context=test_context, config=config)\n",
    "test_plan.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from validmind.tests.data_validation.TimeSeriesMissingValues import TimeSeriesMissingValues\n",
    "\n",
    "params = {\"min_threshold\": 2}\n",
    "\n",
    "metric = TimeSeriesMissingValues(test_context, params)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesOutliers import TimeSeriesOutliers\n",
    "\n",
    "params = {\"zscore_threshold\": 3}\n",
    "\n",
    "metric = TimeSeriesOutliers(test_context, params)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesFrequency import TimeSeriesFrequency\n",
    "\n",
    "metric = TimeSeriesFrequency(test_context)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Method\n",
    "\n",
    "We use time series sampling to create our training and testing sets, a crucial step in our macro-to-micro model. This method maintains the temporal order of the data, preserving the inherent dependencies in our time series of macroeconomic indicators and default rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split date\n",
    "split_date = '2018-01-01'\n",
    "\n",
    "# Split data into train and test \n",
    "df_train = df.loc[df.index < split_date]\n",
    "df_test = df.loc[df.index >= split_date]\n",
    "\n",
    "# Split the train and test sets into X and y\n",
    "X_train = df_train.drop(target_column, axis=1)\n",
    "y_train = df_train[target_column]\n",
    "X_test = df_test.drop(target_column, axis=1)\n",
    "y_test = df_test[target_column]\n",
    "\n",
    "# Concatenate X_train with y_train to form df_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Concatenate X_test with y_test to form df_test\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.test_plans.describe_plan(\"time_series_univariate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesLinePlot import TimeSeriesLinePlot\n",
    "\n",
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TimeSeriesLinePlot(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesHistogram import TimeSeriesHistogram\n",
    "\n",
    "metric = TimeSeriesHistogram(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ACFandPACFPlot import ACFandPACFPlot\n",
    "\n",
    "metric = ACFandPACFPlot(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.SeasonalDecompose import SeasonalDecompose\n",
    "\n",
    "params = {\"seasonal_model\": 'additive'}\n",
    "\n",
    "metric = SeasonalDecompose(test_context, params)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.AutoSeasonality import AutoSeasonality\n",
    "\n",
    "params = {\"min_period\": 1,\n",
    "          \"min_period\": 3}\n",
    "\n",
    "metric = AutoSeasonality(test_context, params)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.AutoStationarity import AutoStationarity\n",
    "\n",
    "params = {\"max_order\": 3,\n",
    "          \"threshold\": 0.05}\n",
    "\n",
    "metric = AutoStationarity(test_context, params)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.RollingStatsPlot import RollingStatsPlot\n",
    "\n",
    "params = {\"window_size\": 4}\n",
    "\n",
    "metric = RollingStatsPlot(test_context, params)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.AutoAR import AutoAR\n",
    "\n",
    "params = {\"max_ar_order\": 2}\n",
    "\n",
    "metric = AutoAR(test_context, params)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.AutoMA import AutoMA\n",
    "\n",
    "params = {\"max_ar_order\": 2}\n",
    "\n",
    "metric = AutoMA(test_context, params)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.test_plans.describe_plan(\"time_series_multivariate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ScatterPlot import ScatterPlot\n",
    "\n",
    "metric = ScatterPlot(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.LaggedCorrelationHeatmap import LaggedCorrelationHeatmap\n",
    "\n",
    "params = {\"target_col\": target_column,\n",
    "          \"independent_vars\": feature_columns}\n",
    "\n",
    "metric = LaggedCorrelationHeatmap(test_context, params)\n",
    "#metric.run()\n",
    "# await metric.result.log()\n",
    "#metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.EngleGrangerCoint import EngleGrangerCoint\n",
    "\n",
    "params = {\"threshold\": 0.05}\n",
    "\n",
    "metric = EngleGrangerCoint(test_context, params)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.SpreadPlot import SpreadPlot\n",
    "\n",
    "metric = SpreadPlot(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Create X_train, y_train \n",
    "y_train = df_train[target_column]\n",
    "X_train = df_train.drop(target_column, axis=1)\n",
    "\n",
    "# Add constant to X_train for intercept term\n",
    "X_train = sm.add_constant(X_train)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Update df_test\n",
    "y_test = df_test[target_column]\n",
    "X_test = df_test.drop(target_column, axis=1)\n",
    "X_test = sm.add_constant(X_test)\n",
    "df_test = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Define the model\n",
    "model = sm.OLS(y_train, X_train)\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Print out the statistics\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Non-Significant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['GDPC1', 'CPIAUCSL']\n",
    "df_train.drop(columns = features_to_drop, inplace=True)\n",
    "\n",
    "# Update df_test \n",
    "df_test.drop(columns = features_to_drop, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Model Fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_train and y_train, X_test and y_test\n",
    "y_train = df_train[target_column]\n",
    "X_train = df_train.drop(target_column, axis=1)\n",
    "\n",
    "# Define the model\n",
    "model = sm.OLS(y_train, X_train)\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Print out the statistics\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create ValidMind Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update VM datasets\n",
    "vm_train_ds = vm.init_dataset(dataset=df_train, type=\"generic\", target_column=target_column)\n",
    "vm_test_ds = vm.init_dataset(dataset=df_test, type=\"generic\", target_column=target_column)\n",
    "\n",
    "# Create VM model\n",
    "vm_model = vm.init_model(\n",
    "    model = model_fit, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.test_plans.describe_plan(\"regression_model_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.DatasetSplit import DatasetSplit\n",
    "\n",
    "test_context = TestContext(model=vm_model)\n",
    "\n",
    "metric = DatasetSplit(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.ModelMetadata import ModelMetadata\n",
    "\n",
    "\n",
    "metric = ModelMetadata(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.test_plans.describe_plan(\"regression_models_evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs import RegressionModelsCoeffs\n",
    "\n",
    "test_context = TestContext(models=[vm_model])\n",
    "metric = RegressionModelsCoeffs(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsPerformance import RegressionModelsPerformance\n",
    "\n",
    "test_context = TestContext(models=[vm_model])\n",
    "metric = RegressionModelsPerformance(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.test_plans.describe_plan(\"time_series_forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelForecastPlotLevels import RegressionModelForecastPlotLevels\n",
    "\n",
    "test_context = TestContext(models=[vm_model])\n",
    "\n",
    "params = {\"transformation\": \"integrate\"}\n",
    "\n",
    "metric = RegressionModelForecastPlotLevels(test_context)\n",
    "metric.run()\n",
    "# await metric.result.log()\n",
    "metric.result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
