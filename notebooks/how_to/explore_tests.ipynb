{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Tests in the Developer Framework:\n",
    "\n",
    "## A Comprehensive Guide to Finding and Viewing Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this comprehensive guide to the ValidMind Developer Framework `tests` module! In this notebook, we'll dive deep into the utilities available for viewing and understanding the various tests that ValidMind provides. Whether you're just getting started or looking for advanced tips, you'll find clear examples and explanations to assist you every step of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we delve into the details, let's import the `describe_test` and `list_tests` functions from the `validmind.tests` module. These are the two functions that can be used to easily filter through tests and view details for individual tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import describe_test, list_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing All Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `list_tests` function provides a convenient way to retrieve all available tests in the `validmind.tests` module. When invoked without any parameters, it returns a pandas DataFrame containing detailed information about each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tags and Task Types\n",
    "\n",
    "Effectively using ValidMind's tests involves a deep understanding of its 'tags' and 'task types'. Here's a breakdown:\n",
    "\n",
    "- **Task Types**: Represent the kind of modeling task associated with a test. For instance:\n",
    "  - **classification:** Works with Classification Models and Datasets\n",
    "  - **regression:** Works with Regression Models and Datasets\n",
    "  - **text classification:** Works with Text Classification Models and Datasets\n",
    "  - **text summarization:** Works with Text Summarization Models and Datasets\n",
    "\n",
    "- **Tags**: Free-form descriptors providing more details about the test, what data and models the test is compatible with and what category the test falls into etc. Some examples include:\n",
    "  - **llm:** Tests that work with Large Language Models\n",
    "  - **nlp:** Tests relevant for natural language processing.\n",
    "  - **binary_classification:** Tests for binary classification tasks.\n",
    "  - **forecasting:** Tests for forecasting and time-series analysis.\n",
    "  - **tabular_data:** Tests for tabular data like CSVs and Excel spreadsheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Specific Tests using `tags` and `task_types`\n",
    "\n",
    "While listing all tests is valuable, there are times when you need to narrow down your search. The `list_tests` function offers `filter`, `task`, and `tags` parameters to assist in this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're targeting a specific test or tests that match a particular task type, the `filter` parameter comes in handy. For example, to list tests that are compatible with 'sklearn' models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tests(filter=\"sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `task` parameter is designed for pinpointing tests that align with a specific task type. For instance, to find tests tailored for 'classification' tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tests(task=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tags` parameter facilitates searching tests by their tags. For instance, if you're interested in only tests associated designed for `model_performance` that produce a plot (denoted by the `visualization` tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tests(tags=[\"model_performance\", \"visualization\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above parameters can be combined to create complex queries. For instance, to find tests that are compatible with 'sklearn' models, designed for 'classification' tasks, and produce a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tests(tags=[\"model_performance\", \"visualization\", \"sklearn\"], task=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatic Use\n",
    "\n",
    "To work with a specific set of tests programmatically, you can store the results in a variable. For instance, let's list all tests that are designed for Text Summarization tests and store them in `text_summarization_tests` for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summarization_tests = list_tests(task=\"text_summarization\", pretty=False)\n",
    "text_summarization_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delving into Test Details with `describe_test`\n",
    "\n",
    "After identifying a set of potential tests, you might want to explore the specifics of an individual test. The `describe_test` function provides a deep dive into the details of a test. It reveals the test name, description, ID, test type, and required inputs. Below, we showcase how to describe a test using its ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "describe_test(\"validmind.model_validation.sklearn.OverfitDiagnosis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "By harnessing the functionalities presented in this guide, you should be able to easily list and filter through all of ValidMind's available tests and find those you are interested in running against your model and/or dataset. The next step is to take the IDs of the tests you'd like to run and either create a Test Suite for reuse or just run them directly to try them out. See the other notebooks for a tutorial on how to do both."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
