{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Custom Metrics and Threshold Tests\n",
    "\n",
    "Custom metrics offer added flexibility by extending the default metrics provided by ValidMind, enabling you to document any type of model or use case. Both metrics and threshold tests assess models but they differ in approach: _metrics_ measure a range of dataset or model behaviors, while _threshold tests_ yield a pass or fail result based on specific criteria. These instructions include the code required to:\n",
    "\n",
    "- Create a metric class signature\n",
    "- Implement a custom metric\n",
    "- Test the custom metric\n",
    "- Add a `summary()` method to the custom metric\n",
    "- Add figures to a metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation components of a metric and threshold test\n",
    "\n",
    "A **metric** is composed of the following documentation elements:\n",
    "\n",
    "- Title\n",
    "- Description\n",
    "- Results Table(s)\n",
    "- Plot(s)\n",
    "\n",
    "A **threshold test** is composed of the following documentation elements:\n",
    "\n",
    "- Title\n",
    "- Description\n",
    "- Test Parameters\n",
    "- Results Table(s)\n",
    "- Plot(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "::: {.callout-tip}\n",
    "### New to ValidMind? \n",
    "To access the ValidMind Platform UI, you'll need an account.\n",
    "\n",
    "Signing up is FREE â€” **[Create your account](https://app.prod.validmind.ai)**.\n",
    ":::\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "Every documentation project in the Platform UI comes with a _code snippet_ that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place. \n",
    "\n",
    "Get your code snippet by creating a documentation project:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. Go to **Documentation Projects** and click **Create new project**.\n",
    "\n",
    "3. Select **`[Demo] Customer Churn Model`** and **`Initial Validation`** for the model name and type, give the project a unique  name to make it yours, and then click **Create project**.\n",
    "\n",
    "4. Go to **Documentation Projects** > **YOUR_UNIQUE_PROJECT_NAME** > **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace with code snippet from your documentation project ##\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"...\",\n",
    "  api_key = \"...\",\n",
    "  api_secret = \"...\",\n",
    "  project = \"...\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a metric class signature\n",
    "\n",
    "In order to implement a custom metric or threshold test, you must create a class that inherits from the `Metric` or `ThresholdTest` class. The class signatures below show the different methods that need to be implemented in order to provide the required documentation elements:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class ExampleMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    # Markdown compatible description of the metric\n",
    "    def description(self):\n",
    "\n",
    "    # Code to compute the metric and cache its results and Figures\n",
    "    def run(self):\n",
    "\n",
    "    # Code to build a list of ResultSummaries that form the results tables\n",
    "    def summary(self, metric_values):\n",
    "```\n",
    "\n",
    "We'll now implement a sample metric to illustrate their different documentation components."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a custom metric\n",
    "\n",
    "The following example shows how to implement a custom metric that calculates the mean of a list of numbers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic metric implementation\n",
    "\n",
    "At its most basic, a metric implementation requires a `run()` method that computes the metric and caches its results and Figures. The run() method is called by the ValidMind client when the metric is executed. The `run()` should return any value that can be serialized to JSON.\n",
    "\n",
    "In the example below we also provide a simple description for the metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from validmind.vm_models import Metric\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    def description(self):\n",
    "        return \"Calculates the mean of the provided values\"\n",
    "\n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "\n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "        return self.cache_results(metric_value={\"Mean\": mean})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the custom metric\n",
    "\n",
    "We should run a metric first without running an entire test suite and test its behavior.\n",
    "\n",
    "The only requirement to run a metric is build a `TestContext` object and pass it to the metric initializer. Test context objects allow metrics and tests to access data inside their class methods in a predictable way. By default, ValidMind provides support for the following special keys in a test context objects:\n",
    "\n",
    "- `dataset`\n",
    "- `model`\n",
    "- `models`\n",
    "\n",
    "When a test context object is build with one of these keys, the corresponding value is automatically added to the object as an attribute. For example, if you build a test context object with the `dataset` key, you can access the dataset inside the metric's `run()` method as `self.dataset`. We'll illustrate this in detail in the next section.\n",
    "\n",
    "In our simple example, we don't need to pass any arguments to the `TestContext` initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_id=\"MeanMetric\", context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also inspect the results of the metric by accessing the `result` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a `summary()` method to the custom metric\n",
    "\n",
    "The `summary()` method is used to build a `ResultSummary` object that can display the results of our test as a list of one or more summray tables. The `ResultSummary` class takes a `results` argument that is a list of `ResultTable` objects.\n",
    "\n",
    "Each `ResultTable` object is composed of a `data` and `metadata` attribute. The `data` attribute is any valid Pandas tabular DataFrame and `metadata` is a `ResultTableMetadata` instance that takes `title` as the table description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "from validmind.vm_models import Metric, ResultSummary, ResultTable, ResultTableMetadata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    def description(self):\n",
    "        return \"Calculates the mean of the provided values\"\n",
    "\n",
    "    def summary(self, metric_value):\n",
    "        # Create a dataframe structure that can be rendered as a table\n",
    "        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n",
    "\n",
    "        return ResultSummary(\n",
    "            results=[\n",
    "                ResultTable(\n",
    "                    data=simple_df,\n",
    "                    metadata=ResultTableMetadata(title=\"Example Table\"),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "\n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "        return self.cache_results(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_id=\"Mean\", context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add figures to a metric\n",
    "\n",
    "You can also add figures to a metric by passing a `figures` list to `cache_results()`. Each figure is a `Figure` object that takes the following arguments:\n",
    "\n",
    "- `for_object`: The name of the object that the figure is for. Usually defaults to `self`\n",
    "- `figure`: A Matplotlib or Plotly figure object\n",
    "- `key`: A unique key for the figure\n",
    "\n",
    "The developer framework uses `for_object` and `key` to link figures to the corresponding metric or test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from validmind.vm_models import Figure, Metric, ResultSummary, ResultTable, ResultTableMetadata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    def description(self):\n",
    "        return \"Calculates the mean of the provided values\"\n",
    "\n",
    "    def summary(self, metric_value):\n",
    "        # Create a dataframe structure that can be rendered as a table\n",
    "        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n",
    "\n",
    "        return ResultSummary(\n",
    "            results=[\n",
    "                ResultTable(\n",
    "                    data=simple_df,\n",
    "                    metadata=ResultTableMetadata(title=\"Example Table\"),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "\n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "\n",
    "        # Create a random histogram with matplotlib\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n",
    "        ax.set_title(\"Histogram of random numbers\")\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Do this if you want to prevent the figure from being displayed\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        figure = Figure(\n",
    "            for_object=self,\n",
    "            key=self.key,\n",
    "            figure=fig\n",
    "        )\n",
    "\n",
    "        return self.cache_results(mean, figures=[figure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models import TestPlan\n",
    "\n",
    "\n",
    "class MyCustomTestPlan(TestPlan):\n",
    "    \"\"\"\n",
    "    Custom test suite\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"my_custom_test_suite\"\n",
    "    required_inputs = []\n",
    "    tests = [MeanMetric]\n",
    "\n",
    "\n",
    "my_custom_test_suite = MyCustomTestPlan(config={\n",
    "    \"mean_of_values\": {\n",
    "        \"values\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    },\n",
    "})\n",
    "results = my_custom_test_suite.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-1QuffXMV-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
