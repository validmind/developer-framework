{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load predictions in ValidMind datasets\n",
                "\n",
                "This notebook guides you through loading predictions in ValidMind dataset objects using the `assign_predictions()` function. The function is designed to enable developers to support various way to load predictions in the dataset object so that tests can make use of it.\n",
                "\n",
                "This guide includes the code required to:\n",
                "\n",
                "- Load the demo dataset\n",
                "- Prepocess the raw dataset and Train a model for testing\n",
                "- Initialize ValidMind objects\n",
                "- Options to load predictions using the developer frameworks\n",
                "  - Load predictions from a file\n",
                "  - Link an existing prediction column in the dataset with a model\n",
                "  - Let the developer framework run predictions and link them to a model\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Install the client library\n",
                "\n",
                "The client library provides Python support for the ValidMind Developer Framework. To install it:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "6G5-kHOZ7YWk"
            },
            "outputs": [],
            "source": [
                "%pip install -q validmind"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "_ZqI8W5jSPc1"
            },
            "source": [
                "## Initialize the client library\n",
                "\n",
                "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
                "\n",
                "Get your code snippet:\n",
                "\n",
                "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
                "\n",
                "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
                "\n",
                "3. Enter the model details and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
                "\n",
                "   For example, to register a model for use with this notebook, select:\n",
                "\n",
                "   - Documentation template: `Binary classification`\n",
                "   - Use case: `Marketing/Sales - Attrition/Churn Management`\n",
                "\n",
                "   You can fill in other options according to your preference.\n",
                "\n",
                "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
                "\n",
                "Next, replace this placeholder with your own code snippet:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "5hqGn9jHSPc2"
            },
            "outputs": [],
            "source": [
                "# Replace with your code snippet\n",
                "\n",
                "import validmind as vm\n",
                "\n",
                "vm.init(\n",
                "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
                "    api_key=\"...\",\n",
                "    api_secret=\"...\",\n",
                "    project=\"...\",\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preview the documentation template\n",
                "\n",
                "A template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\n",
                "\n",
                "You will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the `vm.preview_template()` function from the ValidMind library and note the empty sections:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vm.preview_template()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "WT4iDaNPSPc4"
            },
            "source": [
                "## Load the sample dataset\n",
                "\n",
                "The sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), a two-dimensional tabular data structure that makes use of rows and columns:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "BlNanClPSPc5"
            },
            "outputs": [],
            "source": [
                "# Import the sample dataset from the library\n",
                "\n",
                "from validmind.datasets.classification import customer_churn as demo_dataset\n",
                "\n",
                "print(\n",
                "    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}' \\n\\t• Class labels: {demo_dataset.class_labels}\"\n",
                ")\n",
                "\n",
                "raw_df = demo_dataset.load_data()\n",
                "raw_df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "XNI6mCy0SPc6"
            },
            "source": [
                "## Prepocess the raw dataset\n",
                "\n",
                "Preprocessing performs a number of operations to get ready for the subsequent steps:\n",
                "\n",
                "- Preprocess the data: Splits the DataFrame (`df`) into multiple datasets (`train_df`, `validation_df`, and `test_df`) using `demo_dataset.preprocess` to simplify preprocessing.\n",
                "- Separate features and targets: Drops the target column to create feature sets (`x_train`, `x_val`) and target sets (`y_train`, `y_val`).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df, validation_df, test_df = demo_dataset.preprocess(raw_df)\n",
                "x_train = train_df.drop(demo_dataset.target_column, axis=1)\n",
                "y_train = train_df[demo_dataset.target_column]\n",
                "x_val = validation_df.drop(demo_dataset.target_column, axis=1)\n",
                "y_val = validation_df[demo_dataset.target_column]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train models for testing\n",
                "\n",
                "- Initialize XGBoost and Logistic Regression Classifiers\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "PMeDVcpsSPc7"
            },
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "import xgboost\n",
                "\n",
                "%matplotlib inline\n",
                "\n",
                "xgb = xgboost.XGBClassifier(early_stopping_rounds=10)\n",
                "xgb.set_params(\n",
                "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
                ")\n",
                "xgb.fit(\n",
                "    x_train,\n",
                "    y_train,\n",
                "    eval_set=[(x_val, y_val)],\n",
                "    verbose=False,\n",
                ")\n",
                "\n",
                "lr = LogisticRegression(random_state=0)\n",
                "lr.fit(\n",
                "    x_train,\n",
                "    y_train,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize ValidMind objects\n",
                "\n",
                "### Initialize the ValidMind models\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vm_model_xgb = vm.init_model(\n",
                "    xgb,\n",
                "    input_id=\"xgb\",\n",
                ")\n",
                "vm_model_lr = vm.init_model(\n",
                "    lr,\n",
                "    input_id=\"lr\",\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "DTO0bN4qSPc7"
            },
            "source": [
                "### Initialize the ValidMind datasets\n",
                "\n",
                "Before you can run tests, you must first initialize a ValidMind dataset object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\n",
                "\n",
                "This function takes a number of arguments:\n",
                "\n",
                "- `dataset` — the raw dataset that you want to provide as input to tests\n",
                "- `input_id` - a unique identifier that allows tracking what inputs are used when running each individual test\n",
                "- `target_column` — a required argument if tests require access to true values. This is the name of the target column in the dataset\n",
                "- `class_labels` — an optional value to map predicted classes to class labels\n",
                "\n",
                "With all datasets ready, you can now initialize the raw, training and test datasets (`raw_df`, `train_df` and `test_df`) created earlier into their own dataset objects using [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vm_raw_ds = vm.init_dataset(\n",
                "    input_id=\"raw_dataset\",\n",
                "    dataset=raw_df,\n",
                "    target_column=demo_dataset.target_column,\n",
                ")\n",
                "\n",
                "vm_train_ds = vm.init_dataset(\n",
                "    input_id=\"train_dataset\",\n",
                "    dataset=train_df,\n",
                "    target_column=demo_dataset.target_column,\n",
                ")\n",
                "vm_test_ds = vm.init_dataset(\n",
                "    input_id=\"test_dataset\", dataset=test_df, target_column=demo_dataset.target_column\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Options to load predictions using the developer frameworks\n",
                "\n",
                "### 1. Load predictions from a file\n",
                "\n",
                "This creates a new column called `<model_id>_prediction` in the dataset and assigns metadata to track that the `<model_id>_prediction` column is linked to the model `<model_id>`\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Predictions calculated outside of VM\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "train_xgb_prediction = pd.DataFrame(xgb.predict(x_train), columns=[\"xgb_prediction\"])\n",
                "test__xgb_prediction = pd.DataFrame(xgb.predict(x_val), columns=[\"xgb_prediction\"])\n",
                "\n",
                "train_lr_prediction = pd.DataFrame(lr.predict(x_train), columns=[\"lr_prediction\"])\n",
                "test_lr_prediction = pd.DataFrame(lr.predict(x_val), columns=[\"lr_prediction\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Assign predictions to the training dataset\n",
                "\n",
                "We can now use the `assign_predictions()` method from the `Dataset` object to link existing predictions to any model:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vm_train_ds.assign_predictions(\n",
                "    model=vm_model_xgb, prediction_values=train_xgb_prediction.xgb_prediction.values\n",
                ")\n",
                "vm_train_ds.assign_predictions(\n",
                "    model=vm_model_lr, prediction_values=train_lr_prediction.lr_prediction.values\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Run an example test\n",
                "\n",
                "Now, let's run an example test such as `MinimumAccuracy` twice to show how we're able to load the correct model predictions by using the `model` input parameter, even though we're passing the same `train_ds` dataset instance to the test:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_suite = vm.tests.run_test(\n",
                "    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n",
                "    inputs={\"dataset\": vm_train_ds, \"model\": vm_model_xgb},\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_suite = vm.tests.run_test(\n",
                "    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n",
                "    inputs={\n",
                "        \"dataset\": vm_train_ds,\n",
                "        \"model\": vm_model_lr,\n",
                "    },\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Link an existing prediction column in the dataset with a model\n",
                "\n",
                "This approach allows loading datasets that already have prediction columns in addition to feature and target columns. The developer framework assigns metadata to track the predictions column that are linked to a given `<vm_model>` model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df2 = train_df.copy()\n",
                "train_df2[\"xgb_prediction\"] = train_xgb_prediction.xgb_prediction.values\n",
                "train_df2[\"lr_prediction\"] = train_lr_prediction.lr_prediction.values\n",
                "train_df2.head(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_columns = [\n",
                "    \"CreditScore\",\n",
                "    \"Gender\",\n",
                "    \"Age\",\n",
                "    \"Tenure\",\n",
                "    \"Balance\",\n",
                "    \"NumOfProducts\",\n",
                "    \"HasCrCard\",\n",
                "    \"IsActiveMember\",\n",
                "    \"EstimatedSalary\",\n",
                "    \"Geography_France\",\n",
                "    \"Geography_Germany\",\n",
                "    \"Geography_Spain\",\n",
                "]\n",
                "\n",
                "vm_train_ds = vm.init_dataset(\n",
                "    dataset=train_df2,\n",
                "    input_id=\"train_dataset\",\n",
                "    target_column=demo_dataset.target_column,\n",
                "    feature_columns=feature_columns,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Link prediction column to a specific model\n",
                "\n",
                "The `prediction_column` parameter informs the `Dataset` object about the model that should be linked to that column.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vm_train_ds.assign_predictions(model=vm_model_xgb, prediction_column=\"xgb_prediction\")\n",
                "vm_train_ds.assign_predictions(model=vm_model_lr, prediction_column=\"lr_prediction\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "wE0OckXjSPc7"
            },
            "outputs": [],
            "source": [
                "full_suite = vm.tests.run_test(\n",
                "    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n",
                "    inputs={\"dataset\": vm_train_ds, \"model\": vm_model_xgb},\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_suite = vm.tests.run_test(\n",
                "    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n",
                "    inputs={\"dataset\": vm_train_ds, \"model\": vm_model_lr},\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Link an existing prediction column in the dataset with a model\n",
                "\n",
                "This lets the developer framework run model predictions, creates a new column called `<model_id>_prediction`, and assign metadata to track that the `<model_id>_prediction` column is linked to the `<vm_model>` model.\n",
                "\n",
                "There are two ways run and assign model predictions with the developer framework:\n",
                "\n",
                "- When initializing a `Dataset` with `init_dataset()`. This is the most straightforward method to assign predictions for a single model.\n",
                "- Using `dataset.assign_predictions()`. This allows assigning predictions to a dataset for one or more models.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.1 Pass `<vm_model>` in dataset interface\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_columns = [\n",
                "    \"CreditScore\",\n",
                "    \"Gender\",\n",
                "    \"Age\",\n",
                "    \"Tenure\",\n",
                "    \"Balance\",\n",
                "    \"NumOfProducts\",\n",
                "    \"HasCrCard\",\n",
                "    \"IsActiveMember\",\n",
                "    \"EstimatedSalary\",\n",
                "    \"Geography_France\",\n",
                "    \"Geography_Germany\",\n",
                "    \"Geography_Spain\",\n",
                "]\n",
                "\n",
                "vm_train_ds = vm.init_dataset(\n",
                "    model=vm_model_xgb,\n",
                "    dataset=train_df,\n",
                "    input_id=\"train_dataset\",\n",
                "    target_column=demo_dataset.target_column,\n",
                "    feature_columns=feature_columns,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3.2 Through `assign_predictions` interface\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vm_train_ds = vm.init_dataset(\n",
                "    dataset=train_df,\n",
                "    input_id=\"train_dataset\",\n",
                "    target_column=demo_dataset.target_column,\n",
                "    feature_columns=feature_columns,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### Perform predictions using the same `assign_predictions` interface\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vm_train_ds.assign_predictions(model=vm_model_xgb)\n",
                "vm_train_ds.assign_predictions(model=vm_model_lr)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Run an example test\n",
                "\n",
                "Now, let's run an example test such as `MinimumAccuracy` twice to show how we're able to load the correct model predictions by using the `model` input parameter, even though we're passing the same `train_ds` dataset instance to the test:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_suite = vm.tests.run_test(\n",
                "    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n",
                "    inputs={\"dataset\": vm_train_ds, \"model\": vm_model_xgb},\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_suite = vm.tests.run_test(\n",
                "    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n",
                "    inputs={\n",
                "        \"dataset\": vm_train_ds,\n",
                "        \"model\": vm_model_lr,\n",
                "    },\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "gpuClass": "standard",
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
