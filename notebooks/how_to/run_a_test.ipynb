{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running an Individual Test\n",
        "This notebook shows how to run individual metrics or thresholds tests that is part of the ValidMind Developer Framework. These instructions include the code required to:\n",
        "\n",
        "- Load a demo dataset\n",
        "- Prepocess the raw dataset\n",
        "- Train a model for testing\n",
        "- Set up test inputs and run the test\n",
        "- Initialize the required ValidMind objects\n",
        "- Run the test\n",
        "- Log the test results to ValidMind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you begin\n",
        "\n",
        "::: {.callout-tip}\n",
        "### New to ValidMind? \n",
        "For access to all features available in this notebook, create a free ValidMind account. \n",
        "\n",
        "Signing up is FREE â€” [**Sign up now**](https://app.prod.validmind.ai)\n",
        ":::\n",
        "\n",
        "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install the client library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the client library\n",
        "\n",
        "Every documentation project in the Platform UI comes with a _code snippet_ that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place. \n",
        "\n",
        "Get your code snippet by creating a documentation project:\n",
        "\n",
        "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
        "\n",
        "2. Go to **Documentation Projects** and click **Create new project**.\n",
        "\n",
        "3. Select **`[Demo] Customer Churn Model`** and **`Initial Validation`** for the model name and type, give the project a unique  name to make it yours, and then click **Create project**.\n",
        "\n",
        "4. Go to **Documentation Projects** > **YOUR_UNIQUE_PROJECT_NAME** > **Getting Started** and click **Copy snippet to clipboard**.\n",
        "\n",
        "Next, replace this placeholder with your own code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Replace with code snippet from your documentation project ##\n",
        "\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n",
        "  api_key = \"...\",\n",
        "  api_secret = \"...\",\n",
        "  project = \"...\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the demo dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.datasets.classification import customer_churn as demo_dataset\n",
        "\n",
        "# You can also import customer_churn like this:\n",
        "# from validmind.datasets.classification import taiwan_credit as demo_dataset\n",
        "\n",
        "df = demo_dataset.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepocess the raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, validation_df, test_df = demo_dataset.preprocess(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a model for testing\n",
        "\n",
        "We train a simple customer churn model for our test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = train_df.drop(demo_dataset.target_column, axis=1)\n",
        "y_train = train_df[demo_dataset.target_column]\n",
        "x_val = validation_df.drop(demo_dataset.target_column, axis=1)\n",
        "y_val = validation_df[demo_dataset.target_column]\n",
        "\n",
        "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
        "model.set_params(\n",
        "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
        ")\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    eval_set=[(x_val, y_val)],\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up test inputs and run the test\n",
        "\n",
        "### Initialize ValidMind objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm_train_ds = vm.init_dataset(\n",
        "    input_id=\"train_dataset\",\n",
        "    dataset=train_df,\n",
        "    type=\"generic\",\n",
        "    target_column=demo_dataset.target_column\n",
        ")\n",
        "\n",
        "vm_test_ds = vm.init_dataset(\n",
        "    input_id=\"test_dataset\",\n",
        "    dataset=test_df,\n",
        "    type=\"generic\",\n",
        "    target_column=demo_dataset.target_column\n",
        ")\n",
        "\n",
        "vm_model = vm.init_model(\n",
        "    model,\n",
        "    input_id=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assign predictions to the datasets\n",
        "We can now use the `assign_predictions()` method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm_train_ds.assign_predictions(\n",
        "    model=vm_model,\n",
        "    prediction_values=list(model.predict(x_train))\n",
        ")\n",
        "vm_test_ds.assign_predictions(\n",
        "    model=vm_model,\n",
        "    prediction_values=list(model.predict(x_val))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the test\n",
        "\n",
        "Individual tests can be easily run by calling the `run_test` function provided by the `validmind.tests` module. The function takes the following arguments:\n",
        "\n",
        "- `test_id`: The ID of the test to run. To find a particular test and get its, refer to the `explore_tests.ipynb` notebook\n",
        "- `params`: A dictionary of parameters for the test. These will override any `default_params` set in the test definition. Refer to the `explore_tests.ipynb` notebook to find the default parameters for a test.\n",
        "\n",
        "You can then pass in any inputs for the test as keyword arguments. Most likely, these will be `dataset` and `model` objects. Again, you may refer to the `explore_tests.ipynb` notebook to find the required inputs for a test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "test = vm.tests.run_test(\n",
        "    test_id=\"validmind.model_validation.sklearn.TrainingTestDegradation\",\n",
        "    params={}, # can be used to set overrides to the test's default parameters\n",
        "    inputs = {\n",
        "        \"model\":vm_model,\n",
        "        \"datasets\": (vm_train_ds, vm_test_ds)\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Log the test results to ValidMind\n",
        "\n",
        "After the test has been run, you can save the results to ValidMind by calling the `log` method of the test object returned after running the test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.log()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "validmind-1QuffXMV-py3.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
