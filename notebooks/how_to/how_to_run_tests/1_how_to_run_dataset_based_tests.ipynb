{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run Dataset Based Tests\n",
    "\n",
    "The ValidMind developer framework provides a `run_test` function that allows you to run built-in or custom tests that take any dataset or model as input. These tests generate outputs in the form of text, tables and images that get populated in model documentation.\n",
    "\n",
    "In this notebook, we will take you through the process of learning how to find tests, understand how to initialize a ValidMind dataset and pass it to the `run_test` function, for any test that takes a `dataset` input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Requisites\n",
    "\n",
    "We recommended that you have gone through the [explore_tests.ipynb](../explore_tests.ipynb) notebook to understand the basics of how to find and describe all the available tests in the developer framework.\n",
    "\n",
    "## High-Level Steps\n",
    "\n",
    "- Listing and filtering available tests\n",
    "- Building a sample dataset\n",
    "- Understanding how to initialize a VM dataset\n",
    "- Running a test with the sample dataset\n",
    "- Running a test that accepts parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "::: {.callout-tip}\n",
    "\n",
    "### New to ValidMind?\n",
    "\n",
    "For access to all features available in this notebook, create a free ValidMind account.\n",
    "\n",
    "Signing up is FREE — [**Sign up now**](https://app.prod.validmind.ai)\n",
    ":::\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "Get your code snippet:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
    "\n",
    "3. Enter the model details, making sure to select **Binary classification** as the template and **Marketing/Sales - Attrition/Churn Management** as the use case, and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
    "\n",
    "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    project=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing and filtering available tests\n",
    "\n",
    "Before we run a test, let's find a suitable metric for this demonstration. Let's assume you want to generate the pearson correlation matrix for a dataset. In the [explore_tests.ipynb](../explore_tests.ipynb) notebook we saw how to pass a `filter` to the `list_tests` function. Let's do the same here to find the test ID for the pearson correlation matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.list_tests(filter=\"PearsonCorrelationMatrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, you can see that the test ID for the pearson correlation matrix is `validmind.data_validation.PearsonCorrelationMatrix`. The `describe_test` function gives you more information about the test, including its **Required Inputs**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = \"validmind.data_validation.PearsonCorrelationMatrix\"\n",
    "vm.tests.describe_test(test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this test requires a dataset, it should throw an error if you were to run it without passing a `dataset` input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vm.tests.run_test(test_id)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sample dataset\n",
    "\n",
    "Now, let's build a sample dataset so you can generate its pearson correlation matrix. The sklearn `make_classification` function can generate a random dataset for testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=10,\n",
    "    weights=[0.1],\n",
    "    random_state=42,\n",
    ")\n",
    "X.shape\n",
    "y.shape\n",
    "\n",
    "df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "df[\"target\"] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to initialize a ValidMind dataset\n",
    "\n",
    "ValidMind dataset objects provide a wrapper to any type of dataset (NumPy, Pandas, Polars, etc.) so that tests can run transparently regardless of the underlying library. A VM dataset object can be created using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\n",
    "\n",
    "This function takes a number of arguments:\n",
    "\n",
    "- `dataset` — the raw dataset that you want to provide as input to tests\n",
    "- `input_id` - a unique identifier that allows tracking what inputs are used when running each individual test\n",
    "- `target_column` — a required argument if tests require access to true values. This is the name of the target column in the dataset\n",
    "\n",
    "Below you can see how to initialize a VM dataset for the sample `df` you created previously:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_dataset = vm.init_dataset(\n",
    "    df,\n",
    "    input_id=\"my_demo_dataset\",\n",
    "    target_column=\"target\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now call `run_test` with the new `vm_dataset` object as input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    test_id,\n",
    "    inputs={\"dataset\": vm_dataset},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset can also be used for any other test that requires a dataset input. Let's try to find a \"class imbalance\" to understand the distribution of the target column in the dataset.\n",
    "\n",
    "We'll use `list_tests` again to showcase how to filter tests for tabular data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vm.tests.list_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.list_tests(tags=[\"binary_classification\", \"tabular_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test ID for the class imbalance test is `validmind.data_validation.ClassImbalance`. If you describe this test you will find that it also accepts some parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.describe_test(\"validmind.data_validation.ClassImbalance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `min_percent_threshold` will allow you configure the threshold for an acceptable class imbalance. Let's run the test without any parameters to see its output using a default value for the threshold. We also call the `log` method on the result to send the results of the tests to the ValidMind platform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    \"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_dataset},\n",
    ")\n",
    "\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test passes the pass-fail criteria with the default threshold of 10%. Let's try to run the test with a threshold of 20% to see if it fails. Notice the use of the \"custom_threshold\" `result_id` in the test ID. This allows you to submit individual results for the same test to the platform, as we'll see in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    \"validmind.data_validation.ClassImbalance:custom_threshold\",\n",
    "    inputs={\"dataset\": vm_dataset},\n",
    "    params={\"min_percent_threshold\": 20},\n",
    ")\n",
    "\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding test results to documentation\n",
    "\n",
    "The previous result shows that the test doesn't pass the threshold of 20% for class imbalance. With these results logged, you can now add them to your model documentation. To do this, go to the documentation page of the model you connected to above and navigate to Data Preparation -> Data Description section. Then hover between any existing content block to reveal the + button as shown in the screenshot below.\n",
    "\n",
    "![screenshot showing insert button for test-driven blocks](../../images/insert-test-driven-block.png)\n",
    "\n",
    "Now click on the + button and select the Test-Driven Block option. This will open a dialog where you can select `Threshold Test` as the type of test and the `Class Imbalance Custom Threshold Test` from the list of available metrics. You can preview the result and then click Insert Block to add it to the documentation.\n",
    "\n",
    "![screenshot showing custom class imbalance result](../../images/insert-test-driven-block-custom-class-imbalance.jpg)\n",
    "\n",
    "The test should match the result you see above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In the next notebook, you will learn how to run tests that require a dataset and model as inputs. This will allow you to generate documentation for model evaluation metrics such as ROC-AUC, F1 score, etc. for your model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
