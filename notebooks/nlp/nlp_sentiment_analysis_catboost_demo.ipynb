{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis\n",
    "## Natural Language Processing Analysis & Binary Classification using CatBoost\n",
    "This notebook aims to provide an introduction to documenting an NLP model using the ValidMind Developer Framework. The use case presented is a sentiment analysis of tweets related to COVID-19 into \"positive\" and \"negative\"; the model is a binary text classification using the CatBoost library.\n",
    "\n",
    "We will train a sample model and demonstrate the following documentation functionalities:\n",
    "\n",
    "- Initializing the ValidMind Developer Framework\n",
    "- Using a sample datasets provided by the library to train a simple nlp classification model using CatBoost library\n",
    "- Running a test various tests to quickly generate document about the data and model\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "To use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\n",
    "\n",
    "If you don't already have one, you should also [create a documentation project](https://docs.validmind.ai/guide/create-your-first-documentation-project.html) on the ValidMind platform. You will use this project to upload your documentation and test results.\n",
    "\n",
    "## Install the client library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "In a browser, go to the **Client Integration** page of your documentation project and click **Copy to clipboard** next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n",
    "\n",
    "::: {.column-margin}\n",
    "::: {.callout-tip}\n",
    "This step requires a documentation project. [Learn how you can create one](https://docs.validmind.ai/guide/create-your-first-documentation-project.html).\n",
    ":::\n",
    ":::\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace with code snippet from your documentation project ##\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    project=\"...\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explorary Data Analysis of Covid tweets data\n",
    "The emphasis in this section is on the in-depth analysis and preprocessing of the text data (tweets). In this section, we introduce the manually tagged COVID-19 tweets, which range from Highly Negative to Highly Positive, representing five distinct classes. In this Exploratory Data Analysis (EDA), these five classes will be simplified to two classes: Positive and Negative.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env PYTORCH_MPS_HIGH_WATERMARK_RATIO 0.8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "train_model = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load covid-19 tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.datasets.nlp import twitter_covid_19 as demo_data\n",
    "df = demo_data.load_data()\n",
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run text data quality test suite\n",
    "In this section we use the ValidMind Developer Framework to run various data quality checks on the dataset, and send the results to the model document on the ValidMind Platform UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_ds = vm.init_dataset(dataset=df, type=\"generic\", text_column='OriginalTweet', target_column=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"class_imbalance\":{\"min_percent_threshold\": 3}\n",
    "}\n",
    "text_data_test_suite = vm.run_test_suite(\"text_data_quality\",\n",
    "                                       dataset=vm_ds,\n",
    "                                       config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle class bias \n",
    "One way to handle class bias is to merge a specific class data with related class. \n",
    "Here, we will copy the text and class lables in separate columns so that the original text is also there for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Classes:\", df.Sentiment.unique())\n",
    "\n",
    "df['text'] = df.OriginalTweet\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "def classes_def(x):\n",
    "    if x ==  \"Extremely Positive\":\n",
    "        return \"positive\"\n",
    "    elif x == \"Extremely Negative\":\n",
    "        return \"negative\"\n",
    "    elif x == \"Negative\":\n",
    "        return \"negative\"\n",
    "    elif x ==  \"Positive\":\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "    \n",
    "df['sentiment']=df['Sentiment'].apply(lambda x:classes_def(x))\n",
    "target=df['sentiment']\n",
    "\n",
    "print(df.sentiment.value_counts(normalize= True))\n",
    "print(\"Modified Classes:\", df.sentiment.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove neutral class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"sentiment\"] != \"neutral\"]\n",
    "print(df.sentiment.unique())\n",
    "print(df.sentiment.value_counts(normalize= True))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove urls and html links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Urls and HTML links\n",
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', text)\n",
    "\n",
    "df['text']=df['text'].apply(lambda x:remove_urls(x))\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "df['text']=df['text'].apply(lambda x:remove_html(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lower case \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing\n",
    "def lower(text):\n",
    "    low_text= text.lower()\n",
    "    return low_text\n",
    "df['text']=df['text'].apply(lambda x:lower(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number removal\n",
    "def remove_num(text):\n",
    "    remove= re.sub(r'\\d+', '', text)\n",
    "    return remove\n",
    "df['text']=df['text'].apply(lambda x:remove_num(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "df['text']=df['text'].apply(lambda x:remove_stopwords(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuations\n",
    "\n",
    "def punct_remove(text):\n",
    "    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "    return punct\n",
    "df['text']=df['text'].apply(lambda x:punct_remove(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove mentions \n",
    "def remove_mention(x):\n",
    "    text=re.sub(r'@\\w+','',x)\n",
    "    return text\n",
    "df['text']=df['text'].apply(lambda x:remove_mention(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hashtags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove hashtags \n",
    "\n",
    "def remove_hash(x):\n",
    "    text=re.sub(r'#\\w+','',x)\n",
    "    return text\n",
    "df['text']=df['text'].apply(lambda x:remove_hash(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra white space left while removing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove extra white space left while removing stuff\n",
    "def remove_space(text):\n",
    "    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n",
    "    return space_remove\n",
    "df['text']=df['text'].apply(lambda x:remove_space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run text data quality tests again\n",
    "Here, we are checking the quality of the data again by running data quality tests so verify that we have preprocess data well and tests are passing according to our requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_ds = vm.init_dataset(dataset=df, type=\"generic\", text_column='text', target_column=\"sentiment\")\n",
    "\n",
    "config = {\n",
    "    \"class_imbalance\":{\"min_percent_threshold\": 3}\n",
    "}\n",
    "text_data_test_suite = vm.run_test_suite(\"text_data_quality\",\n",
    "                                       dataset=vm_ds,\n",
    "                                       config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, validation, test\n",
    "\n",
    "With our data in nice shape, we'll split it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df['sentiment'] != \"neutral\"]\n",
    "df.loc[df['sentiment'] == \"positive\", 'sentiment'] = 1\n",
    "df.loc[df['sentiment'] == \"negative\", 'sentiment'] = 0\n",
    "print(np.unique(df['sentiment']))\n",
    "\n",
    "print(df.head())\n",
    "train, test = train_test_split(df[['text','sentiment']], test_size=0.33, random_state=42)\n",
    "train = train[['text','sentiment']]\n",
    "test = test[['text','sentiment']]\n",
    "\n",
    "train, valid = train_test_split(\n",
    "    train,\n",
    "    train_size=0.7,\n",
    "    random_state=0,\n",
    "    stratify=train['sentiment'])\n",
    "y_train, X_train = \\\n",
    "    train['sentiment'], train.drop(['sentiment'], axis=1)\n",
    "y_valid, X_valid = \\\n",
    "    valid['sentiment'], valid.drop(['sentiment'], axis=1)\n",
    "y_test, X_test= \\\n",
    "    test['sentiment'], test.drop(['sentiment'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train, y_train,val_data, **kwargs):\n",
    "    model = CatBoostClassifier(\n",
    "        task_type='CPU',\n",
    "        iterations=5000,\n",
    "        eval_metric='Accuracy',\n",
    "        od_type='Iter',\n",
    "        od_wait=500,\n",
    "        **kwargs\n",
    "    )\n",
    "    return model.fit(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        eval_set=val_data,\n",
    "        verbose=100,\n",
    "        plot=True,\n",
    "        use_best_model=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_model(\n",
    "    X_train, y_train,\n",
    "    val_data=(X_valid,y_valid),\n",
    "    text_features=['text'],\n",
    "    learning_rate=0.35,\n",
    "    tokenizers=[\n",
    "        {\n",
    "            'tokenizer_id': 'Sense',\n",
    "            'separator_type': 'BySense',\n",
    "            'lowercasing': 'True',\n",
    "            'token_types':['Word', 'Number', 'SentenceBreak'],\n",
    "            'sub_tokens_policy':'SeveralTokens'\n",
    "        }      \n",
    "    ],\n",
    "    dictionaries = [\n",
    "        {\n",
    "            'dictionary_id': 'Word',\n",
    "            'max_dictionary_size': '5000'\n",
    "        }\n",
    "    ],\n",
    "    feature_calcers = [\n",
    "        'BoW:top_tokens_count=10000'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize validmind objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds = vm.init_dataset(dataset=pd.concat([X_train, y_train], axis=1), type=\"generic\", target_column=\"sentiment\")\n",
    "vm_test_ds = vm.init_dataset(dataset=pd.concat([X_test, y_test], axis=1), type=\"generic\",target_column=\"sentiment\")\n",
    "vm_model = vm.init_model(model, train_ds=vm_train_ds, test_ds=vm_test_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model metrics test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_test_suite = vm.run_test_suite(\"binary_classifier_metrics\", \n",
    "                                             model=vm_model\n",
    "                                            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model validation test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validation_test_suite = vm.run_test_suite(\"binary_classifier_validation\", \n",
    "                                             model=vm_model\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev Framework 3.9.16",
   "language": "python",
   "name": "dev-framework-3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
