{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of a RAG Model for RFP Question Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU qdrant-client lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise Exception(\"OPENAI_API_KEY not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from IPython.display import HTML, display\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def _format_cell_text(text, width=50):\n",
    "    \"\"\"Private function to format a cell's text.\"\"\"\n",
    "    return \"\\n\".join([textwrap.fill(line, width=width) for line in text.split(\"\\n\")])\n",
    "\n",
    "\n",
    "def _format_dataframe_for_tabulate(df):\n",
    "    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Format all string columns\n",
    "    for column in df_out.columns:\n",
    "        # Check if column is of type object (likely strings)\n",
    "        if df_out[column].dtype == object:\n",
    "            df_out[column] = df_out[column].apply(_format_cell_text)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def _dataframe_to_html_table(df):\n",
    "    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n",
    "    headers = df.columns.tolist()\n",
    "    table_data = df.values.tolist()\n",
    "    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n",
    "\n",
    "\n",
    "def display_nice(df, num_rows=None):\n",
    "    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n",
    "    if num_rows is not None:\n",
    "        df = df.head(num_rows)\n",
    "    formatted_df = _format_dataframe_for_tabulate(df)\n",
    "    html_table = _dataframe_to_html_table(formatted_df)\n",
    "    display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_keys(data, indent=0):\n",
    "    for key, value in data.items():\n",
    "        print(' ' * indent + str(key))\n",
    "        if isinstance(value, dict):  # if the value is another dictionary, recurse\n",
    "            print_dict_keys(value, indent + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing RFPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV file paths\n",
    "existing_rfp_paths = [\n",
    "    \"datasets/rag/rfp_existing_questions_client_2.csv\",\n",
    "]\n",
    "\n",
    "existing_rfp_df = [pd.read_csv(file_path) for file_path in existing_rfp_paths]\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "existing_rfp_df = pd.concat(existing_rfp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rfp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Iterate through each file path in the list\n",
    "for file_path in existing_rfp_paths:\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path,\n",
    "        metadata_columns=[\"Area\"]\n",
    "    )\n",
    "\n",
    "    # Load a document from the current CSV file\n",
    "    doc = loader.load()\n",
    "    \n",
    "    # Append documents\n",
    "    documents.extend(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `CSVLoader`, each document represents a single row and includes its respective contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 5\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    print(f\"Document {i + 1}: {document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the page content of each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 2\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    print(f\"Page content for document {i + 1}:\")\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when adding metadata, it is appended to the default metadata, which consists of the row number and the source: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 5\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    print(f\"Metadata for document {i + 1}: {document.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=10, add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some general information about the chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the length of the bigger and smaller chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chunk_length = max([len(chunk.page_content) for chunk in chunks])\n",
    "min_chunk_length = min([len(chunk.page_content) for chunk in chunks])\n",
    "mean_chunk_length = sum([len(chunk.page_content) for chunk in chunks]) / len(chunks)\n",
    "\n",
    "print(f\"Maximum chunk length: {max_chunk_length}\")\n",
    "print(f\"Minimum chunk length: {min_chunk_length}\")\n",
    "print(f\"Mean chunk length: {mean_chunk_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Calculate lengths of each chunk's page_content\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "# Creating a histogram of chunk lengths\n",
    "fig = px.histogram(chunk_lengths, nbins=50, title=\"Distribution of Chunk Lengths\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Chunk Length\",\n",
    "    yaxis_title=\"Count\",\n",
    "    bargap=0.2,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add summary statistics as text on the plot\n",
    "fig.add_annotation(\n",
    "    x=max(chunk_lengths),\n",
    "    y=0,\n",
    "    showarrow=False,\n",
    "    yshift=10\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for index, chunk in enumerate(chunks[:i]):\n",
    "    print(f\"Chunk {index + 1}: {chunk}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the page content of each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5\n",
    "\n",
    "for i, document in enumerate(chunks[:number_of_chunks]):\n",
    "    print(f\"Page content for chunk {i + 1}:\")\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the metadata for individual chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for i, chunk in enumerate(chunks[:number_of_chunks]):\n",
    "    print(f\"Metadata for chunk {i + 1}: {chunk.metadata}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the source of each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for i, chunk in enumerate(chunks[:number_of_chunks]):\n",
    "    print(f\"Source for chunk {i + 1}: {chunk.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store chunks into a vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all RFPs into a single pandas DataFrame\n",
    "\n",
    "rag_evaluation_df = pd.read_csv(\"datasets/rag/rag_evaluation_dataset_01.csv\")\n",
    "\n",
    "# Set the constant variable to the number of rows in the DataFrame\n",
    "NUM_OF_NEW_RFP_QUESTIONS = len(rag_evaluation_df)\n",
    "\n",
    "print(\"Number of New RFP Questions:\", NUM_OF_NEW_RFP_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df.info()\n",
    "rag_evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. \n",
    "If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# Step 1: \"question\": Retrieved from the \"question\" key.\n",
    "# Step 2: \"context\": Retrieved from the \"question\" key and fed into the retriever.\n",
    "# Step 3: \"context\": Assigned to a RunnablePassthrough object using the \"context\" key from the previous step.\n",
    "# Step 4: \"answer\": \"context\" and \"question\" are combined to format the prompt, then sent to the LLM and stored under the \"answer\" key.\n",
    "# Step 5: \"context\": Repopulated using the \"context\" key from the previous step.\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    \n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"answer\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a question to test the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Find a similar question as this one: 'What is your experience in developing AI-based applications?'\"\n",
    "response = rag_chain.invoke({\"question\" : question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As defined in the earlier chat prompt, the RAG response includes two fields: `answer` and `context`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict_keys(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the answer, we see that the `rag_chain` is functioning correctly and identifies the most similar question in the `vectorstore`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question:\")\n",
    "print(question)\n",
    "print()\n",
    "print(f\"Answer:\")\n",
    "print(response[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we inspect the content of the `answer` and the `context` retrieved based on the `question`. The context should contain `k` chunks, the most relevant based on the question. Remember that we set`k` in the `retriever` earlier. These `k` chunks are pasted into the prompt as text, informing the LLM to generate an answer that is closer in the embedding space to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for i, chunk in enumerate(response[\"context\"][:number_of_chunks]):\n",
    "    print(f\"Content for chunk {i + 1}:\")  # i + 1 to start counting from 1 instead of 0\n",
    "    print(chunk.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now inspect the `response_metadata` object to understand its contents and identify what could be useful to incorporate in our RAG evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"answer\"].response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict_keys(response[\"answer\"].response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the LLM used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: {response['answer'].response_metadata['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we showed earlier, we can also extract some token usage statistics that can help us understand and optimize our interactions with the language model for cost-effectiveness and efficiency.\n",
    "\n",
    "- **Prompt tokens**: tokens that form the input text sent to the language model. This includes all the text provided to the LLM to generate a response.\n",
    "- **Completion tokens**: number of tokens in the generated text or output from the model.\n",
    "- **Total tokens**: total number of tokens processed by the model. It is the sum of both `prompt_tokens` and `completion_tokens`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Completion tokens: {response['answer'].response_metadata['token_usage']['completion_tokens']}\")\n",
    "print(f\"Prompt tokens: {response['answer'].response_metadata['token_usage']['prompt_tokens']}\")\n",
    "print(f\"Total tokens: {response['answer'].response_metadata['token_usage']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now expand our evaluation dataset to capture some metadata generated by the LLM, which will be used later when validating our RAG pipeline. We will add the following additional columns to our dataframe: `context`, `model_name`, `completion_tokens`, prompt_tokens, and `total_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df['context'] = ''\n",
    "\n",
    "rag_evaluation_df['question_embeddings'] = ''\n",
    "rag_evaluation_df['answer_embeddings'] = ''\n",
    "rag_evaluation_df['context_embeddings'] = ''\n",
    "\n",
    "rag_evaluation_df['similarity_score_question_vs_context'] = ''\n",
    "rag_evaluation_df['similarity_score_question_vs_answer'] = ''\n",
    "rag_evaluation_df['similarity_score_context_vs_answer'] = ''\n",
    "\n",
    "rag_evaluation_df['model'] = ''\n",
    "\n",
    "rag_evaluation_df['completion_tokens'] = ''\n",
    "rag_evaluation_df['prompt_tokens'] = ''\n",
    "rag_evaluation_df['total_tokens'] = ''\n",
    "\n",
    "rag_evaluation_df['response_time'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to also compute few similarity metrics between embeddings such as cosine similaruty or euclidean distance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_score(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding1 (array-like): Embedding of the first entity.\n",
    "    - embedding2 (array-like): Embedding of the second entity.\n",
    "\n",
    "    Returns:\n",
    "    - float: Cosine similarity score between the two embeddings.\n",
    "\n",
    "    Note: The order of the embeddings does not affect the result as cosine similarity is symmetric.\n",
    "    \"\"\"\n",
    "    # Ensure the embeddings are reshaped to 2D arrays for sklearn's cosine_similarity\n",
    "    embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "    embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "\n",
    "    # Calculate and return the cosine similarity\n",
    "    return cosine_similarity(embedding1, embedding2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between two embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding1 (array-like): First embedding vector.\n",
    "    - embedding2 (array-like): Second embedding vector.\n",
    "\n",
    "    Returns:\n",
    "    - float: Euclidean distance between the two embeddings.\n",
    "    \"\"\"\n",
    "    # Convert inputs to NumPy arrays if they aren't already\n",
    "    embedding1 = np.array(embedding1)\n",
    "    embedding2 = np.array(embedding2)\n",
    "    \n",
    "    # Calculate and return the Euclidean distance\n",
    "    return np.linalg.norm(embedding1 - embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Number of questions to process by the RAG model\n",
    "number_of_rows_to_process = NUM_OF_NEW_RFP_QUESTIONS\n",
    "\n",
    "for i, (index, row) in enumerate(rag_evaluation_df.iloc[:number_of_rows_to_process].iterrows()):\n",
    "    print(f\"Processing row {i}...\")\n",
    "\n",
    "    # Check if the 'answer' field is 'None' (as a string) for the current row\n",
    "    if row[\"answer\"] == \"None\":\n",
    "        print(f\"Answer is 'None' for question ID {index}. Invoking RAG model...\")\n",
    "\n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        # Invoke the RAG model with the question from the current row\n",
    "        response = rag_chain.invoke({\"question\": row[\"question_to_llm\"]})\n",
    "\n",
    "        end_time = time.time()  # End timing\n",
    "\n",
    "        # Calculate the response time and store it\n",
    "        rag_evaluation_df.at[index, 'response_time'] = round(end_time - start_time, 1)\n",
    "\n",
    "        # Store whatever response comes from the LLM\n",
    "        rag_evaluation_df.at[index, \"answer\"] = response[\"answer\"].content\n",
    "        print(f\"Question ID {index} answer updated with the response from the RAG model.\")\n",
    "    \n",
    "        # Store the context included in the prompt\n",
    "        context = \"\\n\\n\".join(chunk.page_content for chunk in response[\"context\"])\n",
    "        rag_evaluation_df.at[index, \"context\"] = context\n",
    "        \n",
    "        # Compute and store embeddings for the question, context and answer\n",
    "        print(\"Computing embeddings for the question...\")\n",
    "        question_embeddings = np.array(embeddings_model.embed_query(row[\"question_to_llm\"]))\n",
    "        rag_evaluation_df.at[index, 'question_embeddings'] = question_embeddings\n",
    "        \n",
    "        print(\"Computing embeddings for the context...\")\n",
    "        context_embeddings = np.array(embeddings_model.embed_query(context))\n",
    "        rag_evaluation_df.at[index, 'context_embeddings'] = context_embeddings\n",
    "        \n",
    "        print(\"Computing embeddings for the answer...\")\n",
    "        answer_embeddings = np.array(embeddings_model.embed_query(response[\"answer\"].content))\n",
    "        rag_evaluation_df.at[index, 'answer_embeddings'] = answer_embeddings\n",
    "        \n",
    "        # Compute similarity measures between embeddings \n",
    "        print(\"Computing cosine similarity between question and context...\")\n",
    "        rag_evaluation_df.at[index, 'similarity_score_question_vs_context'] = cosine_similarity_score(question_embeddings, context_embeddings)\n",
    "        \n",
    "        print(\"Computing cosine similarity between question and answer...\")\n",
    "        rag_evaluation_df.at[index, 'similarity_score_question_vs_answer'] = cosine_similarity_score(question_embeddings, answer_embeddings)\n",
    "\n",
    "        print(\"Computing cosine similarity between context and answer...\")\n",
    "        rag_evaluation_df.at[index, 'similarity_score_context_vs_answer'] = cosine_similarity_score(context_embeddings, answer_embeddings)\n",
    "        \n",
    "        # Store some metadata such as model name and tokens statistics\n",
    "        rag_evaluation_df.at[index, \"model\"] = response[\"answer\"].response_metadata[\"model_name\"]\n",
    "        rag_evaluation_df.at[index, \"completion_tokens\"] = response['answer'].response_metadata['token_usage']['completion_tokens']\n",
    "        rag_evaluation_df.at[index, \"prompt_tokens\"] = response['answer'].response_metadata['token_usage']['prompt_tokens']\n",
    "        rag_evaluation_df.at[index, \"total_tokens\"] = response['answer'].response_metadata['token_usage']['total_tokens']\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check if all responses have been generated by the RAG pipeline or if there are any `None` values in the answers column. If there are any rows with `None` answers, remove these before they are passed to the RAGAS metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df = rag_evaluation_df[rag_evaluation_df['answer'] != 'None']\n",
    "rag_evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the results in a CSV file for convenience to avoid having to execute the entire RAG pipeline every time we want to test our RAG evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "rag_evaluation_df.to_csv('datasets/rag/rag_evaluation_dataset_02.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to evaluate our RAG pipeline using RAGAS metrics from the `ragas` package. The `evaluate()` function expects a Dataset with specific column names: `question`, `contexts`, `ground_truth`, and `answer`. We will now rename these columns to conform to the expected column names in RAGAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataframe for RAGAS evaluation\n",
    "ragas_results_df = rag_evaluation_df.copy()\n",
    "\n",
    "# Rename the columns to match ragas convention\n",
    "ragas_results_df.rename(\n",
    "    columns={\n",
    "        \"question_to_llm\": \"question\",\n",
    "        \"context\": \"contexts\"}, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the 'contexts' column from a string to a list of strings for each row\n",
    "ragas_results_df['contexts'] = ragas_results_df['contexts'].apply(lambda x: [x])\n",
    "\n",
    "ragas_results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ],\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the RAGAS evaluation metrics row by row, adding the results to corresponding columns for each metric in our evaluation dataset. We first initialize the columns where the evaluation metrics will be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df['context_precision'] = ''\n",
    "ragas_results_df['faithfulness'] = ''\n",
    "ragas_results_df['answer_relevancy'] = ''\n",
    "ragas_results_df['context_recall'] = ''\n",
    "ragas_results_df['context_relevancy'] = ''\n",
    "ragas_results_df['answer_correctness'] = ''\n",
    "ragas_results_df['answer_similarity'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "required_fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "metrics = [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\", \"context_relevancy\", \"answer_correctness\", \"answer_similarity\"]\n",
    "\n",
    "# Set the variable to the number of rows, limited to a maximum of NUM_OF_NEW_RFP_QUESTIONS\n",
    "number_of_rows_to_process = min(len(ragas_results_df), NUM_OF_NEW_RFP_QUESTIONS)\n",
    "\n",
    "# Mapping of metric names to their respective functions, assuming these functions are predefined\n",
    "metrics_functions = {\n",
    "    \"context_precision\": context_precision,\n",
    "    \"faithfulness\": faithfulness,\n",
    "    \"answer_relevancy\": answer_relevancy,\n",
    "    \"context_recall\": context_recall,\n",
    "    \"context_relevancy\": context_relevancy,\n",
    "    \"answer_correctness\": answer_correctness,\n",
    "    \"answer_similarity\": answer_similarity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This loop processes each row up to a predefined number of rows, evaluating them with specified metrics and storing the results\n",
    "for i, (index, row) in enumerate(rag_evaluation_df.iloc[:number_of_rows_to_process].iterrows()):\n",
    "    print(f\"Processing RFP question {i+1}...\")\n",
    "    print(f\"Question: {ragas_results_df.iloc[i]['question']}\")\n",
    "    print(f\"Answer: {ragas_results_df.iloc[i]['answer']}\")\n",
    "\n",
    "    # Create a temporary Dataset for the current row\n",
    "    ragas_dataset = Dataset.from_pandas(ragas_results_df.iloc[i: i + 1][required_fields])\n",
    "\n",
    "    # Evaluate using RAGAS metrics\n",
    "    evaluation_result = evaluate(\n",
    "        ragas_dataset, \n",
    "        [metrics_functions[metric] for metric in metrics if metric in metrics_functions])\n",
    "    print(\"Evaluation completed.\")\n",
    "\n",
    "    # Store evaluation results back into the DataFrame\n",
    "    for metric in metrics:\n",
    "        if metric in evaluation_result:\n",
    "            ragas_results_df.at[i, metric] = evaluation_result[metric]\n",
    "            print(f\"{metric}: {evaluation_result[metric]}\")\n",
    "\n",
    "print(\"All RFP questions processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "ragas_results_df.to_csv('datasets/rag/rag_evaluation_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-mI3jzOkk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
