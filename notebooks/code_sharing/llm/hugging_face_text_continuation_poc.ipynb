{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summarization of Financial Data Using Hugging Face LLMs\n",
        "This notebook aims to provide an introduction to documenting an LLM model using the ValidMind Developer Framework. The use case presented is a summarization of financial data (https://huggingface.co/datasets/financial_phrasebank).\n",
        "\n",
        "- Initializing the ValidMind Developer Framework\n",
        "- Running a test various tests to quickly generate document about the data and model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you begin\n",
        "\n",
        "::: {.callout-tip}\n",
        "### New to ValidMind? \n",
        "For access to all features available in this notebook, create a free ValidMind account. \n",
        "\n",
        "Signing up is FREE â€” [**Sign up now!**](https://app.prod.validmind.ai)\n",
        ":::\n",
        "\n",
        "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Install the client library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " the client library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install --upgrade validmind"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the client library\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "Get your code snippet:\n",
        "\n",
        "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
        "\n",
        "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
        "\n",
        "3. Enter the model details, making sure to select **NLP-based Text Summarization** as the template and **Marketing/Sales - Analytics** as the use case, and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
        "\n",
        "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
        "\n",
        "Next, replace this placeholder with your own code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace with your code snippet\n",
        "\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    project=\"...\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display, HTML\n",
        "from rouge import Rouge\n",
        "import plotly.graph_objects as go\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import torch\n",
        "import string\n",
        "import plotly.express as px\n",
        "import plotly.subplots as sp\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_toxic_prompt_data(file_path, threshold=None):\n",
        "    \"\"\"\n",
        "    Reads a .jsonl file and returns a dataframe with all records.\n",
        "\n",
        "    :param file_path: Path to the .jsonl file.\n",
        "    :param threshold: Optional toxicity threshold. If provided, only records with a toxicity\n",
        "                      score exceeding this threshold will be returned.\n",
        "    :return: DataFrame with all records.\n",
        "    \"\"\"\n",
        "    # Load records\n",
        "    records = []\n",
        "    with open(file_path, \"r\") as file:\n",
        "        for line in file:\n",
        "            data = json.loads(line)\n",
        "            records.append(data)\n",
        "\n",
        "    # Prepare data for dataframe\n",
        "    data_list = []\n",
        "    for record in records:\n",
        "        prompt_toxicity = record[\"prompt\"].get(\"toxicity\", None)\n",
        "        cont_toxicity = record[\"continuation\"].get(\"toxicity\", None)\n",
        "\n",
        "        if threshold:\n",
        "            if not prompt_toxicity or not cont_toxicity:\n",
        "                continue  # If we don't have a toxicity value, skip the record\n",
        "            elif prompt_toxicity < threshold and cont_toxicity < threshold:\n",
        "                continue  # If neither toxicity value exceeds the threshold, skip the record\n",
        "\n",
        "        row = {\n",
        "            \"Filename\": record[\"filename\"],\n",
        "            \"Begin\": record[\"begin\"],\n",
        "            \"End\": record[\"end\"],\n",
        "            \"Challenging\": record[\"challenging\"],\n",
        "            \"Prompt Text\": record[\"prompt\"][\"text\"],\n",
        "            \"Prompt Profanity\": record[\"prompt\"].get(\"profanity\", \"N/A\"),\n",
        "            \"Prompt Sexually Explicit\": record[\"prompt\"].get(\"sexually_explicit\", \"N/A\"),\n",
        "            \"Prompt Identity Attack\": record[\"prompt\"].get(\"identity_attack\", \"N/A\"),\n",
        "            \"Prompt Flirtation\": record[\"prompt\"].get(\"flirtation\", \"N/A\"),\n",
        "            \"Prompt Threat\": record[\"prompt\"].get(\"threat\", \"N/A\"),\n",
        "            \"Prompt Insult\": record[\"prompt\"].get(\"insult\", \"N/A\"),\n",
        "            \"Prompt Severe Toxicity\": record[\"prompt\"].get(\"severe_toxicity\", \"N/A\"),\n",
        "            \"Prompt Toxicity\": record[\"prompt\"].get(\"toxicity\", \"N/A\"),\n",
        "            \"Continuation Text\": record[\"continuation\"][\"text\"],\n",
        "            \"Cont. Severe Toxicity\": record[\"continuation\"].get(\"severe_toxicity\", \"N/A\"),\n",
        "            \"Cont. Toxicity\": record[\"continuation\"].get(\"toxicity\", \"N/A\"),\n",
        "            \"Cont. Profanity\": record[\"continuation\"].get(\"profanity\", \"N/A\"),\n",
        "            \"Cont. Sexually Explicit\": record[\"continuation\"].get(\"sexually_explicit\", \"N/A\"),\n",
        "            \"Cont. Identity Attack\": record[\"continuation\"].get(\"identity_attack\", \"N/A\"),\n",
        "            \"Cont. Flirtation\": record[\"continuation\"].get(\"flirtation\", \"N/A\"),\n",
        "            \"Cont. Threat\": record[\"continuation\"].get(\"threat\", \"N/A\"),\n",
        "            \"Cont. Insult\": record[\"continuation\"].get(\"insult\", \"N/A\")\n",
        "        }\n",
        "        data_list.append(row)\n",
        "\n",
        "    # Convert list of dicts to dataframe\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _format_cell_text(text, width=50):\n",
        "    \"\"\"Private function to format a cell's text.\"\"\"\n",
        "    return '\\n'.join([textwrap.fill(line, width=width) for line in text.split('\\n')])\n",
        "\n",
        "\n",
        "def _format_dataframe_for_tabulate(df):\n",
        "    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Format all string columns\n",
        "    for column in df_out.columns:\n",
        "        # Check if column is of type object (likely strings)\n",
        "        if df_out[column].dtype == object:\n",
        "            df_out[column] = df_out[column].apply(_format_cell_text)\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def _dataframe_to_html_table(df):\n",
        "    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n",
        "    headers = df.columns.tolist()\n",
        "    table_data = df.values.tolist()\n",
        "    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n",
        "\n",
        "\n",
        "def display_formatted_dataframe(df, num_rows=None):\n",
        "    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n",
        "    if num_rows is not None:\n",
        "        df = df.head(num_rows)\n",
        "    formatted_df = _format_dataframe_for_tabulate(df)\n",
        "    html_table = _dataframe_to_html_table(formatted_df)\n",
        "    display(HTML(html_table))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_summaries_to_df(df, summaries):\n",
        "    \"\"\"\n",
        "    Adds a new column 'summary_X' to the dataframe df that contains the given summaries, where X is an incremental number.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The original pandas DataFrame.\n",
        "    - summaries: List/array of summarized texts.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with an additional summary column, with 'labels' being the first column followed by the original 'text'.\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()  # Make an explicit copy of the DataFrame\n",
        "\n",
        "    # Check if the length of summaries matches the number of rows in the DataFrame\n",
        "    if len(summaries) != len(df):\n",
        "        raise ValueError(\n",
        "            f\"The number of summaries ({len(summaries)}) does not match the number of rows in the DataFrame ({len(df)}).\")\n",
        "\n",
        "    # Determine the name for the new summary column\n",
        "    col_index = 1\n",
        "    col_name = 'summary_1'\n",
        "    while col_name in df.columns:\n",
        "        col_index += 1\n",
        "        col_name = f'summary_{col_index}'\n",
        "\n",
        "    # Add the summaries to the DataFrame\n",
        "    df[col_name] = summaries\n",
        "\n",
        "    # Rearrange the DataFrame columns to have 'topic' first, then the original 'input', followed by summary columns\n",
        "    summary_columns = [col for col in df.columns if col.startswith('summary')]\n",
        "    other_columns = [col for col in df.columns if col not in summary_columns\n",
        "                     + ['topic', 'input', 'reference_summary']]\n",
        "\n",
        "    columns_order = ['topic', 'input', 'reference_summary'] + \\\n",
        "        sorted(summary_columns) + other_columns\n",
        "    df = df[columns_order]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### POC Validation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "toxicity = evaluate.load(\"toxicity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "def hf_toxicity_plot(df, params):\n",
        "    \"\"\"\n",
        "    Compute toxicity scores for texts and then plot line plots for input and generated texts\n",
        "    where the generated text score surpasses the given threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The dataframe containing texts.\n",
        "    - params (dict): Parameters containing toxicity evaluation object, column names, and generated score threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract necessary parameters\n",
        "    toxicity = params[\"hf_toxicity_obj\"]\n",
        "    input_text_col = params[\"input_text_col\"]\n",
        "    generated_text_col = params[\"generated_text_col\"]\n",
        "    generated_score_threshold = params.get(\n",
        "        \"generated_score_threshold\", 0)  # default to 0 if not provided\n",
        "\n",
        "    # Get list of texts from dataframe\n",
        "    input_texts = df[input_text_col].tolist()\n",
        "    generated_texts = df[generated_text_col].tolist()\n",
        "\n",
        "    # Compute toxicity for input texts and generated texts\n",
        "    input_toxicity_scores = toxicity.compute(predictions=input_texts)['toxicity']\n",
        "    generated_toxicity_scores = toxicity.compute(\n",
        "        predictions=generated_texts)['toxicity']\n",
        "\n",
        "    # Filter records where the generated score is above the threshold\n",
        "    indices = [i for i, gen_score in enumerate(\n",
        "        generated_toxicity_scores) if gen_score > generated_score_threshold]\n",
        "\n",
        "    input_toxicity_scores = [score for i, score in enumerate(\n",
        "        input_toxicity_scores) if i in indices]\n",
        "    generated_toxicity_scores = [score for i, score in enumerate(\n",
        "        generated_toxicity_scores) if i in indices]\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add traces for the scores with modified line width\n",
        "    fig.add_trace(go.Scatter(x=indices, y=input_toxicity_scores, mode='lines+markers', name='Input Text',\n",
        "                             line=dict(width=1)))  # Set width to 1 for a thinner line\n",
        "    fig.add_trace(go.Scatter(x=indices, y=generated_toxicity_scores, mode='lines+markers', name='Generated Text',\n",
        "                             line=dict(width=1)))  # Set width to 1 for a thinner line\n",
        "\n",
        "    # Add a trace for the threshold to appear in the legend (without actual data)\n",
        "    fig.add_trace(go.Scatter(x=[None], y=[None], mode='lines', name=f'Threshold ({generated_score_threshold})',\n",
        "                             line=dict(color=\"grey\", width=0.5, dash=\"dash\")))\n",
        "\n",
        "    # Add a horizontal line for the threshold\n",
        "    fig.add_shape(\n",
        "        go.layout.Shape(\n",
        "            type=\"line\",\n",
        "            x0=min(indices) if indices else 0,\n",
        "            x1=max(indices) if indices else 1,\n",
        "            y0=generated_score_threshold,\n",
        "            y1=generated_score_threshold,\n",
        "            line=dict(color=\"grey\", width=0.8, dash=\"dash\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(title=\"Toxicity Scores for Input and Generated Texts with Generated Score above threshold\",\n",
        "                      xaxis_title=\"Index\",\n",
        "                      yaxis_title=\"Toxicity Score\",\n",
        "                      legend_title=\"Text Type\")\n",
        "\n",
        "    # Show figure\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hf_toxicity_table(df, params):\n",
        "    \"\"\"\n",
        "    Update and return dataframe with toxicity scores for prompt and continuation texts.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The dataframe containing texts.\n",
        "    - params (dict): Parameters containing toxicity evaluation object, column names, and the max and min generated toxicity thresholds.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Updated dataframe with toxicity scores.\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()  # Create a deep copy of the DataFrame\n",
        "\n",
        "    # Extract necessary parameters\n",
        "    toxicity = params[\"hf_toxicity_obj\"]\n",
        "    input_text_col = params[\"input_text_col\"]\n",
        "    generated_text_col = params[\"generated_text_col\"]\n",
        "    max_generated_toxicity_threshold = params.get(\n",
        "        \"max_generated_toxicity_threshold\", 1)  # default to 1 if not provided\n",
        "    min_generated_toxicity_threshold = params.get(\n",
        "        \"min_generated_toxicity_threshold\", 0)  # default to 0 if not provided\n",
        "\n",
        "    # Get list of texts from dataframe\n",
        "    input_texts = df[input_text_col].tolist()\n",
        "    generated_texts = df[generated_text_col].tolist()\n",
        "\n",
        "    # Compute toxicity for input texts and generated texts\n",
        "    input_toxicity_scores = toxicity.compute(predictions=input_texts)['toxicity']\n",
        "    generated_toxicity_scores = toxicity.compute(\n",
        "        predictions=generated_texts)['toxicity']\n",
        "\n",
        "    # Assign the new toxicity scores to the dataframe using .loc to avoid the warning\n",
        "    df.loc[:, \"Input Text Toxicity\"] = input_toxicity_scores\n",
        "    df.loc[:, \"Generated Text Toxicity\"] = generated_toxicity_scores\n",
        "\n",
        "    # Filter the dataframe to return only rows where the generated text toxicity score is between the thresholds\n",
        "    df = df[(df[\"Generated Text Toxicity\"] >= min_generated_toxicity_threshold)\n",
        "            & (df[\"Generated Text Toxicity\"] <= max_generated_toxicity_threshold)]\n",
        "\n",
        "    # Order the results by \"Generated Text Toxicity\" in descending order\n",
        "    df = df.sort_values(by=\"Generated Text Toxicity\", ascending=False)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.subplots as sp\n",
        "\n",
        "\n",
        "def hf_toxicity_histograms(df, params):\n",
        "    \"\"\"\n",
        "    Compute toxicity scores for texts and then plot histograms for input and generated texts.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The dataframe containing texts.\n",
        "    - params (dict): Parameters containing toxicity evaluation object and column names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract necessary parameters\n",
        "    toxicity = params[\"hf_toxicity_obj\"]\n",
        "    input_text_col = params[\"input_text_col\"]\n",
        "    generated_text_col = params[\"generated_text_col\"]\n",
        "\n",
        "    # Get list of texts from dataframe\n",
        "    input_texts = df[input_text_col].tolist()\n",
        "    generated_texts = df[generated_text_col].tolist()\n",
        "\n",
        "    # Compute toxicity for input texts and generated texts\n",
        "    input_toxicity_scores = toxicity.compute(predictions=input_texts)['toxicity']\n",
        "    generated_toxicity_scores = toxicity.compute(\n",
        "        predictions=generated_texts)['toxicity']\n",
        "\n",
        "    # Create a subplot layout\n",
        "    fig = sp.make_subplots(rows=1, cols=2, subplot_titles=(\n",
        "        \"Input Text Toxicity\", \"Generated Text Toxicity\"))\n",
        "\n",
        "    # Add traces\n",
        "    fig.add_trace(go.Histogram(x=input_toxicity_scores,\n",
        "                  name=\"Input Text\"), row=1, col=1)\n",
        "    fig.add_trace(go.Histogram(x=generated_toxicity_scores,\n",
        "                  name=\"Generated Text\"), row=1, col=2)\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(title_text=\"Histograms of Toxicity Scores\")\n",
        "    fig.update_xaxes(title_text=\"Toxicity Score\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Toxicity Score\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\n",
        "\n",
        "    # Show figure\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First function\n",
        "def general_text_metrics(df, text_column):\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for text in df[text_column]:\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        words = nltk.word_tokenize(text)\n",
        "        paragraphs = text.split(\"\\n\\n\")\n",
        "\n",
        "        total_words = len(words)\n",
        "        total_sentences = len(sentences)\n",
        "        avg_sentence_length = round(sum(len(sentence.split(\n",
        "        )) for sentence in sentences) / total_sentences if total_sentences else 0, 1)\n",
        "        total_paragraphs = len(paragraphs)\n",
        "\n",
        "        results.append([total_words, total_sentences,\n",
        "                       avg_sentence_length, total_paragraphs])\n",
        "\n",
        "    return pd.DataFrame(results, columns=[\"Total Words\", \"Total Sentences\", \"Avg Sentence Length\", \"Total Paragraphs\"])\n",
        "\n",
        "# Second function\n",
        "\n",
        "\n",
        "def vocabulary_structure_metrics(df, text_column, unwanted_tokens, num_top_words, lang):\n",
        "    stop_words = set(word.lower() for word in stopwords.words(lang))\n",
        "    unwanted_tokens = set(token.lower() for token in unwanted_tokens)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for text in df[text_column]:\n",
        "        words = nltk.word_tokenize(text)\n",
        "\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words and word.lower(\n",
        "        ) not in unwanted_tokens and word not in string.punctuation]\n",
        "\n",
        "        total_unique_words = len(set(filtered_words))\n",
        "        total_punctuations = sum(1 for word in words if word in string.punctuation)\n",
        "        lexical_diversity = round(\n",
        "            total_unique_words / len(filtered_words) if filtered_words else 0, 1)\n",
        "\n",
        "        results.append([total_unique_words, total_punctuations, lexical_diversity])\n",
        "\n",
        "    return pd.DataFrame(results, columns=[\"Total Unique Words\", \"Total Punctuations\", \"Lexical Diversity\"])\n",
        "\n",
        "# Wrapper function that combines the outputs\n",
        "\n",
        "\n",
        "def text_description_table(df, params):\n",
        "    text_column = params[\"text_column\"]\n",
        "    unwanted_tokens = params[\"unwanted_tokens\"]\n",
        "    num_top_words = params[\"num_top_words\"]\n",
        "    lang = params[\"lang\"]\n",
        "\n",
        "    gen_metrics_df = general_text_metrics(df, text_column)\n",
        "    vocab_metrics_df = vocabulary_structure_metrics(\n",
        "        df, text_column, unwanted_tokens, num_top_words, lang)\n",
        "\n",
        "    combined_df = pd.concat([gen_metrics_df, vocab_metrics_df], axis=1)\n",
        "\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_description_histograms(df, params):\n",
        "\n",
        "    text_column = params[\"text_column\"]\n",
        "    num_docs_to_plot = params[\"num_docs_to_plot\"]\n",
        "\n",
        "    # Ensure the nltk punkt tokenizer is downloaded\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "    # Decide on the number of documents to plot\n",
        "    if not num_docs_to_plot or num_docs_to_plot > len(df):\n",
        "        num_docs_to_plot = len(df)\n",
        "\n",
        "    # Colors for each subplot\n",
        "    colors = ['blue', 'green', 'red', 'purple']\n",
        "\n",
        "    # Axis titles for clarity\n",
        "    x_titles = [\n",
        "        \"Word Frequencies\",\n",
        "        \"Sentence Position in Document\",\n",
        "        \"Sentence Lengths (Words)\",\n",
        "        \"Word Lengths (Characters)\"\n",
        "    ]\n",
        "    y_titles = [\n",
        "        \"Number of Words\",\n",
        "        \"Sentence Length (Words)\",\n",
        "        \"Number of Sentences\",\n",
        "        \"Number of Words\"\n",
        "    ]\n",
        "\n",
        "    # Iterate over each document in the DataFrame up to the user-specified limit\n",
        "    for index, (idx, row) in enumerate(df.head(num_docs_to_plot).iterrows()):\n",
        "        # Create subplots with a 2x2 grid for each metric\n",
        "        fig = sp.make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=[\n",
        "                \"Word Frequencies\",\n",
        "                \"Sentence Positions\",\n",
        "                \"Sentence Lengths\",\n",
        "                \"Word Lengths\"\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Tokenize document into sentences and words\n",
        "        sentences = nltk.sent_tokenize(row[text_column])\n",
        "        words = nltk.word_tokenize(row[text_column])\n",
        "\n",
        "        # Metrics computation\n",
        "        word_freq = Counter(words)\n",
        "        freq_counts = Counter(word_freq.values())\n",
        "        word_frequencies = list(freq_counts.keys())\n",
        "        word_frequency_counts = list(freq_counts.values())\n",
        "\n",
        "        sentence_positions = list(range(1, len(sentences) + 1))\n",
        "        sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
        "        word_lengths = [len(word) for word in words]\n",
        "\n",
        "        # Adding data to subplots\n",
        "        fig.add_trace(go.Bar(x=word_frequencies, y=word_frequency_counts,\n",
        "                      marker_color=colors[0], showlegend=False), row=1, col=1)\n",
        "        fig.add_trace(go.Bar(x=sentence_positions, y=sentence_lengths,\n",
        "                      marker_color=colors[1], showlegend=False), row=1, col=2)\n",
        "        fig.add_trace(go.Histogram(x=sentence_lengths, nbinsx=50, opacity=0.75,\n",
        "                      marker_color=colors[2], showlegend=False), row=2, col=1)\n",
        "        fig.add_trace(go.Histogram(x=word_lengths, nbinsx=50, opacity=0.75,\n",
        "                      marker_color=colors[3], showlegend=False), row=2, col=2)\n",
        "\n",
        "        # Update x and y axis titles\n",
        "        for i, (x_title, y_title) in enumerate(zip(x_titles, y_titles)):\n",
        "            fig['layout'][f'xaxis{\n",
        "                i + 1}'].update(title=x_title, titlefont=dict(size=10))\n",
        "            fig['layout'][f'yaxis{\n",
        "                i + 1}'].update(title=y_title, titlefont=dict(size=10))\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title=f\"Text Description for Document {index + 1}\",\n",
        "            barmode='overlay',\n",
        "            height=800\n",
        "        )\n",
        "\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to plot scatter plots for specified combinations using Plotly\n",
        "def text_description_scatter_plot(df, combinations_to_plot):\n",
        "\n",
        "    combinations_to_plot = params[\"combinations_to_plot\"]\n",
        "\n",
        "    for metric1, metric2 in combinations_to_plot:\n",
        "        fig = px.scatter(df, x=metric1, y=metric2,\n",
        "                         title=f\"Scatter Plot: {metric1} vs {metric2}\")\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "def token_disparity_histograms(df, params):\n",
        "    \"\"\"\n",
        "    Visualize the token counts distribution of two given columns using histograms.\n",
        "\n",
        "    :param df: DataFrame containing the text columns.\n",
        "    :param params: Dictionary with the keys [\"reference_column\", \"generated_column\"].\n",
        "    \"\"\"\n",
        "\n",
        "    reference_column = params[\"reference_column\"]\n",
        "    generated_column = params[\"generated_column\"]\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Tokenize the columns and get the number of tokens\n",
        "    df['tokens_1'] = df[reference_column].apply(lambda x: len(tokenizer.tokenize(x)))\n",
        "    df['tokens_2'] = df[generated_column].apply(lambda x: len(tokenizer.tokenize(x)))\n",
        "\n",
        "    # Create subplots: 1 row, 2 columns\n",
        "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
        "        f'Tokens in {reference_column}', f'Tokens in {generated_column}'))\n",
        "\n",
        "    # Add histograms\n",
        "    fig.add_trace(go.Histogram(x=df['tokens_1'],\n",
        "                               marker_color='blue',\n",
        "                               name=f'Tokens in {reference_column}'),\n",
        "                  row=1, col=1)\n",
        "\n",
        "    fig.add_trace(go.Histogram(x=df['tokens_2'],\n",
        "                               marker_color='red',\n",
        "                               name=f'Tokens in {generated_column}'),\n",
        "                  row=1, col=2)\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(title_text='Token Distributions',\n",
        "                      bargap=0.1)\n",
        "\n",
        "    fig.update_yaxes(title_text='Number of Documents')\n",
        "    fig.update_xaxes(title_text='Number of Tokens', row=1, col=1)\n",
        "    fig.update_xaxes(title_text='Number of Tokens', row=1, col=2)\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "def rouge_scores_plot(df, params):\n",
        "    \"\"\"\n",
        "    Compute ROUGE scores for each row in the DataFrame and visualize them.\n",
        "\n",
        "    :param df: DataFrame containing the summaries.\n",
        "    :param params: Dictionary with the keys [\"metric\", \"ref_column\", \"gen_column\"].\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract parameters\n",
        "    metric = params.get(\"metric\", \"rouge-2\")\n",
        "    ref_column = params[\"ref_column\"]\n",
        "    gen_column = params[\"gen_column\"]\n",
        "\n",
        "    if metric not in [\"rouge-1\", \"rouge-2\", \"rouge-l\", \"rouge-s\"]:\n",
        "        raise ValueError(\n",
        "            \"Invalid metric. Choose from 'rouge-1', 'rouge-2', 'rouge-l', 'rouge-s'.\")\n",
        "\n",
        "    rouge = Rouge(metrics=[metric])\n",
        "    score_list = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        scores = rouge.get_scores(row[gen_column], row[ref_column], avg=True)[metric]\n",
        "        score_list.append(scores)\n",
        "\n",
        "    df_scores = pd.DataFrame(score_list)\n",
        "\n",
        "    # Visualization part\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Adding the line plots\n",
        "    fig.add_trace(go.Scatter(x=df_scores.index,\n",
        "                  y=df_scores['p'], mode='lines+markers', name='Precision'))\n",
        "    fig.add_trace(go.Scatter(x=df_scores.index,\n",
        "                  y=df_scores['r'], mode='lines+markers', name='Recall'))\n",
        "    fig.add_trace(go.Scatter(x=df_scores.index,\n",
        "                  y=df_scores['f'], mode='lines+markers', name='F1 Score'))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"ROUGE Scores for Each Row\",\n",
        "        xaxis_title=\"Row Index\",\n",
        "        yaxis_title=\"Score\"\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hugging Face models wrappers\n",
        "\n",
        "The following code template showcases how to wrap a Hugging Face model for compatibility with the ValidMind Developer Framework. We will load an example model using the transformers API and then run some predictions on our test dataset.\n",
        "\n",
        "The ValidMind developer framework provides support for Hugging Face transformers out of the box, so in the following section we will show how to initialize multiple transformers models with the `init_model` function, removing the need for a custom wrapper. In cases where you need extra pre-processing or post-processing steps, you can use the following code template as a starting point to wrap your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TextGeneration_HuggingFace:\n",
        "    \"\"\"\n",
        "    A Model instance wrapper for text generation using HuggingFace Transformers' GPT-2.\n",
        "    \"\"\"\n",
        "    model: GPT2LMHeadModel\n",
        "    tokenizer: GPT2Tokenizer\n",
        "    model_continuations: list = None\n",
        "\n",
        "    def __init__(self, model_name=\"gpt2\", model=None, tokenizer=None):\n",
        "        pipeline_task = \"text-generation\"\n",
        "        self.model_name = model_name\n",
        "\n",
        "        if model and tokenizer:\n",
        "            self.model = pipeline(pipeline_task, model=model, tokenizer=tokenizer)\n",
        "            self.tokenizer = tokenizer\n",
        "        else:\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "            self.model = pipeline(pipeline_task, model=model_name,\n",
        "                                  tokenizer=self.tokenizer)\n",
        "\n",
        "        self.model_continuations = []\n",
        "\n",
        "    def predict(self, prompts, params):\n",
        "        \"\"\"\n",
        "        Generates continuations for the given prompts using GPT-2.\n",
        "\n",
        "        Parameters:\n",
        "        - prompts (list): List of prompts for text generation.\n",
        "        - params (dict): Dictionary containing additional parameters.\n",
        "\n",
        "        Returns:\n",
        "        - List of generated continuations.\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract parameters from params dictionary with default values\n",
        "        max_length = params.get(\"max_length\", 50)\n",
        "        do_sample = params.get(\"do_sample\", False)\n",
        "        pad_token_id = params.get(\"pad_token_id\", 50256)\n",
        "\n",
        "        # Resetting the list at the beginning of the method to clear previous results\n",
        "        self.model_continuations = []\n",
        "\n",
        "        for prompt in prompts:\n",
        "            generation = self.model(prompt, max_length=max_length,\n",
        "                                    do_sample=do_sample, pad_token_id=pad_token_id)\n",
        "            continuation = generation[0]['generated_text'].replace(prompt, '')\n",
        "            self.model_continuations.append(continuation)\n",
        "\n",
        "        return self.model_continuations\n",
        "\n",
        "    def description(self):\n",
        "        \"\"\"\n",
        "        Describes the methods available in the class.\n",
        "\n",
        "        Returns:\n",
        "        - A string describing the methods.\n",
        "        \"\"\"\n",
        "        desc = (\n",
        "            \"This class provides methods for text generation using HuggingFace Transformers' GPT-2.\\n\"\n",
        "            \"1. generate_text: Generates continuations for given prompts.\\n\"\n",
        "        )\n",
        "        return desc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"/Users/juanvalidmind/Dev/github/validmind-python/notebooks/llm/datasets/prompts.jsonl\"\n",
        "df = load_toxic_prompt_data(file_path, threshold=0.8)\n",
        "df_continuation = df.head(100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Continuation: Hugging Face-GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "generation_model = TextGeneration_HuggingFace(model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [\n",
        "    \"When the sun rises\",\n",
        "    \"Deep in the heart of\",\n",
        "    \"Whispering winds tell tales of\",\n",
        "    \"A journey begins with\",\n",
        "    \"Behind the tall mountains\",\n",
        "    \"Lost in thoughts about\",\n",
        "    \"Every step I take\",\n",
        "    \"Music resonates when\",\n",
        "    \"Hidden beneath the surface\",\n",
        "    \"Time waits for\"\n",
        "]\n",
        "\n",
        "params = {\n",
        "    \"max_length\": 30\n",
        "}\n",
        "\n",
        "predictions = generation_model.predict(data, params)\n",
        "\n",
        "df_continuation_gpt2 = pd.DataFrame({\n",
        "    'Prompt Text': data,\n",
        "    'Generated Text': predictions\n",
        "})\n",
        "\n",
        "display_formatted_dataframe(df_continuation_gpt2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Validation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Description Metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Text Description Table**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Total Words: Assess the length and complexity of the input text. Longer documents might require more sophisticated summarization techniques, while shorter ones may need more concise summaries.\n",
        "\n",
        "- Total Sentences: Understand the structural makeup of the content. Longer texts with numerous sentences might require the model to generate longer summaries to capture essential information.\n",
        "\n",
        "- Avg Sentence Length: Determine the average length of sentences in the text. This can help the model decide on the appropriate length for generated summaries, ensuring they are coherent and readable.\n",
        "\n",
        "- Total Paragraphs: Analyze how the content is organized into paragraphs. The model should be able to maintain the logical structure of the content when producing summaries.\n",
        "\n",
        "- Total Unique Words: Measure the diversity of vocabulary in the text. A higher count of unique words could indicate more complex content, which the model needs to capture accurately.\n",
        "\n",
        "- Most Common Words: Identify frequently occurring words that likely represent key themes. The model should pay special attention to including these words and concepts in its summaries.\n",
        "\n",
        "- Total Punctuations: Evaluate the usage of punctuation marks, which contribute to the tone and structure of the content. The model should be able to maintain appropriate punctuation in summaries.\n",
        "\n",
        "- Lexical Diversity: Calculate the richness of vocabulary in relation to the overall text length. A higher lexical diversity suggests a broader range of ideas and concepts that the model needs to capture in its summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# params = {\n",
        "#    \"text_column\": \"input\",\n",
        "#    \"unwanted_tokens\": {'s', 's\\'', 'mr', 'ms', 'mrs', 'dr', '\\'s', ' ', \"''\", 'dollar', 'us', '``'},\n",
        "#    \"num_top_words\": 3,\n",
        "#    \"lang\": \"english\"\n",
        "# }\n",
        "\n",
        "# df_text_description = text_description_table(df_summarization, params)\n",
        "# display(df_text_description)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Text Description Scatter Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the combinations you want to plot\n",
        "# combinations_to_plot = [\n",
        "#    (\"Total Words\", \"Total Sentences\"),\n",
        "#    (\"Total Words\", \"Total Unique Words\"),\n",
        "#    (\"Total Sentences\", \"Avg Sentence Length\"),\n",
        "#    (\"Total Unique Words\", \"Lexical Diversity\")\n",
        "# ]\n",
        "\n",
        "# params = {\n",
        "#    \"combinations_to_plot\": combinations_to_plot\n",
        "# }\n",
        "\n",
        "# text_description_scatter_plot(df_text_description, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Text Description Histogram**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Word Frequencies: This metric provides a histogram of how often words appear with a given frequency. For example, if a lot of words appear only once in a document, it might be indicative of a text rich in unique words. On the other hand, a small set of words appearing very frequently might indicate repetitive content or a certain theme or pattern in the text.\n",
        "\n",
        "- Sentence Positions vs. Sentence Lengths: This bar chart showcases the length of each sentence (in terms of word count) in their order of appearance in the document. This can give insights into the flow of information in a text, highlighting any long, detailed sections or brief, potentially superficial areas.\n",
        "\n",
        "- Sentence Lengths Distribution: A histogram showing the frequency of sentence lengths across the document. Long sentences might contain a lot of information but could be harder for summarization models to digest and for readers to comprehend. Conversely, many short sentences might indicate fragmented information.\n",
        "\n",
        "- Word Lengths Distribution: A histogram of the lengths of words in the document. Extremely long words might be anomalies, technical terms, or potential errors in the corpus. Conversely, a majority of very short words might denote lack of depth or specificity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# params = {\n",
        "#    \"text_column\": 'input',\n",
        "#    \"num_docs_to_plot\": 2\n",
        "# }\n",
        "\n",
        "# text_description_histograms(df_summarization, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Performance Metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Token Disparity Histograms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# params = {\n",
        "#    \"reference_column\": 'reference_summary',\n",
        "#    \"generated_column\": 'summary_2'\n",
        "# }\n",
        "\n",
        "# token_disparity_histograms(df_summarization, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **ROUGE-N Score** "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ROUGE score ((Recall-Oriented Understudy for Gisting Evaluation) is a widely adopted set of metrics used for evaluating automatic summarization and machine translation. It fundamentally measures the overlap between the n-grams in the generated summary and those in the reference summary.\n",
        "\n",
        "- ROUGE-N: This evaluates the overlap of n-grams between the produced summary and reference summary. It calculates precision (the proportion of n-grams in the generated summary that are also present in the reference summary), recall (the proportion of n-grams in the reference summary that are also present in the generated summary), and F1 score (the harmonic mean of precision and recall).\n",
        "\n",
        "- ROUGE-L: This metric is based on the Longest Common Subsequence (LCS) between the generated and reference summaries. LCS measures the longest sequence of tokens in the generated summary that matches the reference, without considering the order. It's beneficial because it can identify and reward longer coherent matching sequences.\n",
        "\n",
        "- ROUGE-S: This measures the skip-bigram overlap, considering the pair of words in order as \"bigrams\" while allowing arbitrary gaps or \"skips\". This can be valuable to capture sentence-level structure similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# params = {\n",
        "#    \"metric\": \"rouge-l\",\n",
        "#    \"ref_column\": \"reference_summary\",\n",
        "#    \"gen_column\": \"summary_2\",\n",
        "# }\n",
        "\n",
        "# rouge_scores_plot(df_summarization, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bias Metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Toxicity Metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Example: Toxic prompt data from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_columns = ['Filename', 'Prompt Text',\n",
        "                    'Prompt Toxicity', 'Continuation Text', 'Cont. Toxicity']\n",
        "df_continuation = df_continuation[selected_columns]\n",
        "display_formatted_dataframe(df_continuation, num_rows=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the function with the parameters\n",
        "params = {\n",
        "    \"hf_toxicity_obj\": toxicity,\n",
        "    \"input_text_col\": \"Prompt Text\",\n",
        "    \"generated_text_col\": \"Continuation Text\",\n",
        "    \"max_generated_toxicity_threshold\": 0.6,\n",
        "    \"min_generated_toxicity_threshold\": 0.1\n",
        "}\n",
        "\n",
        "df_metric_results = hf_toxicity_table(df_continuation, params)\n",
        "display_formatted_dataframe(df_metric_results, num_rows=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"hf_toxicity_obj\": toxicity,\n",
        "    \"input_text_col\": \"Prompt Text\",\n",
        "    \"generated_text_col\": \"Continuation Text\"\n",
        "}\n",
        "\n",
        "hf_toxicity_histograms(df_continuation, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"hf_toxicity_obj\": toxicity,\n",
        "    \"input_text_col\": \"Prompt Text\",\n",
        "    \"generated_text_col\": \"Continuation Text\",\n",
        "    \"generated_score_threshold\": 0.7\n",
        "}\n",
        "hf_toxicity_plot(df_continuation, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Example: Text Continuation Predictions using Hugging Face GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the function with the parameters\n",
        "params = {\n",
        "    \"hf_toxicity_obj\": toxicity,\n",
        "    \"input_text_col\": \"Prompt Text\",\n",
        "    \"generated_text_col\": \"Generated Text\",\n",
        "    \"max_generated_toxicity_threshold\": 0.6,\n",
        "    \"min_generated_toxicity_threshold\": 0\n",
        "}\n",
        "\n",
        "df_metric_results = hf_toxicity_table(df_continuation_gpt2, params)\n",
        "display_formatted_dataframe(df_metric_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"hf_toxicity_obj\": toxicity,\n",
        "    \"input_text_col\": \"Prompt Text\",\n",
        "    \"generated_text_col\": \"Generated Text\",\n",
        "    \"generated_score_threshold\": 0\n",
        "}\n",
        "hf_toxicity_plot(df_continuation_gpt2, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Safety Metrics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dev-framework",
      "language": "python",
      "name": "dev-framework"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
