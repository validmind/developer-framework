{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summarization of financial data using a Large Language Model (LLM)\n",
        "\n",
        "This notebook aims to provide an introduction to documenting an LLM using the ValidMind Developer Framework. The use case presented is a summarization of financial news (https://huggingface.co/datasets/cnn_dailymail).\n",
        "\n",
        "- Initializing the ValidMind Developer Framework\n",
        "- Running a test various tests to quickly generate documentation about the data and model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you begin\n",
        "\n",
        "::: {.callout-tip}\n",
        "### New to ValidMind? \n",
        "To access the ValidMind Platform UI, you'll need an account.\n",
        "\n",
        "Signing up is FREE â€” **[Create your account](https://app.prod.validmind.ai)**.\n",
        ":::\n",
        "\n",
        "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Install the client library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install --upgrade validmind"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the client library\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "Get your code snippet:\n",
        "\n",
        "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
        "\n",
        "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
        "\n",
        "3. Enter the model details, making sure to select **LLM-based Text Summarization** as the template and **Marketing/Sales - Analytics** as the use case, and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
        "\n",
        "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
        "\n",
        "Next, replace this placeholder with your own code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace with your code snippet\n",
        "\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    project=\"...\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use case"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the realm of financial news, accurate interpretation of information is key. Given the vast amounts of data, we use Hugging Face's LLMs for summarization of financial articles. This notebook focuses on evaluating these text summarization models, especially using validation metrics. These metrics don't just measure the data and model's performance and accuracy; they also check for any bias or toxicity in the summaries. This helps ensures developers and users that the summarized content is both trustworthy and compliant with AI principles."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display, HTML\n",
        "from rouge import Rouge\n",
        "import plotly.graph_objects as go\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import torch\n",
        "import string\n",
        "import plotly.express as px\n",
        "import plotly.subplots as sp\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_id_column(df):\n",
        "    \"\"\"\n",
        "    Adds an 'ID' column to the dataframe.\n",
        "\n",
        "    Args:\n",
        "    - df (pd.DataFrame): The input dataframe.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The dataframe with an added 'ID' column.\n",
        "    \"\"\"\n",
        "    df.insert(0, 'ID', range(1, 1 + len(df)))\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_text_data(filepath, num_records=5):\n",
        "    \"\"\"\n",
        "    Load a CSV file, limit the number of records, and add an 'ID' column.\n",
        "\n",
        "    Args:\n",
        "    - filepath (str): Path to the CSV file.\n",
        "    - num_records (int): Number of records to load.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The dataframe with the specified number of records and an added 'ID' column.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(filepath)\n",
        "    df = df.head(num_records)\n",
        "    return add_id_column(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_toxic_prompt_data(file_path, threshold=None):\n",
        "    \"\"\"\n",
        "    Reads a .jsonl file and returns a dataframe with all records.\n",
        "\n",
        "    :param file_path: Path to the .jsonl file.\n",
        "    :param threshold: Optional toxicity threshold. If provided, only records with a toxicity\n",
        "                      score exceeding this threshold will be returned.\n",
        "    :return: DataFrame with all records.\n",
        "    \"\"\"\n",
        "    # Load records\n",
        "    records = []\n",
        "    with open(file_path, \"r\") as file:\n",
        "        for line in file:\n",
        "            data = json.loads(line)\n",
        "            records.append(data)\n",
        "\n",
        "    # Prepare data for dataframe\n",
        "    data_list = []\n",
        "    for record in records:\n",
        "        prompt_toxicity = record[\"prompt\"].get(\"toxicity\", None)\n",
        "        cont_toxicity = record[\"continuation\"].get(\"toxicity\", None)\n",
        "\n",
        "        if threshold:\n",
        "            if not prompt_toxicity or not cont_toxicity:\n",
        "                continue  # If we don't have a toxicity value, skip the record\n",
        "            elif prompt_toxicity < threshold and cont_toxicity < threshold:\n",
        "                continue  # If neither toxicity value exceeds the threshold, skip the record\n",
        "\n",
        "        row = {\n",
        "            \"Filename\": record[\"filename\"],\n",
        "            \"Begin\": record[\"begin\"],\n",
        "            \"End\": record[\"end\"],\n",
        "            \"Challenging\": record[\"challenging\"],\n",
        "            \"Prompt Text\": record[\"prompt\"][\"text\"],\n",
        "            \"Prompt Profanity\": record[\"prompt\"].get(\"profanity\", \"N/A\"),\n",
        "            \"Prompt Sexually Explicit\": record[\"prompt\"].get(\"sexually_explicit\", \"N/A\"),\n",
        "            \"Prompt Identity Attack\": record[\"prompt\"].get(\"identity_attack\", \"N/A\"),\n",
        "            \"Prompt Flirtation\": record[\"prompt\"].get(\"flirtation\", \"N/A\"),\n",
        "            \"Prompt Threat\": record[\"prompt\"].get(\"threat\", \"N/A\"),\n",
        "            \"Prompt Insult\": record[\"prompt\"].get(\"insult\", \"N/A\"),\n",
        "            \"Prompt Severe Toxicity\": record[\"prompt\"].get(\"severe_toxicity\", \"N/A\"),\n",
        "            \"Prompt Toxicity\": record[\"prompt\"].get(\"toxicity\", \"N/A\"),\n",
        "            \"Continuation Text\": record[\"continuation\"][\"text\"],\n",
        "            \"Cont. Severe Toxicity\": record[\"continuation\"].get(\"severe_toxicity\", \"N/A\"),\n",
        "            \"Cont. Toxicity\": record[\"continuation\"].get(\"toxicity\", \"N/A\"),\n",
        "            \"Cont. Profanity\": record[\"continuation\"].get(\"profanity\", \"N/A\"),\n",
        "            \"Cont. Sexually Explicit\": record[\"continuation\"].get(\"sexually_explicit\", \"N/A\"),\n",
        "            \"Cont. Identity Attack\": record[\"continuation\"].get(\"identity_attack\", \"N/A\"),\n",
        "            \"Cont. Flirtation\": record[\"continuation\"].get(\"flirtation\", \"N/A\"),\n",
        "            \"Cont. Threat\": record[\"continuation\"].get(\"threat\", \"N/A\"),\n",
        "            \"Cont. Insult\": record[\"continuation\"].get(\"insult\", \"N/A\")\n",
        "        }\n",
        "        data_list.append(row)\n",
        "\n",
        "    # Convert list of dicts to dataframe\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _format_cell_text(text, width=50):\n",
        "    \"\"\"Private function to format a cell's text.\"\"\"\n",
        "    return '\\n'.join([textwrap.fill(line, width=width) for line in text.split('\\n')])\n",
        "\n",
        "\n",
        "def _format_dataframe_for_tabulate(df):\n",
        "    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Format all string columns\n",
        "    for column in df_out.columns:\n",
        "        # Check if column is of type object (likely strings)\n",
        "        if df_out[column].dtype == object:\n",
        "            df_out[column] = df_out[column].apply(_format_cell_text)\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def _dataframe_to_html_table(df):\n",
        "    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n",
        "    headers = df.columns.tolist()\n",
        "    table_data = df.values.tolist()\n",
        "    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n",
        "\n",
        "\n",
        "def display_nice(df, num_rows=None):\n",
        "    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n",
        "    if num_rows is not None:\n",
        "        df = df.head(num_rows)\n",
        "    formatted_df = _format_dataframe_for_tabulate(df)\n",
        "    html_table = _dataframe_to_html_table(formatted_df)\n",
        "    display(HTML(html_table))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_list_to_df(df, column_data, column_name):\n",
        "    \"\"\"\n",
        "    Adds a new column to the dataframe df that contains the given data.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The original pandas DataFrame.\n",
        "    - column_data (list/array): List/array of data to be added as a new column.\n",
        "    - column_name (str): Name of the new column.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame with an additional column.\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()  # Make an explicit copy of the DataFrame\n",
        "\n",
        "    # Check if the length of column_data matches the number of rows in the DataFrame\n",
        "    if len(column_data) != len(df):\n",
        "        raise ValueError(\n",
        "            f\"The length of column_data ({len(column_data)}) does not match the number of rows in the DataFrame ({len(df)}).\")\n",
        "\n",
        "    # Add the column_data to the DataFrame\n",
        "    df[column_name] = column_data\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_summaries_to_df(df, summaries):\n",
        "    \"\"\"\n",
        "    Adds a new column 'summary_X' to the dataframe df that contains the given summaries, where X is an incremental number.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The original pandas DataFrame.\n",
        "    - summaries: List/array of summarized texts.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with an additional summary column, with 'labels' being the first column followed by the original 'text'.\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()  # Make an explicit copy of the DataFrame\n",
        "\n",
        "    # Check if the length of summaries matches the number of rows in the DataFrame\n",
        "    if len(summaries) != len(df):\n",
        "        raise ValueError(\n",
        "            f\"The number of summaries ({len(summaries)}) does not match the number of rows in the DataFrame ({len(df)}).\")\n",
        "\n",
        "    # Determine the name for the new summary column\n",
        "    col_index = 1\n",
        "    col_name = 'summary_1'\n",
        "    while col_name in df.columns:\n",
        "        col_index += 1\n",
        "        col_name = f'summary_{col_index}'\n",
        "\n",
        "    # Add the summaries to the DataFrame\n",
        "    df[col_name] = summaries\n",
        "\n",
        "    # Rearrange the DataFrame columns to have 'topic' first, then the original 'input', followed by summary columns\n",
        "    summary_columns = [col for col in df.columns if col.startswith('summary')]\n",
        "    other_columns = [col for col in df.columns if col not in summary_columns\n",
        "                     + ['topic', 'input', 'reference_summary']]\n",
        "\n",
        "    columns_order = ['topic', 'input', 'reference_summary'] + \\\n",
        "        sorted(summary_columns) + other_columns\n",
        "    df = df[columns_order]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### POC Validation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "toxicity = evaluate.load(\"toxicity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "def hf_toxicity_plot(df, params):\n",
        "    \"\"\"\n",
        "    Compute toxicity scores for texts and then plot line plots for each text column\n",
        "    where the generated text score surpasses the given threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The dataframe containing texts.\n",
        "    - params (dict): Parameters containing toxicity evaluation object, column names, and score threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract necessary parameters\n",
        "    toxicity = params[\"hf_toxicity_obj\"]\n",
        "    text_columns = params[\"text_columns\"]\n",
        "    score_threshold = params.get(\"score_threshold\", 0)  # default to 0 if not provided\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for col in text_columns:\n",
        "        # Get list of texts from dataframe\n",
        "        texts = df[col].tolist()\n",
        "\n",
        "        # Compute toxicity for texts\n",
        "        toxicity_scores = toxicity.compute(predictions=texts)['toxicity']\n",
        "\n",
        "        # Filter records where the score is above the threshold\n",
        "        indices = [i for i, score in enumerate(\n",
        "            toxicity_scores) if score > score_threshold]\n",
        "        filtered_scores = [score for i, score in enumerate(\n",
        "            toxicity_scores) if i in indices]\n",
        "\n",
        "        # Add trace for the scores with modified line width\n",
        "        fig.add_trace(go.Scatter(x=indices, y=filtered_scores, mode='lines+markers', name=col,\n",
        "                                 # Set width to 1 for a thinner line\n",
        "                                 line=dict(width=1)))\n",
        "\n",
        "    # Add a horizontal line for the threshold\n",
        "    fig.add_shape(\n",
        "        go.layout.Shape(\n",
        "            type=\"line\",\n",
        "            x0=0,\n",
        "            x1=max(indices) if indices else 1,\n",
        "            y0=score_threshold,\n",
        "            y1=score_threshold,\n",
        "            line=dict(color=\"grey\", width=0.8, dash=\"dash\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(title=\"Toxicity Scores for Text Columns with Score above threshold\",\n",
        "                      xaxis_title=\"Index\",\n",
        "                      yaxis_title=\"Toxicity Score\",\n",
        "                      legend_title=\"Text Type\")\n",
        "\n",
        "    # Show figure\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hf_toxicity_table(df, params):\n",
        "    \"\"\"\n",
        "    Update and return dataframe with toxicity scores for all the text columns provided.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The dataframe containing texts.\n",
        "    - params (dict): Parameters containing toxicity evaluation object, column names, and the max and min generated toxicity thresholds.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Updated dataframe with toxicity scores.\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()  # Create a deep copy of the DataFrame\n",
        "\n",
        "    # Extract necessary parameters\n",
        "    toxicity = params[\"hf_toxicity_obj\"]\n",
        "    text_columns = params[\"text_columns\"]\n",
        "    max_toxicity_threshold = params.get(\n",
        "        \"max_toxicity_threshold\", 1)  # default to 1 if not provided\n",
        "    min_toxicity_threshold = params.get(\n",
        "        \"min_toxicity_threshold\", 0)  # default to 0 if not provided\n",
        "\n",
        "    for col in text_columns:\n",
        "        # Get list of texts from dataframe\n",
        "        texts = df[col].tolist()\n",
        "\n",
        "        # Compute toxicity for texts\n",
        "        toxicity_scores = toxicity.compute(predictions=texts)['toxicity']\n",
        "\n",
        "        # Assign the new toxicity scores to the dataframe using .loc to avoid the warning\n",
        "        df.loc[:, f\"{col} Toxicity\"] = toxicity_scores\n",
        "\n",
        "        # If you want to filter rows for each column separately based on their toxicity\n",
        "        # df = df[(df[f\"{col} Toxicity\"] >= min_toxicity_threshold) & (df[f\"{col} Toxicity\"] <= max_toxicity_threshold)]\n",
        "\n",
        "    # If you want to filter rows based on the toxicity of a specific column, you can do it here.\n",
        "    # For example, if you want to filter rows based on the toxicity of the first column in text_columns:\n",
        "    # df = df[(df[f\"{text_columns[0]} Toxicity\"] >= min_toxicity_threshold) & (df[f\"{text_columns[0]} Toxicity\"] <= max_toxicity_threshold)]\n",
        "\n",
        "    # Order the results by \"<Column Name> Toxicity\" in descending order for the first text column\n",
        "    df = df.sort_values(by=f\"{text_columns[0]} Toxicity\", ascending=False)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.subplots as sp\n",
        "\n",
        "\n",
        "def hf_toxicity_histograms(df, params):\n",
        "    \"\"\"\n",
        "    Compute toxicity scores for texts and then plot histograms for specified text columns.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The dataframe containing texts.\n",
        "    - params (dict): Parameters containing toxicity evaluation object and column names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract necessary parameters\n",
        "    toxicity = params[\"hf_toxicity_obj\"]\n",
        "    text_columns = params[\"text_columns\"]\n",
        "\n",
        "    # Determine the number of rows required based on the number of text columns\n",
        "    num_rows = (len(text_columns) + 1) // 2  # +1 to handle odd number of columns\n",
        "\n",
        "    # Create a subplot layout\n",
        "    fig = sp.make_subplots(rows=num_rows, cols=2, subplot_titles=text_columns)\n",
        "\n",
        "    subplot_height = 350  # Height of each subplot\n",
        "    total_height = num_rows * subplot_height + 200  # 200 for padding, titles, etc.\n",
        "\n",
        "    for idx, col in enumerate(text_columns, start=1):\n",
        "        row = (idx - 1) // 2 + 1\n",
        "        col_idx = (idx - 1) % 2 + 1  # to place subplots in two columns\n",
        "\n",
        "        # Get list of texts from dataframe\n",
        "        texts = df[col].tolist()\n",
        "\n",
        "        # Compute toxicity for texts\n",
        "        toxicity_scores = toxicity.compute(predictions=texts)['toxicity']\n",
        "\n",
        "        # Add traces to the corresponding subplot without legend\n",
        "        fig.add_trace(go.Histogram(x=toxicity_scores,\n",
        "                      showlegend=False), row=row, col=col_idx)\n",
        "\n",
        "        # Update xaxes and yaxes titles only for the first subplot\n",
        "        if idx == 1:\n",
        "            fig.update_xaxes(title_text=\"Toxicity Score\", row=row, col=col_idx)\n",
        "            fig.update_yaxes(title_text=\"Frequency\", row=row, col=col_idx)\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(title_text=\"Histograms of Toxicity Scores\", height=total_height)\n",
        "\n",
        "    # Show figure\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Secondary functions\n",
        "def general_text_metrics(df, text_column):\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for text in df[text_column]:\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        words = nltk.word_tokenize(text)\n",
        "        paragraphs = text.split(\"\\n\\n\")\n",
        "\n",
        "        total_words = len(words)\n",
        "        total_sentences = len(sentences)\n",
        "        avg_sentence_length = round(sum(len(sentence.split(\n",
        "        )) for sentence in sentences) / total_sentences if total_sentences else 0, 1)\n",
        "        total_paragraphs = len(paragraphs)\n",
        "\n",
        "        results.append([total_words, total_sentences,\n",
        "                       avg_sentence_length, total_paragraphs])\n",
        "\n",
        "    return pd.DataFrame(results, columns=[\"Total Words\", \"Total Sentences\", \"Avg Sentence Length\", \"Total Paragraphs\"])\n",
        "\n",
        "\n",
        "def vocabulary_structure_metrics(df, text_column, unwanted_tokens, lang):\n",
        "    stop_words = set(word.lower() for word in stopwords.words(lang))\n",
        "    unwanted_tokens = set(token.lower() for token in unwanted_tokens)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for text in df[text_column]:\n",
        "        words = nltk.word_tokenize(text)\n",
        "\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words and word.lower(\n",
        "        ) not in unwanted_tokens and word not in string.punctuation]\n",
        "\n",
        "        total_unique_words = len(set(filtered_words))\n",
        "        total_punctuations = sum(1 for word in words if word in string.punctuation)\n",
        "        lexical_diversity = round(\n",
        "            total_unique_words / len(filtered_words) if filtered_words else 0, 1)\n",
        "\n",
        "        results.append([total_unique_words, total_punctuations, lexical_diversity])\n",
        "\n",
        "    return pd.DataFrame(results, columns=[\"Total Unique Words\", \"Total Punctuations\", \"Lexical Diversity\"])\n",
        "\n",
        "# Primary function\n",
        "\n",
        "\n",
        "def text_description_table(df, params):\n",
        "    text_column = params[\"text_column\"]\n",
        "    unwanted_tokens = params[\"unwanted_tokens\"]\n",
        "    lang = params[\"lang\"]\n",
        "\n",
        "    gen_metrics_df = general_text_metrics(df, text_column)\n",
        "    vocab_metrics_df = vocabulary_structure_metrics(\n",
        "        df, text_column, unwanted_tokens, lang)\n",
        "\n",
        "    combined_df = pd.concat([gen_metrics_df, vocab_metrics_df], axis=1)\n",
        "\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Ensuring NLTK resources are downloaded\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "\n",
        "def text_description_histograms(df, params):\n",
        "    \"\"\"\n",
        "    This function takes a dataframe and plots histograms for the specified metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): DataFrame containing the text data.\n",
        "    - params (dict): Dictionary containing parameters like \"text_column\", \"unwanted_tokens\", and \"lang\".\n",
        "\n",
        "    Returns:\n",
        "    - Plotly figure containing the histograms.\n",
        "    \"\"\"\n",
        "\n",
        "    text_column = params[\"text_column\"]\n",
        "\n",
        "    # Combined function to get all metrics\n",
        "    gen_metrics_df = general_text_metrics(df, text_column)\n",
        "    vocab_metrics_df = vocabulary_structure_metrics(\n",
        "        df, text_column, params[\"unwanted_tokens\"], params[\"lang\"])\n",
        "    combined_df = pd.concat([gen_metrics_df, vocab_metrics_df], axis=1)\n",
        "\n",
        "    # Determine number of rows based on number of metrics\n",
        "    # Ceiling division to ensure every metric gets a subplot\n",
        "    num_rows = (combined_df.shape[1] + 1) // 2\n",
        "\n",
        "    # Create subplots\n",
        "    fig = make_subplots(rows=num_rows, cols=2, subplot_titles=combined_df.columns)\n",
        "\n",
        "    # For each metric, plot a histogram\n",
        "    for index, column in enumerate(combined_df.columns):\n",
        "        row, col = divmod(index, 2)\n",
        "        fig.add_trace(\n",
        "            go.Histogram(x=combined_df[column], name=column),\n",
        "            row=row + 1,\n",
        "            col=col + 1\n",
        "        )\n",
        "\n",
        "    # Update layout for better appearance and adjust the height\n",
        "    subplot_height = 400  # Define the height of each individual subplot\n",
        "    total_height = num_rows * subplot_height\n",
        "    fig.update_layout(title_text=\"Distribution of Text Metrics\",\n",
        "                      bargap=0.2, bargroupgap=0.1, height=total_height)\n",
        "\n",
        "    return fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_structure_histograms(df, params):\n",
        "\n",
        "    text_column = params[\"text_column\"]\n",
        "    num_docs_to_plot = params[\"num_docs_to_plot\"]\n",
        "\n",
        "    # Ensure the nltk punkt tokenizer is downloaded\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "    # Decide on the number of documents to plot\n",
        "    if not num_docs_to_plot or num_docs_to_plot > len(df):\n",
        "        num_docs_to_plot = len(df)\n",
        "\n",
        "    # Colors for each subplot\n",
        "    colors = ['blue', 'green', 'red', 'purple']\n",
        "\n",
        "    # Axis titles for clarity\n",
        "    x_titles = [\n",
        "        \"Word Frequencies\",\n",
        "        \"Sentence Position in Document\",\n",
        "        \"Sentence Lengths (Words)\",\n",
        "        \"Word Lengths (Characters)\"\n",
        "    ]\n",
        "    y_titles = [\n",
        "        \"Number of Words\",\n",
        "        \"Sentence Length (Words)\",\n",
        "        \"Number of Sentences\",\n",
        "        \"Number of Words\"\n",
        "    ]\n",
        "\n",
        "    # Iterate over each document in the DataFrame up to the user-specified limit\n",
        "    for index, (idx, row) in enumerate(df.head(num_docs_to_plot).iterrows()):\n",
        "        # Create subplots with a 2x2 grid for each metric\n",
        "        fig = sp.make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=[\n",
        "                \"Word Frequencies\",\n",
        "                \"Sentence Positions\",\n",
        "                \"Sentence Lengths\",\n",
        "                \"Word Lengths\"\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Tokenize document into sentences and words\n",
        "        sentences = nltk.sent_tokenize(row[text_column])\n",
        "        words = nltk.word_tokenize(row[text_column])\n",
        "\n",
        "        # Metrics computation\n",
        "        word_freq = Counter(words)\n",
        "        freq_counts = Counter(word_freq.values())\n",
        "        word_frequencies = list(freq_counts.keys())\n",
        "        word_frequency_counts = list(freq_counts.values())\n",
        "\n",
        "        sentence_positions = list(range(1, len(sentences) + 1))\n",
        "        sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
        "        word_lengths = [len(word) for word in words]\n",
        "\n",
        "        # Adding data to subplots\n",
        "        fig.add_trace(go.Bar(x=word_frequencies, y=word_frequency_counts,\n",
        "                      marker_color=colors[0], showlegend=False), row=1, col=1)\n",
        "        fig.add_trace(go.Bar(x=sentence_positions, y=sentence_lengths,\n",
        "                      marker_color=colors[1], showlegend=False), row=1, col=2)\n",
        "        fig.add_trace(go.Histogram(x=sentence_lengths, nbinsx=50, opacity=0.75,\n",
        "                      marker_color=colors[2], showlegend=False), row=2, col=1)\n",
        "        fig.add_trace(go.Histogram(x=word_lengths, nbinsx=50, opacity=0.75,\n",
        "                      marker_color=colors[3], showlegend=False), row=2, col=2)\n",
        "\n",
        "        # Update x and y axis titles\n",
        "        for i, (x_title, y_title) in enumerate(zip(x_titles, y_titles)):\n",
        "            fig['layout'][f'xaxis{\n",
        "                i + 1}'].update(title=x_title, titlefont=dict(size=10))\n",
        "            fig['layout'][f'yaxis{\n",
        "                i + 1}'].update(title=y_title, titlefont=dict(size=10))\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title=f\"Text Description for Document {index + 1}\",\n",
        "            barmode='overlay',\n",
        "            height=800\n",
        "        )\n",
        "\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Assuming the previously defined functions and parameters are present\n",
        "\n",
        "\n",
        "def text_description_scatter(df, params):\n",
        "    \"\"\"\n",
        "    This function takes a dataframe and plots scatter plots for the specified combinations of metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): DataFrame containing the text data.\n",
        "    - params (dict): Dictionary containing parameters like \"combinations_to_plot\", \"text_column\", \"unwanted_tokens\", and \"lang\".\n",
        "\n",
        "    Returns:\n",
        "    - Plotly figure containing the scatter plots.\n",
        "    \"\"\"\n",
        "\n",
        "    text_column = params[\"text_column\"]\n",
        "\n",
        "    # Combined function to get all metrics\n",
        "    gen_metrics_df = general_text_metrics(df, text_column)\n",
        "    vocab_metrics_df = vocabulary_structure_metrics(\n",
        "        df, text_column, params[\"unwanted_tokens\"], params[\"lang\"])\n",
        "    combined_df = pd.concat([gen_metrics_df, vocab_metrics_df], axis=1)\n",
        "\n",
        "    combinations_to_plot = params[\"combinations_to_plot\"]\n",
        "    num_combinations = len(combinations_to_plot)\n",
        "\n",
        "    # Determine number of rows based on number of combinations\n",
        "    num_rows = (num_combinations + 1) // 2  # Ceiling division\n",
        "\n",
        "    # Create subplots\n",
        "    fig = make_subplots(rows=num_rows, cols=2, subplot_titles=[\n",
        "                        f\"{x} vs {y}\" for x, y in combinations_to_plot])\n",
        "\n",
        "    # For each combination, plot a scatter plot\n",
        "    for index, (x_column, y_column) in enumerate(combinations_to_plot):\n",
        "        row, col = divmod(index, 2)\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=combined_df[x_column], y=combined_df[y_column],\n",
        "                       mode='markers', showlegend=False),\n",
        "            row=row + 1,\n",
        "            col=col + 1\n",
        "        )\n",
        "        # Update axis titles\n",
        "        fig.update_xaxes(title_text=x_column, row=row + 1, col=col + 1)\n",
        "        fig.update_yaxes(title_text=y_column, row=row + 1, col=col + 1)\n",
        "\n",
        "    # Update layout for better appearance\n",
        "    subplot_height = 400  # Define the height of each individual subplot\n",
        "    total_height = num_rows * subplot_height\n",
        "    fig.update_layout(\n",
        "        title_text=\"Scatter Plots of Text Metrics Combinations\", height=total_height)\n",
        "\n",
        "    return fig.show()\n",
        "\n",
        "# You can call the function with:\n",
        "# text_description_scatter(df, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "def token_distribution_histograms(df, params):\n",
        "    \"\"\"\n",
        "    Visualize the token counts distribution of given columns using histograms.\n",
        "\n",
        "    :param df: DataFrame containing the text columns.\n",
        "    :param params: Dictionary with the key [\"text_columns\"].\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()  # Create a deep copy of the DataFrame\n",
        "    text_columns = params[\"text_columns\"]\n",
        "\n",
        "    # Define an extended list of colors for the subplots\n",
        "    colors = [\n",
        "        'blue', 'red', 'green', 'purple', 'orange', 'pink', 'yellow', 'brown',\n",
        "        'grey', 'cyan', 'magenta', 'lime', 'navy', 'maroon', 'olive', 'teal',\n",
        "        'aqua', 'fuchsia', 'salmon', 'indigo'\n",
        "    ]\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Determine the number of rows required based on the number of text columns\n",
        "    num_rows = (len(text_columns) + 1) // 2\n",
        "\n",
        "    # Create subplots\n",
        "    fig = make_subplots(rows=num_rows, cols=2, subplot_titles=text_columns)\n",
        "\n",
        "    for idx, col in enumerate(text_columns, start=1):\n",
        "        row = (idx - 1) // 2 + 1\n",
        "        col_idx = (idx - 1) % 2 + 1\n",
        "\n",
        "        # Tokenize the column and get the number of tokens\n",
        "        tokens_col = f'tokens_{idx}'\n",
        "        df[tokens_col] = df[col].apply(lambda x: len(tokenizer.tokenize(x)))\n",
        "\n",
        "        # Add histogram with color corresponding to the index\n",
        "        fig.add_trace(go.Histogram(x=df[tokens_col],\n",
        "                                   # Cycle through the color list\n",
        "                                   marker_color=colors[(idx - 1) % len(colors)],\n",
        "                                   showlegend=False),\n",
        "                      row=row, col=col_idx)\n",
        "\n",
        "        # Only add axis titles to the first subplot\n",
        "        if idx == 1:\n",
        "            fig.update_yaxes(title_text='Number of Documents', row=row, col=col_idx)\n",
        "            fig.update_xaxes(title_text='Number of Tokens', row=row, col=col_idx)\n",
        "\n",
        "    # Update layout\n",
        "    subplot_height = 300  # Height of each subplot\n",
        "    total_height = num_rows * subplot_height + 200  # 200 for padding, titles, etc.\n",
        "    fig.update_layout(title_text='Token Distributions', bargap=0.1,\n",
        "                      height=total_height, showlegend=False)\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from itertools import combinations\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "\n",
        "def rouge_scores_plot(df, params):\n",
        "    \"\"\"\n",
        "    Compute ROUGE scores for each unique pair of text columns in the DataFrame and visualize them.\n",
        "\n",
        "    :param df: DataFrame containing the summaries.\n",
        "    :param params: Dictionary with the keys [\"text_columns\", \"metric\"].\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract parameters\n",
        "    metric = params.get(\"metric\", \"rouge-2\")\n",
        "    text_columns = params[\"text_columns\"]\n",
        "\n",
        "    if metric not in [\"rouge-1\", \"rouge-2\", \"rouge-l\", \"rouge-s\"]:\n",
        "        raise ValueError(\n",
        "            \"Invalid metric. Choose from 'rouge-1', 'rouge-2', 'rouge-l', 'rouge-s'.\")\n",
        "\n",
        "    rouge = Rouge(metrics=[metric])\n",
        "\n",
        "    # Determine all unique pairs of text columns\n",
        "    pairs = list(combinations(text_columns, 2))\n",
        "    num_rows = (len(pairs) + 1) // 2\n",
        "\n",
        "    # Create subplots\n",
        "    fig = make_subplots(rows=num_rows, cols=2, subplot_titles=[\n",
        "                        f'{ref} vs {gen}' for ref, gen in pairs])\n",
        "\n",
        "    color_dict = {'p': 'blue', 'r': 'green', 'f': 'red'}\n",
        "\n",
        "    for idx, (ref_column, gen_column) in enumerate(pairs, start=1):\n",
        "        score_list = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            scores = rouge.get_scores(\n",
        "                row[gen_column], row[ref_column], avg=True)[metric]\n",
        "            score_list.append(scores)\n",
        "\n",
        "        df_scores = pd.DataFrame(score_list)\n",
        "        row = (idx - 1) // 2 + 1\n",
        "        col = (idx - 1) % 2 + 1\n",
        "\n",
        "        # Adding the line plots\n",
        "        for score_type, color in color_dict.items():\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=df_scores.index, y=df_scores[score_type], mode='lines+markers',\n",
        "                           name=\"Precision\" if score_type == 'p' else \"Recall\" if score_type == 'r' else \"F1\",\n",
        "                           line=dict(color=color),\n",
        "                           showlegend=True if idx == 1 else False),  # Show legend only in the first subplot\n",
        "                row=row, col=col\n",
        "            )\n",
        "\n",
        "    # Set layout properties\n",
        "    fig.update_layout(\n",
        "        title=f\"ROUGE-{metric.split('-')[-1].upper()} Scores\",\n",
        "        xaxis_title=\"Row Index\",\n",
        "        yaxis_title=\"Score\",\n",
        "        height=num_rows * 300  # You might want to adjust this value based on your actual needs\n",
        "    )\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hugging Face models wrappers\n",
        "\n",
        "The following code template showcases how to wrap a Hugging Face model for compatibility with the ValidMind Developer Framework. We will load an example model using the transformers API and then run some predictions on our test dataset.\n",
        "\n",
        "The ValidMind developer framework provides support for Hugging Face transformers out of the box, so in the following section we will show how to initialize multiple transformers models with the `init_model` function, removing the need for a custom wrapper. In cases where you need extra pre-processing or post-processing steps, you can use the following code template as a starting point to wrap your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AbstractSummarization_HuggingFace:\n",
        "    \"\"\"\n",
        "    A VM Model instance wrapper for abstract summarization using HuggingFace Transformers.\n",
        "    \"\"\"\n",
        "    model: any\n",
        "    tokenizer: any\n",
        "    predicted_prob_values: list = None\n",
        "\n",
        "    def __init__(self, model_name=None, model=None, tokenizer=None):\n",
        "        pipeline_task = \"summarization\"\n",
        "        self.model_name = model_name\n",
        "        self.pipeline_task = pipeline_task\n",
        "        self.model = pipeline(pipeline_task, model=model, tokenizer=tokenizer)\n",
        "\n",
        "    def predict(self, texts, params={}):\n",
        "        \"\"\"\n",
        "        Generates summaries for the given texts.\n",
        "\n",
        "        Parameters:\n",
        "        - texts (list): List of texts to be summarized.\n",
        "        - params (dict, optional): Dictionary that may contain \"min_length\" and/or \"max_length\" to control the produced summary's length.\n",
        "\n",
        "        Returns:\n",
        "        - List of summaries.\n",
        "        \"\"\"\n",
        "\n",
        "        min_length = params.get(\"min_length\")\n",
        "        max_length = params.get(\"max_length\")\n",
        "\n",
        "        # If either value is None, don't pass it to the model function\n",
        "        model_args = {}\n",
        "        if min_length is not None:\n",
        "            model_args[\"min_length\"] = min_length\n",
        "        if max_length is not None:\n",
        "            model_args[\"max_length\"] = max_length\n",
        "\n",
        "        summaries = []\n",
        "\n",
        "        for text in texts:\n",
        "            data = [str(text)]\n",
        "            # Using ** unpacking to pass arguments conditionally\n",
        "            results = self.model(data, **model_args)\n",
        "            results_df = pd.DataFrame(results)\n",
        "            summary = results_df[\"summary_text\"].values[0] if \"summary_text\" in results_df.columns else results_df[\"label\"].values[0]\n",
        "            summaries.append(summary)\n",
        "\n",
        "        return summaries\n",
        "\n",
        "    def predict_proba(self):\n",
        "        \"\"\"\n",
        "        Retrieves predicted probabilities after prediction.\n",
        "        Note: Not all models provide predicted probabilities.\n",
        "        \"\"\"\n",
        "        if self.predicted_prob_values is None:\n",
        "            raise ValueError(\n",
        "                \"First run predict method to retrieve predicted probabilities\")\n",
        "        return self.predicted_prob_values\n",
        "\n",
        "    def description(self):\n",
        "        \"\"\"\n",
        "        Describes the methods available in the class.\n",
        "\n",
        "        The class provides methods for abstract summarization using HuggingFace Transformers.\n",
        "\n",
        "        The predict method:\n",
        "        1. Generates summaries for given texts.\n",
        "        2. Accepts a 'params' dictionary which can contain optional 'min_length' and 'max_length' parameters to control the length of the produced summary.\n",
        "\n",
        "        The predict_proba method:\n",
        "        Retrieves predicted probabilities after prediction (if available by the model).\n",
        "        \"\"\"\n",
        "        return self.description.__doc__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExtractiveSummarization_BERT:\n",
        "    model: any\n",
        "    tokenizer: any\n",
        "\n",
        "    def _get_embedding(self, text):\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\",\n",
        "                                truncation=True, max_length=512, padding='max_length')\n",
        "        with torch.no_grad():\n",
        "            output = self.model(**inputs)\n",
        "        return output['last_hidden_state'].mean(dim=1).squeeze().detach().numpy()\n",
        "\n",
        "    def predict(self, texts, params={}):\n",
        "        summaries = []\n",
        "\n",
        "        # Extract summary_length from params, default to None if not provided\n",
        "        summary_length = params.get(\"summary_length\", None)\n",
        "\n",
        "        for text in texts:\n",
        "            sentences = text.split('. ')\n",
        "\n",
        "            document_embedding = self._get_embedding(' '.join(sentences))\n",
        "            sentence_embeddings = [self._get_embedding(\n",
        "                sentence) for sentence in sentences]\n",
        "            similarities = cosine_similarity(sentence_embeddings, [document_embedding])\n",
        "            sorted_indices = np.argsort(similarities, axis=0)[::-1].squeeze()\n",
        "\n",
        "            # Determine summary length\n",
        "            if summary_length is None:  # If not provided, use 20% of the total sentence count\n",
        "                top_k = int(len(sentences) * 0.2)\n",
        "            else:  # If provided, use the user-defined length\n",
        "                top_k = summary_length\n",
        "\n",
        "            # Extract the top sentences based on the chosen summary length\n",
        "            selected_sentences = [sentences[i] for i in sorted_indices[:top_k]]\n",
        "\n",
        "            summaries.append(' '.join(selected_sentences))\n",
        "\n",
        "        return summaries\n",
        "\n",
        "    def description(self):\n",
        "        \"\"\"\n",
        "        Provides a description of the methods available for extractive summarization with the current model.\n",
        "\n",
        "        The model ranks sentences in the input text based on their similarity to the overall document meaning, as determined by BERT embeddings. The top-ranked sentences are selected to form the summary.\n",
        "\n",
        "        The length of the summary can be controlled in two ways:\n",
        "        1. Automatically: Where the model summarizes to approximately 20% of the original text's sentence count.\n",
        "        2. User-defined: By specifying the 'summary_length' parameter in the 'params' dictionary when calling the 'predict' method.\n",
        "\n",
        "        \"\"\"\n",
        "        return self.description.__doc__"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we'll load the financial dataset, which will be the foundation for our summarization analysis tasks. \n",
        "\n",
        "The dataset is structured with several columns, namely \"ID\", \"topic\", \"input\", and \"reference_summary\". Each record is identified by a unique \"ID\". The \"topic\" column specifies the category or theme of the news, the \"input\" column contains detailed articles or news content, and the \"reference_summary\" provides a concise summary of the respective article in the \"input\". The dataset sources its content from BBC news."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_summarization = load_text_data(\n",
        "    filepath='./datasets/bbc_text_cls_reference.csv',\n",
        "    num_records=100\n",
        ")\n",
        "\n",
        "display_nice(df_summarization, num_rows=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extractive Summarization: Hugging Face-BERT Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extractive summarization is a technique used to produce a summary by selecting and extracting whole sentences or passages from the source document without modifying the original content. Essentially, it identifies and \"extracts\" the most important and relevant information from the larger text to create a condensed version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "extractive_model = ExtractiveSummarization_BERT(model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df_summarization.input.values.tolist()\n",
        "\n",
        "params = {\n",
        "    \"summary_length\": None,\n",
        "}\n",
        "\n",
        "extractive_summary = extractive_model.predict(data, params)\n",
        "\n",
        "df_summarization = add_list_to_df(\n",
        "    df=df_summarization,\n",
        "    column_data=extractive_summary,\n",
        "    column_name=\"extractive_summary\"\n",
        ")\n",
        "\n",
        "display_nice(df_summarization, num_rows=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Abstract Summarization: Hugging Face-T5 Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abstractive summarization is a technique used to produce a summary by understanding the main ideas of a source document and then expressing those ideas in a concise manner using new words and sentences, often not present in the original text. Instead of just extracting and repurposing existing sentences from the document, as is done in extractive summarization, abstractive summarization aims to capture the essence or meaning of the content and convey it in a shorter form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "abstract_model = AbstractSummarization_HuggingFace(model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df_summarization.input.values.tolist()\n",
        "\n",
        "params = {\n",
        "    \"summary_length\": None\n",
        "}\n",
        "\n",
        "abstract_summary = abstract_model.predict(data, params)\n",
        "\n",
        "df_summarization = add_list_to_df(\n",
        "    df=df_summarization,\n",
        "    column_data=abstract_summary,\n",
        "    column_name=\"abstract_summary\"\n",
        ")\n",
        "\n",
        "display_nice(df_summarization, num_rows=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Text Poisoning: Hugging Face-GPT2 Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Validation of Input Text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input Text Description"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Text Description Table"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Total Words: Assess the length and complexity of the input text. Longer documents might require more sophisticated summarization techniques, while shorter ones may need more concise summaries.\n",
        "\n",
        "- Total Sentences: Understand the structural makeup of the content. Longer texts with numerous sentences might require the model to generate longer summaries to capture essential information.\n",
        "\n",
        "- Avg Sentence Length: Determine the average length of sentences in the text. This can help the model decide on the appropriate length for generated summaries, ensuring they are coherent and readable.\n",
        "\n",
        "- Total Paragraphs: Analyze how the content is organized into paragraphs. The model should be able to maintain the logical structure of the content when producing summaries.\n",
        "\n",
        "- Total Unique Words: Measure the diversity of vocabulary in the text. A higher count of unique words could indicate more complex content, which the model needs to capture accurately.\n",
        "\n",
        "- Most Common Words: Identify frequently occurring words that likely represent key themes. The model should pay special attention to including these words and concepts in its summaries.\n",
        "\n",
        "- Total Punctuations: Evaluate the usage of punctuation marks, which contribute to the tone and structure of the content. The model should be able to maintain appropriate punctuation in summaries.\n",
        "\n",
        "- Lexical Diversity: Calculate the richness of vocabulary in relation to the overall text length. A higher lexical diversity suggests a broader range of ideas and concepts that the model needs to capture in its summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"text_column\": \"input\",\n",
        "    \"unwanted_tokens\": {'s', 's\\'', 'mr', 'ms', 'mrs', 'dr', '\\'s', ' ', \"''\", 'dollar', 'us', '``'},\n",
        "    \"lang\": \"english\"\n",
        "}\n",
        "\n",
        "df_text_description = text_description_table(df_summarization, params)\n",
        "display(df_text_description)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Text Description Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"text_column\": \"input\",\n",
        "    \"unwanted_tokens\": {'s', 's\\'', 'mr', 'ms', 'mrs', 'dr', '\\'s', ' ', \"''\", 'dollar', 'us', '``'},\n",
        "    \"lang\": \"english\"\n",
        "}\n",
        "\n",
        "text_description_histograms(df_summarization, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Text Description Scatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the combinations you want to plot\n",
        "combinations_to_plot = [\n",
        "    (\"Total Words\", \"Total Sentences\"),\n",
        "    (\"Total Words\", \"Total Unique Words\"),\n",
        "    (\"Total Sentences\", \"Avg Sentence Length\"),\n",
        "    (\"Total Unique Words\", \"Lexical Diversity\")\n",
        "]\n",
        "\n",
        "params = {\n",
        "    \"combinations_to_plot\": combinations_to_plot,\n",
        "    \"text_column\": \"input\",\n",
        "    \"unwanted_tokens\": {'s', 's\\'', 'mr', 'ms', 'mrs', 'dr', '\\'s', ' ', \"''\", 'dollar', 'us', '``'},\n",
        "    \"lang\": \"english\"\n",
        "}\n",
        "\n",
        "text_description_scatter(df_summarization, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Text Structure Histogram"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Word Frequencies: This metric provides a histogram of how often words appear with a given frequency. For example, if a lot of words appear only once in a document, it might be indicative of a text rich in unique words. On the other hand, a small set of words appearing very frequently might indicate repetitive content or a certain theme or pattern in the text.\n",
        "\n",
        "- Sentence Positions vs. Sentence Lengths: This bar chart showcases the length of each sentence (in terms of word count) in their order of appearance in the document. This can give insights into the flow of information in a text, highlighting any long, detailed sections or brief, potentially superficial areas.\n",
        "\n",
        "- Sentence Lengths Distribution: A histogram showing the frequency of sentence lengths across the document. Long sentences might contain a lot of information but could be harder for summarization models to digest and for readers to comprehend. Conversely, many short sentences might indicate fragmented information.\n",
        "\n",
        "- Word Lengths Distribution: A histogram of the lengths of words in the document. Extremely long words might be anomalies, technical terms, or potential errors in the corpus. Conversely, a majority of very short words might denote lack of depth or specificity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"text_column\": 'input',\n",
        "    \"num_docs_to_plot\": 2\n",
        "}\n",
        "\n",
        "text_structure_histograms(df_summarization, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Validation of Generated Text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generated Summary Description"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Token Distribution Histograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_columns = [\"input\", \"reference_summary\", \"extractive_summary\", \"abstract_summary\"]\n",
        "\n",
        "params = {\n",
        "    \"text_columns\": text_columns,\n",
        "}\n",
        "\n",
        "token_distribution_histograms(df_summarization, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generated Summary Accuracy "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ROUGE-N Score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ROUGE score ((Recall-Oriented Understudy for Gisting Evaluation) is a widely adopted set of metrics used for evaluating automatic summarization and machine translation. It fundamentally measures the overlap between the n-grams in the generated summary and those in the reference summary.\n",
        "\n",
        "- ROUGE-N: This evaluates the overlap of n-grams between the produced summary and reference summary. It calculates precision (the proportion of n-grams in the generated summary that are also present in the reference summary), recall (the proportion of n-grams in the reference summary that are also present in the generated summary), and F1 score (the harmonic mean of precision and recall).\n",
        "\n",
        "- ROUGE-L: This metric is based on the Longest Common Subsequence (LCS) between the generated and reference summaries. LCS measures the longest sequence of tokens in the generated summary that matches the reference, without considering the order. It's beneficial because it can identify and reward longer coherent matching sequences.\n",
        "\n",
        "- ROUGE-S: This measures the skip-bigram overlap, considering the pair of words in order as \"bigrams\" while allowing arbitrary gaps or \"skips\". This can be valuable to capture sentence-level structure similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"text_columns\": text_columns,\n",
        "    \"metric\": \"rouge-2\",\n",
        "}\n",
        "\n",
        "rouge_scores_plot(df_summarization, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generated Summary Toxicity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Toxicity Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"hf_toxicity_obj\": toxicity,\n",
        "    \"text_columns\": text_columns,\n",
        "    \"max_toxicity_threshold\": 0.7,\n",
        "    \"min_toxicity_threshold\": 0\n",
        "}\n",
        "\n",
        "df_metric_results = hf_toxicity_table(df_summarization, params)\n",
        "display(df_metric_results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Toxicity Histograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"hf_toxicity_obj\": toxicity,\n",
        "    \"text_columns\": text_columns\n",
        "}\n",
        "\n",
        "hf_toxicity_histograms(df_summarization, params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Toxicity Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"hf_toxicity_obj\": toxicity,\n",
        "    \"text_columns\": text_columns,\n",
        "    \"score_threshold\": 0\n",
        "}\n",
        "hf_toxicity_plot(df_summarization, params)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Developer Framework (Python 3.10.13)",
      "language": "python",
      "name": "dev-framework"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
