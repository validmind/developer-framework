{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFP RAG w/o LangChain Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q qdrant-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the existing questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "import os\n",
    "from csv import DictReader\n",
    "from uuid import uuid4\n",
    "\n",
    "## grab all csv files with the name  \"rfp_existing_questions_<client_name>.csv\"\n",
    "documents = []\n",
    "root_dir = \"datasets/rag/\"\n",
    "for file in os.listdir(root_dir):\n",
    "    if file.startswith(\"rfp_existing_questions_\") and file.endswith(\".csv\"):\n",
    "        # use csv dict reader to load the csv file\n",
    "        with open(os.path.join(root_dir, file)) as f:\n",
    "            reader = DictReader(f)\n",
    "            for row in reader:\n",
    "                # add a unique id to the row\n",
    "                row[\"id\"] = str(uuid4())\n",
    "                documents.append(row)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# now we can import and use the openai client\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    return client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\",\n",
    "    ).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for doc in tqdm(documents):\n",
    "    doc[\"embedding_question\"] = get_embedding(doc[\"RFP_Question\"])\n",
    "    doc[\"embedding_answer\"] = get_embedding(doc[\"RFP_Answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Distance, PointStruct, VectorParams\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    \"my_rag_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "qdrant.upsert(\n",
    "    \"my_rag_collection\",\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=doc[\"id\"],\n",
    "            vector=doc[\"embedding_question\"],\n",
    "            payload={\n",
    "                k: v\n",
    "                for k, v in doc.items()\n",
    "                if k not in [\"embedding_question\", \"embedding_answer\"]\n",
    "            },\n",
    "        )\n",
    "        for doc in documents\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the new questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new questions from `rfp_new_questions_client_100.csv`\n",
    "new_questions = []\n",
    "root_dir = \"datasets/rag/\"\n",
    "for file in os.listdir(root_dir):\n",
    "    if file.startswith(\"rfp_new_questions_\") and file.endswith(\".csv\"):\n",
    "        # use csv dict reader to load the csv file\n",
    "        with open(os.path.join(root_dir, file)) as f:\n",
    "            reader = DictReader(f)\n",
    "            for row in reader:\n",
    "                # add a unique id to the row\n",
    "                row[\"id\"] = str(uuid4())\n",
    "                new_questions.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `query` function for Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question, limit=10, _print=True):\n",
    "    \"\"\"Returns the top 10 results for the given question. If _print is True, it will print the results.\"\"\"\n",
    "    response = qdrant.search(\n",
    "        \"my_rag_collection\",\n",
    "        query_vector=get_embedding(question),\n",
    "        limit=limit,\n",
    "    )\n",
    "\n",
    "    if _print:\n",
    "        print(f\"For the question:\\n\\\"{new_questions[0]['RFP_Question']}\\\"\\n\")\n",
    "        print(f\"Top 10 results are:\\n\")\n",
    "        for result in response:\n",
    "            print(f\"Score: {result.score} (higher is better)\")\n",
    "            print(f\"Question: {result.payload['RFP_Question']}\")\n",
    "            print(f\"Answer: {result.payload['RFP_Answer']}\\n\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `query` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query(new_questions[0][\"RFP_Question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Pipeline that takes question, queries Vector DB and generates an answer with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert RFP AI assistant.\n",
    "You are tasked with answering new RFP questions based on existing RFP questions and answers.\n",
    "You will be provided with the existing RFP questions and answer pairs that are the most relevant to the new RFP question.\n",
    "After that you will be provided with a new RFP question.\n",
    "You will generate an answer and respond only with the answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_answer(question):\n",
    "    \"\"\"Generates an answer for the given question using RAG and an LLM.\"\"\"\n",
    "    context = \"\"\n",
    "    results = query(new_questions[0][\"RFP_Question\"], _print=False)\n",
    "    for result in results:\n",
    "        context += f\"Q: {result.payload['RFP_Question']}\\nA: {result.payload['RFP_Answer']}\"\n",
    "        context += f\"Model: {result.payload['Model_Title']}\\n\\n\"\n",
    "\n",
    "    print(f\"For Question: \\\"{question}\\\"\\n\")\n",
    "    print(f\"Context:\\n```\\n{context}```\\n\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    print(f\"Answer:\\n{answer}\")\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = generate_answer(new_questions[0][\"RFP_Question\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
