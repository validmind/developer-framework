{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Framework RAG Model Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "import os\n",
    "from csv import DictReader\n",
    "from uuid import uuid4\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "column_map = {\"RFP_Question\": \"question\", \"RFP_Answer\": \"ground_truth\"}\n",
    "\n",
    "\n",
    "def load_documents(prefix):\n",
    "    documents = []\n",
    "    root_dir = \"datasets/rag/\"\n",
    "    for file in os.listdir(root_dir):\n",
    "        if file.startswith(prefix) and file.endswith(\".csv\"):\n",
    "            # use csv dict reader to load the csv file\n",
    "            with open(os.path.join(root_dir, file)) as f:\n",
    "                reader = DictReader(f)\n",
    "                for row in reader:\n",
    "                    # add a unique id to the row\n",
    "                    row[\"id\"] = str(uuid4())\n",
    "                    documents.append(row)\n",
    "\n",
    "    df = pd.DataFrame(documents)\n",
    "    df = df[[\"id\", \"RFP_Question\", \"RFP_Answer\"]]\n",
    "    # df.rename(columns=column_map, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_dataset_split(limit=None):\n",
    "    df = load_documents(\"rfp_existing_questions\")\n",
    "\n",
    "    if limit:\n",
    "        df = df.head(limit)\n",
    "\n",
    "    # split the dataset into a \"train\" - which gets inserted into the vector store\n",
    "    # and a \"test\" - which is used to evaluate the search results\n",
    "    train_df = df.sample(frac=0.8)\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Selection\n",
    "\n",
    "First let's setup our embedding model and run some tests to make sure its working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from validmind.models import FunctionModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def embed(input):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    input[\"embedding\"] = (\n",
    "        client.embeddings.create(\n",
    "            input=input[\"RFP_Question\"],\n",
    "            model=\"text-embedding-3-small\",\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n",
    "    return input\n",
    "\n",
    "vm_embedder = FunctionModel(input_id=\"embedding_model\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our test dataset so we can run it through our different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 13:34:35,965 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b91395d4-44f8-45f3-bfcf-95dc50812ffc</td>\n",
       "      <td>What actions do you undertake to secure user d...</td>\n",
       "      <td>User privacy and data security are paramount. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7d878600-9fd4-4d91-997e-ae4e800407f0</td>\n",
       "      <td>How do your LLMs continuously learn and update...</td>\n",
       "      <td>We implement advanced continuous learning mech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>61acbd2c-e04f-4b59-a38f-f902f9ae552b</td>\n",
       "      <td>How do you ensure your LLMs can efficiently ha...</td>\n",
       "      <td>We conduct extensive performance testing under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>f1f4289e-b136-4cb2-a124-ef4d02dc7c59</td>\n",
       "      <td>What is your approach to maintaining and suppo...</td>\n",
       "      <td>Our post-deployment support is designed to ens...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "3   b91395d4-44f8-45f3-bfcf-95dc50812ffc   \n",
       "9   7d878600-9fd4-4d91-997e-ae4e800407f0   \n",
       "11  61acbd2c-e04f-4b59-a38f-f902f9ae552b   \n",
       "14  f1f4289e-b136-4cb2-a124-ef4d02dc7c59   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "3   What actions do you undertake to secure user d...   \n",
       "9   How do your LLMs continuously learn and update...   \n",
       "11  How do you ensure your LLMs can efficiently ha...   \n",
       "14  What is your approach to maintaining and suppo...   \n",
       "\n",
       "                                           RFP_Answer  \n",
       "3   User privacy and data security are paramount. ...  \n",
       "9   We implement advanced continuous learning mech...  \n",
       "11  We conduct extensive performance testing under...  \n",
       "14  Our post-deployment support is designed to ens...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "train_df, test_df = load_dataset_split(20)\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    test_df,\n",
    "    text_column=\"RFP_Question\", # some NLP which work with text data require a `text_column` to be specified\n",
    "    target_column=\"RFP_Answer\",\n",
    "    __log=False,\n",
    ")\n",
    "\n",
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 13:34:35,972 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-07 13:34:35,972 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-07 13:34:35,972 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-07 13:34:36,959 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    }
   ],
   "source": [
    "vm_test_ds.assign_predictions(vm_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>embedding_model_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b91395d4-44f8-45f3-bfcf-95dc50812ffc</td>\n",
       "      <td>What actions do you undertake to secure user d...</td>\n",
       "      <td>User privacy and data security are paramount. ...</td>\n",
       "      <td>{'embedding': [0.007698851637542248, 0.0075916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7d878600-9fd4-4d91-997e-ae4e800407f0</td>\n",
       "      <td>How do your LLMs continuously learn and update...</td>\n",
       "      <td>We implement advanced continuous learning mech...</td>\n",
       "      <td>{'embedding': [-0.010829819366335869, 0.029368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>61acbd2c-e04f-4b59-a38f-f902f9ae552b</td>\n",
       "      <td>How do you ensure your LLMs can efficiently ha...</td>\n",
       "      <td>We conduct extensive performance testing under...</td>\n",
       "      <td>{'embedding': [0.02485279180109501, 0.03754840...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>f1f4289e-b136-4cb2-a124-ef4d02dc7c59</td>\n",
       "      <td>What is your approach to maintaining and suppo...</td>\n",
       "      <td>Our post-deployment support is designed to ens...</td>\n",
       "      <td>{'embedding': [-0.004068069159984589, 0.049269...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "3   b91395d4-44f8-45f3-bfcf-95dc50812ffc   \n",
       "9   7d878600-9fd4-4d91-997e-ae4e800407f0   \n",
       "11  61acbd2c-e04f-4b59-a38f-f902f9ae552b   \n",
       "14  f1f4289e-b136-4cb2-a124-ef4d02dc7c59   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "3   What actions do you undertake to secure user d...   \n",
       "9   How do your LLMs continuously learn and update...   \n",
       "11  How do you ensure your LLMs can efficiently ha...   \n",
       "14  What is your approach to maintaining and suppo...   \n",
       "\n",
       "                                           RFP_Answer  \\\n",
       "3   User privacy and data security are paramount. ...   \n",
       "9   We implement advanced continuous learning mech...   \n",
       "11  We conduct extensive performance testing under...   \n",
       "14  Our post-deployment support is designed to ens...   \n",
       "\n",
       "                           embedding_model_prediction  \n",
       "3   {'embedding': [0.007698851637542248, 0.0075916...  \n",
       "9   {'embedding': [-0.010829819366335869, 0.029368...  \n",
       "11  {'embedding': [0.02485279180109501, 0.03754840...  \n",
       "14  {'embedding': [-0.004068069159984589, 0.049269...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and run one of the ValidMind embeddings stability analysis tests to make sure our embeddings model is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "# result = run_test(\n",
    "#     \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise\",\n",
    "#     inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "#     params={\"probability\": 0.3},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings for the questions\n",
    "\n",
    "> Note: We use the name `train_df` to refer to the dataset that is loaded into the vector store and used as context. This is not a great name but its consistent with data science terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>68d94049-74b8-465f-ac5b-4d4179d54032</td>\n",
       "      <td>What measures do you employ to ensure your LLM...</td>\n",
       "      <td>We prioritize transparency and explainability ...</td>\n",
       "      <td>[0.010077687911689281, 0.02444615587592125, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>964fb6ab-68c8-4248-989a-74a74eec7d03</td>\n",
       "      <td>What considerations do you take into account f...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "      <td>[-0.0029329643584787846, -0.003287967294454574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>c41ed83b-a827-456f-b6da-f4256aaca536</td>\n",
       "      <td>Could you provide case studies on successful L...</td>\n",
       "      <td>We can share case studies of successful LLM-ba...</td>\n",
       "      <td>[-0.002833468606695533, 0.011640194803476334, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3e598d8d-55c4-4946-a9d7-a580344e941b</td>\n",
       "      <td>How do you evaluate the success of your AI app...</td>\n",
       "      <td>Success measurement is tailored to each projec...</td>\n",
       "      <td>[0.014263585209846497, 0.022252684459090233, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b0d46173-6f05-4ac4-9700-a6d8bcb61e43</td>\n",
       "      <td>How do you perform risk assessment and identif...</td>\n",
       "      <td>We conduct thorough assessments of AI systems ...</td>\n",
       "      <td>[-0.011703639291226864, 0.023393888026475906, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "10  68d94049-74b8-465f-ac5b-4d4179d54032   \n",
       "4   964fb6ab-68c8-4248-989a-74a74eec7d03   \n",
       "12  c41ed83b-a827-456f-b6da-f4256aaca536   \n",
       "6   3e598d8d-55c4-4946-a9d7-a580344e941b   \n",
       "17  b0d46173-6f05-4ac4-9700-a6d8bcb61e43   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "10  What measures do you employ to ensure your LLM...   \n",
       "4   What considerations do you take into account f...   \n",
       "12  Could you provide case studies on successful L...   \n",
       "6   How do you evaluate the success of your AI app...   \n",
       "17  How do you perform risk assessment and identif...   \n",
       "\n",
       "                                           RFP_Answer  \\\n",
       "10  We prioritize transparency and explainability ...   \n",
       "4   Our design philosophy centers on simplicity an...   \n",
       "12  We can share case studies of successful LLM-ba...   \n",
       "6   Success measurement is tailored to each projec...   \n",
       "17  We conduct thorough assessments of AI systems ...   \n",
       "\n",
       "                                            embedding  \n",
       "10  [0.010077687911689281, 0.02444615587592125, 0....  \n",
       "4   [-0.0029329643584787846, -0.003287967294454574...  \n",
       "12  [-0.002833468606695533, 0.011640194803476334, ...  \n",
       "6   [0.014263585209846497, 0.022252684459090233, 0...  \n",
       "17  [-0.011703639291226864, 0.023393888026475906, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"embedding\"] = [embed(row)[\"embedding\"] for _, row in train_df.iterrows()]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, PointStruct, VectorParams\n",
    "\n",
    "qdrant = QdrantClient(\":memory:\")\n",
    "qdrant.recreate_collection(\n",
    "    \"rfp_rag_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "qdrant.upsert(\n",
    "    \"rfp_rag_collection\",\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=row[\"id\"],\n",
    "            vector=row[\"embedding\"],\n",
    "            payload={\"RFP_Question\": row[\"RFP_Question\"], \"RFP_Answer\": row[\"RFP_Answer\"]},\n",
    "        )\n",
    "        for _, row in train_df.iterrows()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def retrieve(input):\n",
    "    input[\"contexts\"] = []\n",
    "\n",
    "    for result in qdrant.search(\n",
    "        \"rfp_rag_collection\",\n",
    "        query_vector=input[\"embedding\"],\n",
    "        limit=input.get(\n",
    "            \"limit\", 10\n",
    "        ),  # we could add a row to the dataset to specify a limit\n",
    "    ):\n",
    "        context = f\"Q: {result.payload['RFP_Question']}\\n\"\n",
    "        context += f\"A: {result.payload['RFP_Answer']}\\n\"\n",
    "\n",
    "        input[\"contexts\"].append(context)\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "vm_retriever = FunctionModel(input_id=\"retrieval_model\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 13:34:41,159 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-07 13:34:41,159 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-07 13:34:41,159 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-07 13:34:41,161 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    }
   ],
   "source": [
    "vm_test_ds.assign_predictions(vm_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>embedding_model_prediction</th>\n",
       "      <th>retrieval_model_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b91395d4-44f8-45f3-bfcf-95dc50812ffc</td>\n",
       "      <td>What actions do you undertake to secure user d...</td>\n",
       "      <td>User privacy and data security are paramount. ...</td>\n",
       "      <td>{'embedding': [0.007698851637542248, 0.0075916...</td>\n",
       "      <td>{'contexts': ['Q: What steps do you take to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7d878600-9fd4-4d91-997e-ae4e800407f0</td>\n",
       "      <td>How do your LLMs continuously learn and update...</td>\n",
       "      <td>We implement advanced continuous learning mech...</td>\n",
       "      <td>{'embedding': [-0.010829819366335869, 0.029368...</td>\n",
       "      <td>{'contexts': ['Q: Please outline the training ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>61acbd2c-e04f-4b59-a38f-f902f9ae552b</td>\n",
       "      <td>How do you ensure your LLMs can efficiently ha...</td>\n",
       "      <td>We conduct extensive performance testing under...</td>\n",
       "      <td>{'embedding': [0.02485279180109501, 0.03754840...</td>\n",
       "      <td>{'contexts': ['Q: Describe your strategy for i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>f1f4289e-b136-4cb2-a124-ef4d02dc7c59</td>\n",
       "      <td>What is your approach to maintaining and suppo...</td>\n",
       "      <td>Our post-deployment support is designed to ens...</td>\n",
       "      <td>{'embedding': [-0.004068069159984589, 0.049269...</td>\n",
       "      <td>{'contexts': ['Q: Could you provide case studi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "3   b91395d4-44f8-45f3-bfcf-95dc50812ffc   \n",
       "9   7d878600-9fd4-4d91-997e-ae4e800407f0   \n",
       "11  61acbd2c-e04f-4b59-a38f-f902f9ae552b   \n",
       "14  f1f4289e-b136-4cb2-a124-ef4d02dc7c59   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "3   What actions do you undertake to secure user d...   \n",
       "9   How do your LLMs continuously learn and update...   \n",
       "11  How do you ensure your LLMs can efficiently ha...   \n",
       "14  What is your approach to maintaining and suppo...   \n",
       "\n",
       "                                           RFP_Answer  \\\n",
       "3   User privacy and data security are paramount. ...   \n",
       "9   We implement advanced continuous learning mech...   \n",
       "11  We conduct extensive performance testing under...   \n",
       "14  Our post-deployment support is designed to ens...   \n",
       "\n",
       "                           embedding_model_prediction  \\\n",
       "3   {'embedding': [0.007698851637542248, 0.0075916...   \n",
       "9   {'embedding': [-0.010829819366335869, 0.029368...   \n",
       "11  {'embedding': [0.02485279180109501, 0.03754840...   \n",
       "14  {'embedding': [-0.004068069159984589, 0.049269...   \n",
       "\n",
       "                           retrieval_model_prediction  \n",
       "3   {'contexts': ['Q: What steps do you take to en...  \n",
       "9   {'contexts': ['Q: Please outline the training ...  \n",
       "11  {'contexts': ['Q: Describe your strategy for i...  \n",
       "14  {'contexts': ['Q: Could you provide case studi...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert RFP AI assistant.\n",
    "You are tasked with answering new RFP questions based on existing RFP questions and answers.\n",
    "You will be provided with the existing RFP questions and answer pairs that are the most relevant to the new RFP question.\n",
    "After that you will be provided with a new RFP question.\n",
    "You will generate an answer and respond only with the answer.\n",
    "Ignore your pre-existing knowledge and answer the question based on the provided context.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate(input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\\n\".join(input[\"contexts\"])},\n",
    "            {\"role\": \"user\", \"content\": input[\"RFP_Question\"]},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    input[\"answer\"] = response.choices[0].message.content\n",
    "\n",
    "    return input\n",
    "\n",
    "vm_generator = FunctionModel(input_id=\"generation_model\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 13:34:41,247 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-07 13:34:41,248 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-07 13:34:41,249 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-07 13:34:52,296 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    }
   ],
   "source": [
    "vm_test_ds.assign_predictions(vm_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>embedding_model_prediction</th>\n",
       "      <th>retrieval_model_prediction</th>\n",
       "      <th>generation_model_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b91395d4-44f8-45f3-bfcf-95dc50812ffc</td>\n",
       "      <td>What actions do you undertake to secure user d...</td>\n",
       "      <td>User privacy and data security are paramount. ...</td>\n",
       "      <td>{'embedding': [0.007698851637542248, 0.0075916...</td>\n",
       "      <td>{'contexts': ['Q: What steps do you take to en...</td>\n",
       "      <td>{'answer': 'We prioritize data security and pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7d878600-9fd4-4d91-997e-ae4e800407f0</td>\n",
       "      <td>How do your LLMs continuously learn and update...</td>\n",
       "      <td>We implement advanced continuous learning mech...</td>\n",
       "      <td>{'embedding': [-0.010829819366335869, 0.029368...</td>\n",
       "      <td>{'contexts': ['Q: Please outline the training ...</td>\n",
       "      <td>{'answer': 'Our LLMs are designed to adapt and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>61acbd2c-e04f-4b59-a38f-f902f9ae552b</td>\n",
       "      <td>How do you ensure your LLMs can efficiently ha...</td>\n",
       "      <td>We conduct extensive performance testing under...</td>\n",
       "      <td>{'embedding': [0.02485279180109501, 0.03754840...</td>\n",
       "      <td>{'contexts': ['Q: Describe your strategy for i...</td>\n",
       "      <td>{'answer': 'To ensure our Large Language Model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>f1f4289e-b136-4cb2-a124-ef4d02dc7c59</td>\n",
       "      <td>What is your approach to maintaining and suppo...</td>\n",
       "      <td>Our post-deployment support is designed to ens...</td>\n",
       "      <td>{'embedding': [-0.004068069159984589, 0.049269...</td>\n",
       "      <td>{'contexts': ['Q: Could you provide case studi...</td>\n",
       "      <td>{'answer': 'We provide ongoing support and mai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "3   b91395d4-44f8-45f3-bfcf-95dc50812ffc   \n",
       "9   7d878600-9fd4-4d91-997e-ae4e800407f0   \n",
       "11  61acbd2c-e04f-4b59-a38f-f902f9ae552b   \n",
       "14  f1f4289e-b136-4cb2-a124-ef4d02dc7c59   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "3   What actions do you undertake to secure user d...   \n",
       "9   How do your LLMs continuously learn and update...   \n",
       "11  How do you ensure your LLMs can efficiently ha...   \n",
       "14  What is your approach to maintaining and suppo...   \n",
       "\n",
       "                                           RFP_Answer  \\\n",
       "3   User privacy and data security are paramount. ...   \n",
       "9   We implement advanced continuous learning mech...   \n",
       "11  We conduct extensive performance testing under...   \n",
       "14  Our post-deployment support is designed to ens...   \n",
       "\n",
       "                           embedding_model_prediction  \\\n",
       "3   {'embedding': [0.007698851637542248, 0.0075916...   \n",
       "9   {'embedding': [-0.010829819366335869, 0.029368...   \n",
       "11  {'embedding': [0.02485279180109501, 0.03754840...   \n",
       "14  {'embedding': [-0.004068069159984589, 0.049269...   \n",
       "\n",
       "                           retrieval_model_prediction  \\\n",
       "3   {'contexts': ['Q: What steps do you take to en...   \n",
       "9   {'contexts': ['Q: Please outline the training ...   \n",
       "11  {'contexts': ['Q: Describe your strategy for i...   \n",
       "14  {'contexts': ['Q: Could you provide case studi...   \n",
       "\n",
       "                          generation_model_prediction  \n",
       "3   {'answer': 'We prioritize data security and pr...  \n",
       "9   {'answer': 'Our LLMs are designed to adapt and...  \n",
       "11  {'answer': 'To ensure our Large Language Model...  \n",
       "14  {'answer': 'We provide ongoing support and mai...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAG Model (Pipeline of \"Component\" Models)\n",
    "\n",
    "Now that we have our individual models setup, let's create a `RAGModel` instance that will chain them together and give us a single model that can be evalated end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from validmind.models import PipelineModel\n",
    "\n",
    "vm_rag_model = PipelineModel(vm_embedder | vm_retriever | vm_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the test dataset through the entire pipeline. It will overwrite the current predictions that we generated from the individual models, but the key here is that calling `predict` on the `RAGModel` will run the entire pipeline and store the intermediate predictions in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m result_df \u001b[38;5;241m=\u001b[39m vm_rag_model\u001b[38;5;241m.\u001b[39mpredict(vm_test_ds\u001b[38;5;241m.\u001b[39mdf)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mresult_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "result_df = vm_rag_model.predict(vm_test_ds.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding_model': [{'embedding': [0.007698851637542248,\n",
       "    0.007591660600155592,\n",
       "    0.07263781875371933,\n",
       "    0.028500253334641457,\n",
       "    0.023102860897779465,\n",
       "    -0.020782485604286194,\n",
       "    0.00657649664208293,\n",
       "    0.023304631933569908,\n",
       "    -0.0067026037722826,\n",
       "    0.007244865410029888,\n",
       "    0.026356428861618042,\n",
       "    -0.03884105384349823,\n",
       "    0.0004776314599439502,\n",
       "    -0.03026575595140457,\n",
       "    -0.01844950020313263,\n",
       "    -0.012497235089540482,\n",
       "    -0.0060310824774205685,\n",
       "    0.015006771311163902,\n",
       "    -0.01144423894584179,\n",
       "    -0.0029162317514419556,\n",
       "    0.0024244131054729223,\n",
       "    0.05072036385536194,\n",
       "    0.00574418855831027,\n",
       "    0.016835326328873634,\n",
       "    -0.015826469287276268,\n",
       "    -0.0134052075445652,\n",
       "    0.022560598328709602,\n",
       "    0.014149241149425507,\n",
       "    -0.025612397119402885,\n",
       "    -0.03901760280132294,\n",
       "    0.03510827571153641,\n",
       "    -0.021955283358693123,\n",
       "    -0.06623156368732452,\n",
       "    0.029055126011371613,\n",
       "    0.015322038903832436,\n",
       "    -0.02414955198764801,\n",
       "    -0.01686054840683937,\n",
       "    -0.018499944359064102,\n",
       "    0.010214692912995815,\n",
       "    0.001694567035883665,\n",
       "    -0.001003341400064528,\n",
       "    -0.05543677881360054,\n",
       "    -7.216688391054049e-05,\n",
       "    0.0012650140561163425,\n",
       "    -0.026532979682087898,\n",
       "    -0.014111408963799477,\n",
       "    0.001369052566587925,\n",
       "    -0.04252338781952858,\n",
       "    -0.032031260430812836,\n",
       "    -0.00784387532621622,\n",
       "    -0.030694521963596344,\n",
       "    0.0012697430793195963,\n",
       "    -0.03334277495741844,\n",
       "    0.00012108273222111166,\n",
       "    0.017818963155150414,\n",
       "    0.02297675423324108,\n",
       "    0.01617956906557083,\n",
       "    -0.010750648565590382,\n",
       "    -0.01829817146062851,\n",
       "    0.031652938574552536,\n",
       "    -0.05019071325659752,\n",
       "    -0.006213937886059284,\n",
       "    -0.0005359561182558537,\n",
       "    0.01940791681408882,\n",
       "    0.027617502957582474,\n",
       "    -0.01228285301476717,\n",
       "    -0.0030849003233015537,\n",
       "    0.012049553915858269,\n",
       "    0.01303949672728777,\n",
       "    -0.0023298326414078474,\n",
       "    0.03117372840642929,\n",
       "    -0.025637617334723473,\n",
       "    0.007736683823168278,\n",
       "    -0.0157634150236845,\n",
       "    0.01587691158056259,\n",
       "    -0.04166585952043533,\n",
       "    -0.009079727344214916,\n",
       "    0.026205100119113922,\n",
       "    -0.020984258502721786,\n",
       "    -0.033090561628341675,\n",
       "    0.015385093167424202,\n",
       "    0.01799551397562027,\n",
       "    -0.015422925353050232,\n",
       "    0.03319144621491432,\n",
       "    -0.034225527197122574,\n",
       "    0.028071489185094833,\n",
       "    -0.05558810755610466,\n",
       "    0.03641979396343231,\n",
       "    -0.07505907863378525,\n",
       "    -0.0180333461612463,\n",
       "    0.012976443395018578,\n",
       "    0.03710077330470085,\n",
       "    -0.054427918046712875,\n",
       "    -0.024275658652186394,\n",
       "    0.015498589724302292,\n",
       "    0.006797184236347675,\n",
       "    0.08509721606969833,\n",
       "    -0.010971336625516415,\n",
       "    0.00984898116439581,\n",
       "    0.03773130849003792,\n",
       "    -0.007106147240847349,\n",
       "    -0.05190577358007431,\n",
       "    -0.0014462933177128434,\n",
       "    0.04143886640667915,\n",
       "    0.011696454137563705,\n",
       "    -0.02528451755642891,\n",
       "    0.0191683117300272,\n",
       "    0.010933504439890385,\n",
       "    -0.011627094820141792,\n",
       "    -0.010391242802143097,\n",
       "    -0.046609263867139816,\n",
       "    -0.027844496071338654,\n",
       "    -0.016015630215406418,\n",
       "    -0.009010368026793003,\n",
       "    -0.024817919358611107,\n",
       "    0.00041930683073587716,\n",
       "    -0.018348615616559982,\n",
       "    0.015788637101650238,\n",
       "    0.03283834457397461,\n",
       "    -0.005501431878656149,\n",
       "    -0.0364450141787529,\n",
       "    0.005920738447457552,\n",
       "    0.02630598656833172,\n",
       "    0.012345906347036362,\n",
       "    0.030820628628134727,\n",
       "    -0.014111408963799477,\n",
       "    0.0261546578258276,\n",
       "    -0.0810113400220871,\n",
       "    -0.03291400894522667,\n",
       "    -0.015675140544772148,\n",
       "    0.026356428861618042,\n",
       "    0.014691502787172794,\n",
       "    0.07258737087249756,\n",
       "    -0.04877831041812897,\n",
       "    0.0068287113681435585,\n",
       "    -0.025826778262853622,\n",
       "    0.04050567001104355,\n",
       "    0.057656265795230865,\n",
       "    0.008625740185379982,\n",
       "    -0.014817609451711178,\n",
       "    -0.003937701229006052,\n",
       "    0.00566537119448185,\n",
       "    -0.026558201760053635,\n",
       "    0.04219551011919975,\n",
       "    -0.08171754330396652,\n",
       "    -0.05881645157933235,\n",
       "    -0.03440207615494728,\n",
       "    0.03160249441862106,\n",
       "    -0.02324157953262329,\n",
       "    0.03276268020272255,\n",
       "    -0.016419172286987305,\n",
       "    0.011816255748271942,\n",
       "    -0.04282604530453682,\n",
       "    0.03742865100502968,\n",
       "    -0.023140693083405495,\n",
       "    0.031123286113142967,\n",
       "    0.04143886640667915,\n",
       "    0.06860238313674927,\n",
       "    -0.01243418175727129,\n",
       "    -0.039925575256347656,\n",
       "    -0.01780635304749012,\n",
       "    0.04870264604687691,\n",
       "    -0.028903797268867493,\n",
       "    -0.044137559831142426,\n",
       "    -0.07334401458501816,\n",
       "    -0.002925689797848463,\n",
       "    0.026633866131305695,\n",
       "    -0.07102364301681519,\n",
       "    -0.03901760280132294,\n",
       "    -0.010605625808238983,\n",
       "    -0.036697231233119965,\n",
       "    -0.004256121814250946,\n",
       "    -0.0007389100501313806,\n",
       "    -0.017491085454821587,\n",
       "    0.01693621277809143,\n",
       "    -0.04010212793946266,\n",
       "    0.004804688971489668,\n",
       "    0.023720785975456238,\n",
       "    -0.004079571925103664,\n",
       "    -0.04640749469399452,\n",
       "    -0.055184561759233475,\n",
       "    -0.014275348745286465,\n",
       "    -0.04416278377175331,\n",
       "    -0.03846273198723793,\n",
       "    -0.03576403483748436,\n",
       "    0.026759972795844078,\n",
       "    -0.04141364246606827,\n",
       "    -0.005463599693030119,\n",
       "    -0.04103532060980797,\n",
       "    0.049257516860961914,\n",
       "    0.04292692989110947,\n",
       "    0.06355808675289154,\n",
       "    0.0195466335862875,\n",
       "    0.012976443395018578,\n",
       "    0.018209896981716156,\n",
       "    -0.01678488403558731,\n",
       "    -0.015435535460710526,\n",
       "    -0.021930063143372536,\n",
       "    -0.0015219576889649034,\n",
       "    -0.015776025131344795,\n",
       "    0.020215002819895744,\n",
       "    -0.013871804811060429,\n",
       "    -0.029937878251075745,\n",
       "    -0.02358206920325756,\n",
       "    0.06093505769968033,\n",
       "    -0.004546168725937605,\n",
       "    0.004032281693071127,\n",
       "    0.017327144742012024,\n",
       "    -0.05372171849012375,\n",
       "    -0.00935716275125742,\n",
       "    0.013833972625434399,\n",
       "    -0.012024332769215107,\n",
       "    0.030744964256882668,\n",
       "    0.004729024600237608,\n",
       "    -0.08499633520841599,\n",
       "    0.03987513482570648,\n",
       "    -0.005428920034319162,\n",
       "    0.014514951966702938,\n",
       "    -0.0009505338966846466,\n",
       "    0.0069106812588870525,\n",
       "    0.016381340101361275,\n",
       "    0.051653556525707245,\n",
       "    0.04943406954407692,\n",
       "    0.0014691503020003438,\n",
       "    0.02141302265226841,\n",
       "    0.023897336795926094,\n",
       "    0.04411233961582184,\n",
       "    -0.03374631702899933,\n",
       "    0.02320374734699726,\n",
       "    -0.01882782205939293,\n",
       "    0.014489730820059776,\n",
       "    0.02320374734699726,\n",
       "    -0.03170337900519371,\n",
       "    0.004590306431055069,\n",
       "    -0.0020082590635865927,\n",
       "    -0.05725272372364998,\n",
       "    0.014615838415920734,\n",
       "    -0.027541838586330414,\n",
       "    0.05921999737620354,\n",
       "    0.020303279161453247,\n",
       "    -0.001878999057225883,\n",
       "    -0.02734006568789482,\n",
       "    -0.026684308424592018,\n",
       "    0.023443350568413734,\n",
       "    0.0187899898737669,\n",
       "    0.03170337900519371,\n",
       "    0.010870451107621193,\n",
       "    -0.02607899345457554,\n",
       "    0.03483084216713905,\n",
       "    0.005406851414591074,\n",
       "    0.022875867784023285,\n",
       "    0.006185563746839762,\n",
       "    0.04469243437051773,\n",
       "    -0.013417818583548069,\n",
       "    -0.02570067159831524,\n",
       "    -0.018928708508610725,\n",
       "    -0.005183010827749968,\n",
       "    0.023632511496543884,\n",
       "    -0.008550075814127922,\n",
       "    0.025044914335012436,\n",
       "    -0.0007550675654783845,\n",
       "    0.011091138236224651,\n",
       "    -0.041211873292922974,\n",
       "    -0.012099997140467167,\n",
       "    0.04073266312479973,\n",
       "    0.028071489185094833,\n",
       "    -0.02384689450263977,\n",
       "    -0.047290243208408356,\n",
       "    -0.029584776610136032,\n",
       "    -0.028878575190901756,\n",
       "    -0.025788946077227592,\n",
       "    0.016356119886040688,\n",
       "    0.009300414472818375,\n",
       "    -0.028651582077145576,\n",
       "    -0.03755475953221321,\n",
       "    0.04476809874176979,\n",
       "    0.053015515208244324,\n",
       "    -0.03924459591507912,\n",
       "    -0.027239181101322174,\n",
       "    -0.005441530607640743,\n",
       "    -0.049408845603466034,\n",
       "    0.02050505019724369,\n",
       "    0.03508305549621582,\n",
       "    0.005466752219945192,\n",
       "    -0.004123709164559841,\n",
       "    0.043532244861125946,\n",
       "    0.018436890095472336,\n",
       "    -0.03132505714893341,\n",
       "    -0.06592890620231628,\n",
       "    0.0066710771061480045,\n",
       "    0.040303897112607956,\n",
       "    -0.009880508296191692,\n",
       "    0.009981393814086914,\n",
       "    0.030139649286866188,\n",
       "    0.004089029971510172,\n",
       "    -0.018890876322984695,\n",
       "    0.05805980786681175,\n",
       "    0.04355746880173683,\n",
       "    -0.01648222655057907,\n",
       "    0.006125662941485643,\n",
       "    0.020290667191147804,\n",
       "    0.0034931728150695562,\n",
       "    -0.027516616508364677,\n",
       "    -0.02513318881392479,\n",
       "    -0.026633866131305695,\n",
       "    0.012137829326093197,\n",
       "    0.0157634150236845,\n",
       "    0.013089939951896667,\n",
       "    0.0165200587362051,\n",
       "    -0.018197286874055862,\n",
       "    0.010485823266208172,\n",
       "    -0.034225527197122574,\n",
       "    0.04020301252603531,\n",
       "    -0.003348149359226227,\n",
       "    -0.029433447867631912,\n",
       "    -0.0466344878077507,\n",
       "    0.01833600364625454,\n",
       "    0.0007148708682507277,\n",
       "    0.01008228026330471,\n",
       "    -0.04408711940050125,\n",
       "    -0.028046267107129097,\n",
       "    -0.029912656173110008,\n",
       "    -0.02088337205350399,\n",
       "    -0.04396101087331772,\n",
       "    -0.030896292999386787,\n",
       "    -0.01106591708958149,\n",
       "    0.01955924555659294,\n",
       "    0.0034963253419846296,\n",
       "    -0.004763703793287277,\n",
       "    -0.053368616849184036,\n",
       "    0.02809671126306057,\n",
       "    -0.0039849914610385895,\n",
       "    -0.011633399873971939,\n",
       "    -0.06915725022554398,\n",
       "    -0.0049623227678239346,\n",
       "    0.030013542622327805,\n",
       "    -0.0026703225448727608,\n",
       "    0.006715214811265469,\n",
       "    -0.023607289418578148,\n",
       "    -0.028525475412607193,\n",
       "    -0.047744229435920715,\n",
       "    -0.055487219244241714,\n",
       "    -0.02983699180185795,\n",
       "    -0.0006908316281624138,\n",
       "    -0.019496191293001175,\n",
       "    -0.011658621951937675,\n",
       "    -0.015700360760092735,\n",
       "    0.02179134450852871,\n",
       "    0.008688794448971748,\n",
       "    0.01810901053249836,\n",
       "    -0.01288816798478365,\n",
       "    -0.02251015603542328,\n",
       "    -0.037680868059396744,\n",
       "    -0.011198329739272594,\n",
       "    -0.04681103676557541,\n",
       "    0.03621802106499672,\n",
       "    0.03810963034629822,\n",
       "    -0.055941205471754074,\n",
       "    -0.007679935544729233,\n",
       "    -0.03634412959218025,\n",
       "    -0.007320529781281948,\n",
       "    -0.020795097574591637,\n",
       "    0.07662280648946762,\n",
       "    -0.0222831629216671,\n",
       "    0.0060752201825380325,\n",
       "    0.0002782242663670331,\n",
       "    -0.04875309020280838,\n",
       "    0.010668679140508175,\n",
       "    0.05046815052628517,\n",
       "    -0.00205554929561913,\n",
       "    -0.006898070219904184,\n",
       "    -0.03003876283764839,\n",
       "    0.002507959259673953,\n",
       "    0.030240535736083984,\n",
       "    -0.0368233360350132,\n",
       "    -0.0032378053292632103,\n",
       "    -0.01363220065832138,\n",
       "    0.011015473864972591,\n",
       "    0.015498589724302292,\n",
       "    0.047138914465904236,\n",
       "    0.011160497553646564,\n",
       "    0.022800203412771225,\n",
       "    -0.007125063333660364,\n",
       "    -0.006362114101648331,\n",
       "    -0.01890348643064499,\n",
       "    0.059018224477767944,\n",
       "    0.005690592806786299,\n",
       "    -0.029811769723892212,\n",
       "    -0.024477429687976837,\n",
       "    -0.000837825471535325,\n",
       "    0.014905884861946106,\n",
       "    -0.014830220490694046,\n",
       "    0.02262365259230137,\n",
       "    0.03884105384349823,\n",
       "    -0.007812348194420338,\n",
       "    -0.03094673529267311,\n",
       "    0.03558748587965965,\n",
       "    0.050669919699430466,\n",
       "    0.017718078568577766,\n",
       "    -0.031072843819856644,\n",
       "    0.00863835122436285,\n",
       "    -0.011110054329037666,\n",
       "    0.0011341776698827744,\n",
       "    0.0036161274183541536,\n",
       "    0.005870295688509941,\n",
       "    -0.0033355385530740023,\n",
       "    0.0026640170253813267,\n",
       "    -0.006245465017855167,\n",
       "    0.005813547410070896,\n",
       "    0.027970602735877037,\n",
       "    0.002872094279155135,\n",
       "    0.022913699969649315,\n",
       "    0.051048241555690765,\n",
       "    0.0027239180635660887,\n",
       "    0.017062319442629814,\n",
       "    0.0015526963397860527,\n",
       "    0.016961434856057167,\n",
       "    0.0006482704193331301,\n",
       "    -0.0033828287851065397,\n",
       "    0.007232254836708307,\n",
       "    -0.02061854675412178,\n",
       "    0.005230301059782505,\n",
       "    0.00631167134270072,\n",
       "    -0.05220843106508255,\n",
       "    0.01636872999370098,\n",
       "    0.011381185613572598,\n",
       "    0.03823573887348175,\n",
       "    -0.010977641679346561,\n",
       "    -0.0016992960590869188,\n",
       "    -0.05523500591516495,\n",
       "    -0.005482515785843134,\n",
       "    0.03755475953221321,\n",
       "    -0.015359871089458466,\n",
       "    -0.0815662145614624,\n",
       "    -0.030845850706100464,\n",
       "    0.005734730511903763,\n",
       "    -0.004051197785884142,\n",
       "    0.03440207615494728,\n",
       "    -0.011746896430850029,\n",
       "    0.015334649942815304,\n",
       "    0.00803303625434637,\n",
       "    -0.03926981985569,\n",
       "    -0.04050567001104355,\n",
       "    -0.004051197785884142,\n",
       "    -0.03742865100502968,\n",
       "    -0.02764272503554821,\n",
       "    -0.00886534433811903,\n",
       "    0.001518016797490418,\n",
       "    -0.006396793760359287,\n",
       "    -0.05432703346014023,\n",
       "    -0.028071489185094833,\n",
       "    -0.025032302364706993,\n",
       "    0.008058257400989532,\n",
       "    0.06219612807035446,\n",
       "    -0.0057977838441729546,\n",
       "    -0.07092275470495224,\n",
       "    0.034679513424634933,\n",
       "    0.0018049109494313598,\n",
       "    -0.044137559831142426,\n",
       "    0.017175815999507904,\n",
       "    0.011280299164354801,\n",
       "    -0.03182948753237724,\n",
       "    0.020858149975538254,\n",
       "    0.0830795019865036,\n",
       "    0.01708754152059555,\n",
       "    -0.04239727929234505,\n",
       "    -0.049988940358161926,\n",
       "    0.005718966946005821,\n",
       "    -0.004722719080746174,\n",
       "    -0.02396039105951786,\n",
       "    0.006299060769379139,\n",
       "    -0.01754152774810791,\n",
       "    0.03193037211894989,\n",
       "    0.021539129316806793,\n",
       "    -0.007213338743895292,\n",
       "    -0.017377588897943497,\n",
       "    0.024199994280934334,\n",
       "    0.010769564658403397,\n",
       "    -0.008127616718411446,\n",
       "    -0.009653515182435513,\n",
       "    0.01121724583208561,\n",
       "    0.040379561483860016,\n",
       "    -0.021463464945554733,\n",
       "    0.048273880034685135,\n",
       "    0.04741635173559189,\n",
       "    0.012610731646418571,\n",
       "    0.005725272465497255,\n",
       "    0.006232853978872299,\n",
       "    0.039093267172575,\n",
       "    0.02915601246058941,\n",
       "    -0.033847205340862274,\n",
       "    -0.029055126011371613,\n",
       "    0.14810043573379517,\n",
       "    -0.013014275580644608,\n",
       "    0.03793308138847351,\n",
       "    -0.009256277233362198,\n",
       "    -0.0011546701425686479,\n",
       "    -0.07016611099243164,\n",
       "    -0.03071974217891693,\n",
       "    0.03213214501738548,\n",
       "    0.022573210299015045,\n",
       "    0.03205648064613342,\n",
       "    -0.0052208430133759975,\n",
       "    0.03732776641845703,\n",
       "    -0.05056903511285782,\n",
       "    -0.03367065265774727,\n",
       "    -0.04610483720898628,\n",
       "    -0.0066458554938435555,\n",
       "    -0.0037296239752322435,\n",
       "    0.007522301748394966,\n",
       "    0.028449811041355133,\n",
       "    0.04249816760420799,\n",
       "    0.017213648185133934,\n",
       "    0.023632511496543884,\n",
       "    -0.009603071957826614,\n",
       "    0.028878575190901756,\n",
       "    0.04870264604687691,\n",
       "    0.019634909927845,\n",
       "    -0.02726440131664276,\n",
       "    0.015712972730398178,\n",
       "    -0.0034143556840717793,\n",
       "    -0.02254798822104931,\n",
       "    0.0037264712154865265,\n",
       "    -0.012339601293206215,\n",
       "    -0.02126169390976429,\n",
       "    -0.022182276472449303,\n",
       "    -0.04615527763962746,\n",
       "    -0.003505783388391137,\n",
       "    -0.01931964047253132,\n",
       "    -0.003430119017139077,\n",
       "    0.04325481131672859,\n",
       "    -0.0006577284657396376,\n",
       "    0.011362269520759583,\n",
       "    -0.010113806463778019,\n",
       "    -0.05054381489753723,\n",
       "    0.0028074642177671194,\n",
       "    -0.027390509843826294,\n",
       "    -0.012856640852987766,\n",
       "    -0.045196861028671265,\n",
       "    0.028424588963389397,\n",
       "    0.010170555673539639,\n",
       "    0.026053771376609802,\n",
       "    0.024931415915489197,\n",
       "    0.0021075685508549213,\n",
       "    0.00486774230375886,\n",
       "    -0.005684287287294865,\n",
       "    -0.025751113891601562,\n",
       "    0.0065891072154045105,\n",
       "    0.0004189127357676625,\n",
       "    -0.00814653281122446,\n",
       "    0.024540483951568604,\n",
       "    -0.018197286874055862,\n",
       "    0.003962922375649214,\n",
       "    0.0032819430343806744,\n",
       "    0.03034142032265663,\n",
       "    -0.02365773357450962,\n",
       "    -0.03427596762776375,\n",
       "    -0.03389764577150345,\n",
       "    -0.014716723933815956,\n",
       "    -0.007515996228903532,\n",
       "    -0.001905796816572547,\n",
       "    -0.003099087392911315,\n",
       "    0.041363202035427094,\n",
       "    -0.024464819580316544,\n",
       "    -0.03621802106499672,\n",
       "    0.005318576004356146,\n",
       "    0.02118602953851223,\n",
       "    -0.009388689883053303,\n",
       "    0.01986190304160118,\n",
       "    0.01644439436495304,\n",
       "    -0.047744229435920715,\n",
       "    0.026053771376609802,\n",
       "    -0.02027805708348751,\n",
       "    0.010321884416043758,\n",
       "    0.022157056257128716,\n",
       "    0.037302546203136444,\n",
       "    0.030063984915614128,\n",
       "    -0.028676804155111313,\n",
       "    -0.006220243405550718,\n",
       "    -0.007837570272386074,\n",
       "    -0.0477694533765316,\n",
       "    -0.021841786801815033,\n",
       "    -0.016091294586658478,\n",
       "    0.024124329909682274,\n",
       "    0.007478164043277502,\n",
       "    0.01784418523311615,\n",
       "    -0.0024874669034034014,\n",
       "    0.04093443602323532,\n",
       "    -0.0054730577394366264,\n",
       "    -0.0032409580890089273,\n",
       "    0.03649545833468437,\n",
       "    -0.03276268020272255,\n",
       "    -0.015712972730398178,\n",
       "    -0.010693900287151337,\n",
       "    -0.01826033927500248,\n",
       "    0.020189780741930008,\n",
       "    0.029029903933405876,\n",
       "    -0.0016441241605207324,\n",
       "    0.01091458834707737,\n",
       "    0.04635705053806305,\n",
       "    0.020681601017713547,\n",
       "    0.008543770760297775,\n",
       "    -0.023909946903586388,\n",
       "    -0.015347260981798172,\n",
       "    0.021501297131180763,\n",
       "    0.002971403766423464,\n",
       "    0.03589014336466789,\n",
       "    -0.0016866853693500161,\n",
       "    0.01788201741874218,\n",
       "    0.00326933222822845,\n",
       "    0.012522457167506218,\n",
       "    -0.026028551161289215,\n",
       "    -0.0016803799662739038,\n",
       "    0.011053306050598621,\n",
       "    0.022119224071502686,\n",
       "    0.00681610032916069,\n",
       "    -0.011166802607476711,\n",
       "    0.02311547100543976,\n",
       "    0.026028551161289215,\n",
       "    0.014678891748189926,\n",
       "    -0.024174772202968597,\n",
       "    0.015675140544772148,\n",
       "    -0.032182589173316956,\n",
       "    0.023594679310917854,\n",
       "    0.004653360228985548,\n",
       "    0.0002933965588454157,\n",
       "    -0.037378210574388504,\n",
       "    0.01155773550271988,\n",
       "    -0.06310410052537918,\n",
       "    0.014893274754285812,\n",
       "    -0.043229587376117706,\n",
       "    0.0030249992851167917,\n",
       "    0.020933814346790314,\n",
       "    -0.01148207113146782,\n",
       "    0.030845850706100464,\n",
       "    -0.01167753804475069,\n",
       "    0.0025000774767249823,\n",
       "    -0.0015794942155480385,\n",
       "    -0.017175815999507904,\n",
       "    -0.03543615713715553,\n",
       "    -0.0036161274183541536,\n",
       "    0.01145054493099451,\n",
       "    0.03831140324473381,\n",
       "    -0.032031260430812836,\n",
       "    0.006563886068761349,\n",
       "    -0.009634599089622498,\n",
       "    -0.032485246658325195,\n",
       "    0.047063250094652176,\n",
       "    -0.0053217289969325066,\n",
       "    -0.0019404763588681817,\n",
       "    -0.07606793195009232,\n",
       "    -0.04138842225074768,\n",
       "    0.0008890565950423479,\n",
       "    0.004189915489405394,\n",
       "    0.018840434029698372,\n",
       "    -0.003981838468462229,\n",
       "    -0.00634950352832675,\n",
       "    -0.013342154212296009,\n",
       "    0.013606979511678219,\n",
       "    0.022989364340901375,\n",
       "    -0.010857840068638325,\n",
       "    0.0038273571990430355,\n",
       "    -0.0027050019707530737,\n",
       "    -0.01099025271832943,\n",
       "    -0.009647210128605366,\n",
       "    0.02483053132891655,\n",
       "    0.007446637377142906,\n",
       "    -0.011822560802102089,\n",
       "    0.0322834737598896,\n",
       "    0.023758618161082268,\n",
       "    -0.051350899040699005,\n",
       "    0.03846273198723793,\n",
       "    -0.016381340101361275,\n",
       "    0.03891671821475029,\n",
       "    -0.013897026889026165,\n",
       "    -0.013026885688304901,\n",
       "    0.021387800574302673,\n",
       "    -0.04370879754424095,\n",
       "    0.031804267317056656,\n",
       "    0.004621833097189665,\n",
       "    -0.018651273101568222,\n",
       "    0.015057213604450226,\n",
       "    -0.026659086346626282,\n",
       "    -0.01914309151470661,\n",
       "    -0.009161696769297123,\n",
       "    -0.010523655451834202,\n",
       "    0.05276330187916756,\n",
       "    0.0015353566268458962,\n",
       "    0.014287958852946758,\n",
       "    0.006176105700433254,\n",
       "    -0.01348087191581726,\n",
       "    -0.011500987224280834,\n",
       "    -0.009237361140549183,\n",
       "    -0.0013911213027313352,\n",
       "    -0.026179879903793335,\n",
       "    0.02343074046075344,\n",
       "    -0.007604271173477173,\n",
       "    0.010258830152451992,\n",
       "    0.012377433478832245,\n",
       "    -0.00826002936810255,\n",
       "    -0.04565085098147392,\n",
       "    0.007257475983351469,\n",
       "    0.0016803799662739038,\n",
       "    -0.00037103137583471835,\n",
       "    -0.028525475412607193,\n",
       "    0.03397331014275551,\n",
       "    -0.00822219718247652,\n",
       "    -0.026583421975374222,\n",
       "    0.020492440089583397,\n",
       "    0.004593458957970142,\n",
       "    -0.01921875588595867,\n",
       "    0.0005659065791405737,\n",
       "    -0.005091582890599966,\n",
       "    0.05326773226261139,\n",
       "    0.02361990138888359,\n",
       "    -0.01810901053249836,\n",
       "    -0.007257475983351469,\n",
       "    0.0033134697005152702,\n",
       "    -0.02524668537080288,\n",
       "    -0.040076903998851776,\n",
       "    -0.005602317396551371,\n",
       "    -0.007093536667525768,\n",
       "    -0.011772118508815765,\n",
       "    -0.0003156623861286789,\n",
       "    0.0010514197638258338,\n",
       "    -0.0006025565089657903,\n",
       "    -0.011854087933897972,\n",
       "    0.014716723933815956,\n",
       "    -0.017339756712317467,\n",
       "    -0.01943313702940941,\n",
       "    -0.015927353873848915,\n",
       "    -0.04913141205906868,\n",
       "    0.007194422651082277,\n",
       "    -0.010674984194338322,\n",
       "    0.001803334685973823,\n",
       "    -0.022724539041519165,\n",
       "    -0.010473213158547878,\n",
       "    -0.011110054329037666,\n",
       "    -0.03863928094506264,\n",
       "    -0.01344303973019123,\n",
       "    0.007831264287233353,\n",
       "    -0.002364512300118804,\n",
       "    -0.02638165093958378,\n",
       "    -0.0022525920066982508,\n",
       "    0.005810394883155823,\n",
       "    -0.002474856097251177,\n",
       "    -0.0066269394010305405,\n",
       "    0.026709530502557755,\n",
       "    0.053015515208244324,\n",
       "    -0.014868052676320076,\n",
       "    -0.009464354254305363,\n",
       "    0.009615682996809483,\n",
       "    -7.797963917255402e-05,\n",
       "    -0.013543926179409027,\n",
       "    -0.03566315025091171,\n",
       "    -0.018651273101568222,\n",
       "    0.012793587520718575,\n",
       "    0.011078528128564358,\n",
       "    -0.011318132281303406,\n",
       "    0.01087675616145134,\n",
       "    -0.006254923064261675,\n",
       "    0.04557518661022186,\n",
       "    -0.010511045344173908,\n",
       "    0.017554137855768204,\n",
       "    -0.0033765234984457493,\n",
       "    -0.004114251118153334,\n",
       "    0.01099025271832943,\n",
       "    0.0020728891249746084,\n",
       "    0.02960999868810177,\n",
       "    -0.013619590550661087,\n",
       "    -0.02254798822104931,\n",
       "    -0.005920738447457552,\n",
       "    -0.018953930586576462,\n",
       "    0.02433871291577816,\n",
       "    0.01644439436495304,\n",
       "    -0.02877769060432911,\n",
       "    0.025486288592219353,\n",
       "    0.018209896981716156,\n",
       "    0.03223302960395813,\n",
       "    0.005529806017875671,\n",
       "    0.018878266215324402,\n",
       "    0.01931964047253132,\n",
       "    0.02309025079011917,\n",
       "    -0.05468013137578964,\n",
       "    -0.040757887065410614,\n",
       "    -0.01739019900560379,\n",
       "    -0.0035593791399151087,\n",
       "    0.0020287514198571444,\n",
       "    -0.032409582287073135,\n",
       "    0.01625523343682289,\n",
       "    -0.03848795220255852,\n",
       "    0.00627383915707469,\n",
       "    0.012717923149466515,\n",
       "    0.0051388731226325035,\n",
       "    -0.016570501029491425,\n",
       "    -0.026608644053339958,\n",
       "    0.006374725140631199,\n",
       "    -0.048954859375953674,\n",
       "    -0.0033985921181738377,\n",
       "    0.03011442720890045,\n",
       "    -0.02688607946038246,\n",
       "    0.03866450488567352,\n",
       "    -0.011671232059597969,\n",
       "    0.03674767166376114,\n",
       "    0.018651273101568222,\n",
       "    0.032031260430812836,\n",
       "    -0.022447101771831512,\n",
       "    0.027390509843826294,\n",
       "    -0.037605203688144684,\n",
       "    -0.016393952071666718,\n",
       "    -0.032258253544569016,\n",
       "    0.026684308424592018,\n",
       "    -0.01935747265815735,\n",
       "    0.008613130077719688,\n",
       "    -0.006201327312737703,\n",
       "    0.03543615713715553,\n",
       "    0.010599319823086262,\n",
       "    -0.04794600233435631,\n",
       "    -0.015864301472902298,\n",
       "    -0.009231056086719036,\n",
       "    0.026785194873809814,\n",
       "    0.010857840068638325,\n",
       "    -0.014187073335051537,\n",
       "    0.016230011358857155,\n",
       "    0.04312870278954506,\n",
       "    -0.026860859245061874,\n",
       "    -0.006024776957929134,\n",
       "    -0.028222817927598953,\n",
       "    -0.012579205445945263,\n",
       "    0.00016206760483328253,\n",
       "    0.027062630280852318,\n",
       "    -0.015183321200311184,\n",
       "    0.005091582890599966,\n",
       "    0.007869096472859383,\n",
       "    0.033771540969610214,\n",
       "    -0.025044914335012436,\n",
       "    0.007503385655581951,\n",
       "    -0.018953930586576462,\n",
       "    -0.017604582011699677,\n",
       "    0.030164871364831924,\n",
       "    0.00251111201941967,\n",
       "    -0.02562500722706318,\n",
       "    -0.0038084411062300205,\n",
       "    0.03064407780766487,\n",
       "    0.004382229410111904,\n",
       "    -0.03649545833468437,\n",
       "    0.0038431205321103334,\n",
       "    0.021211251616477966,\n",
       "    0.029433447867631912,\n",
       "    0.017289312556385994,\n",
       "    -0.03500739112496376,\n",
       "    -0.003978685941547155,\n",
       "    -0.003868341911584139,\n",
       "    -0.013644811697304249,\n",
       "    0.001393485814332962,\n",
       "    -0.01799551397562027,\n",
       "    -0.0046186805702745914,\n",
       "    6.221622606972232e-05,\n",
       "    -0.00232037459500134,\n",
       "    0.030543193221092224,\n",
       "    0.025423236191272736,\n",
       "    0.009533713571727276,\n",
       "    0.013657422736287117,\n",
       "    -0.0009087608777917922,\n",
       "    0.053671274334192276,\n",
       "    -0.027894938364624977,\n",
       "    -0.010832618921995163,\n",
       "    -0.0009237360791303217,\n",
       "    -0.010870451107621193,\n",
       "    -0.007282697595655918,\n",
       "    0.05679873749613762,\n",
       "    0.05130045861005783,\n",
       "    -0.032636575400829315,\n",
       "    0.028147153556346893,\n",
       "    0.04146408662199974,\n",
       "    -2.1120513338246383e-06,\n",
       "    -0.040757887065410614,\n",
       "    0.055335890501737595,\n",
       "    0.023645121604204178,\n",
       "    -0.01935747265815735,\n",
       "    0.015221153385937214,\n",
       "    -0.027743609622120857,\n",
       "    -0.013758308254182339,\n",
       "    0.008234808221459389,\n",
       "    -0.0523093156516552,\n",
       "    -0.019117869436740875,\n",
       "    -0.0022620500531047583,\n",
       "    0.031198950484395027,\n",
       "    0.002493772190064192,\n",
       "    -0.029584776610136032,\n",
       "    -0.02726440131664276,\n",
       "    0.017251480370759964,\n",
       "    0.004990696907043457,\n",
       "    0.01068759523332119,\n",
       "    0.008014120161533356,\n",
       "    0.03536049276590347,\n",
       "    0.007862791419029236,\n",
       "    0.00681610032916069,\n",
       "    -0.029055126011371613,\n",
       "    -0.01773068867623806,\n",
       "    0.030063984915614128,\n",
       "    -0.014086187817156315,\n",
       "    0.025902442634105682,\n",
       "    -0.015813857316970825,\n",
       "    -0.0222831629216671,\n",
       "    -0.014615838415920734,\n",
       "    0.02900468371808529,\n",
       "    0.03291400894522667,\n",
       "    -0.01757935993373394,\n",
       "    -0.00784387532621622,\n",
       "    0.023493792861700058,\n",
       "    0.01319082546979189,\n",
       "    0.014540174044668674,\n",
       "    0.004155236296355724,\n",
       "    0.0008890565950423479,\n",
       "    0.01034710556268692,\n",
       "    -0.02293892204761505,\n",
       "    0.009785927832126617,\n",
       "    -0.020568104460835457,\n",
       "    -0.012907084077596664,\n",
       "    0.017453253269195557,\n",
       "    0.0231280829757452,\n",
       "    -0.0004319175786804408,\n",
       "    -0.03621802106499672,\n",
       "    0.0028011586982756853,\n",
       "    -0.009975088760256767,\n",
       "    -0.012226104736328125,\n",
       "    0.03561270609498024,\n",
       "    0.031350281089544296,\n",
       "    -0.020303279161453247,\n",
       "    -0.005810394883155823,\n",
       "    -0.030139649286866188,\n",
       "    -5.8373894717078656e-05,\n",
       "    0.03957247734069824,\n",
       "    -0.011015473864972591,\n",
       "    0.00551404245197773,\n",
       "    -0.018966540694236755,\n",
       "    -0.005006460472941399,\n",
       "    -0.0058166999369859695,\n",
       "    -0.030366642400622368,\n",
       "    0.010214692912995815,\n",
       "    -0.025082746520638466,\n",
       "    0.017604582011699677,\n",
       "    -0.019811458885669708,\n",
       "    -0.0069674295373260975,\n",
       "    -0.00852485466748476,\n",
       "    -0.021967895328998566,\n",
       "    0.016457004472613335,\n",
       "    -0.0009032436646521091,\n",
       "    -0.023935168981552124,\n",
       "    0.04756768047809601,\n",
       "    -0.022194888442754745,\n",
       "    0.023304631933569908,\n",
       "    -0.01982407085597515,\n",
       "    0.01920614391565323,\n",
       "    -0.013052107766270638,\n",
       "    0.052864186465740204,\n",
       "    0.03102239966392517,\n",
       "    -0.010183165781199932,\n",
       "    -0.04214506596326828,\n",
       "    0.011110054329037666,\n",
       "    0.006377877667546272,\n",
       "    -0.023594679310917854,\n",
       "    0.02870202623307705,\n",
       "    0.03382198140025139,\n",
       "    0.00661432882770896,\n",
       "    -0.014817609451711178,\n",
       "    -0.02688607946038246,\n",
       "    -0.018272951245307922,\n",
       "    0.01901698298752308,\n",
       "    0.005952265579253435,\n",
       "    0.049484509974718094,\n",
       "    0.011545125395059586,\n",
       "    0.002692391164600849,\n",
       "    0.011185718700289726,\n",
       "    0.02407388761639595,\n",
       "    0.0023140693083405495,\n",
       "    0.018323393538594246,\n",
       "    -0.001935747335664928,\n",
       "    -0.012516151182353497,\n",
       "    0.019836680963635445,\n",
       "    0.041287537664175034,\n",
       "    0.040984880179166794,\n",
       "    -0.048349544405937195,\n",
       "    0.002613574266433716,\n",
       "    -0.009710263460874557,\n",
       "    0.006267533637583256,\n",
       "    0.026986965909600258,\n",
       "    -0.034528184682130814,\n",
       "    0.014023133553564548,\n",
       "    0.04277560114860535,\n",
       "    -0.03145116567611694,\n",
       "    -0.04461676999926567,\n",
       "    0.015195932239294052,\n",
       "    -0.014048355631530285,\n",
       "    -0.03364543244242668,\n",
       "    -0.022119224071502686,\n",
       "    -0.06280144304037094,\n",
       "    0.03520916402339935,\n",
       "    -0.016242623329162598,\n",
       "    ...]},\n",
       "  {'embedding': [-0.010829819366335869,\n",
       "    0.029368551447987556,\n",
       "    0.03507687896490097,\n",
       "    0.0011720078764483333,\n",
       "    0.03566371649503708,\n",
       "    0.01744507998228073,\n",
       "    0.00891592912375927,\n",
       "    0.03115573711693287,\n",
       "    -0.00998290628194809,\n",
       "    0.03729085624217987,\n",
       "    0.023313453420996666,\n",
       "    -0.03080897033214569,\n",
       "    -0.03830448538064957,\n",
       "    0.03024880774319172,\n",
       "    -0.0008210723754018545,\n",
       "    0.021006116643548012,\n",
       "    0.01980576664209366,\n",
       "    0.01700495183467865,\n",
       "    -0.0031559187918901443,\n",
       "    0.06215142831206322,\n",
       "    -0.0036443944554775953,\n",
       "    -0.01073645893484354,\n",
       "    0.05628305301070213,\n",
       "    0.006731959991157055,\n",
       "    -0.032249391078948975,\n",
       "    -0.023860279470682144,\n",
       "    -0.018605416640639305,\n",
       "    0.021619627252221107,\n",
       "    -0.015564531087875366,\n",
       "    -0.010896505787968636,\n",
       "    0.053802330046892166,\n",
       "    -0.016751544550061226,\n",
       "    -0.03008875995874405,\n",
       "    0.04350600019097328,\n",
       "    -0.009582789614796638,\n",
       "    0.05396237596869469,\n",
       "    0.006848660763353109,\n",
       "    0.029608620330691338,\n",
       "    0.036677345633506775,\n",
       "    0.0004959777579642832,\n",
       "    0.00998290628194809,\n",
       "    -0.05390902981162071,\n",
       "    0.0002521567512303591,\n",
       "    -0.0007068724953569472,\n",
       "    -0.003597714239731431,\n",
       "    -0.0022056421730667353,\n",
       "    -0.01528444979339838,\n",
       "    0.028835061937570572,\n",
       "    7.101026130840182e-05,\n",
       "    0.019672393798828125,\n",
       "    -0.004928101319819689,\n",
       "    -0.009075975976884365,\n",
       "    0.031049039214849472,\n",
       "    -0.04262574389576912,\n",
       "    -0.03115573711693287,\n",
       "    -0.015351136215031147,\n",
       "    0.00034989352570846677,\n",
       "    0.011576703749597073,\n",
       "    -0.0044879731722176075,\n",
       "    -0.021299535408616066,\n",
       "    0.030915668234229088,\n",
       "    -0.0031275772489607334,\n",
       "    -0.002564079826697707,\n",
       "    -0.031022366136312485,\n",
       "    0.016271404922008514,\n",
       "    -0.02787478268146515,\n",
       "    -0.04974781721830368,\n",
       "    0.02976866625249386,\n",
       "    -0.025994235649704933,\n",
       "    -0.005141497123986483,\n",
       "    0.01034301146864891,\n",
       "    -0.026941176503896713,\n",
       "    0.0006376856472343206,\n",
       "    0.012096854858100414,\n",
       "    -0.029955388978123665,\n",
       "    0.01585794985294342,\n",
       "    0.03363645821809769,\n",
       "    -0.004604673944413662,\n",
       "    0.0022489880211651325,\n",
       "    -0.009402737952768803,\n",
       "    0.02443378046154976,\n",
       "    0.022766629233956337,\n",
       "    -0.02360687218606472,\n",
       "    -0.01140998862683773,\n",
       "    0.008822568692266941,\n",
       "    0.0402517206966877,\n",
       "    0.00275246798992157,\n",
       "    0.012590331956744194,\n",
       "    -0.06577914953231812,\n",
       "    -0.01759178936481476,\n",
       "    0.000656024320051074,\n",
       "    0.030835643410682678,\n",
       "    -0.04059848561882973,\n",
       "    0.033156320452690125,\n",
       "    0.01921892911195755,\n",
       "    0.00936939474195242,\n",
       "    0.0332096703350544,\n",
       "    -0.024500466883182526,\n",
       "    0.04001164808869362,\n",
       "    0.029875364154577255,\n",
       "    -0.021539604291319847,\n",
       "    -0.03809108957648277,\n",
       "    0.015071054920554161,\n",
       "    -0.07239440828561783,\n",
       "    0.0565497986972332,\n",
       "    -0.036197204142808914,\n",
       "    -0.02626097947359085,\n",
       "    -0.05286872759461403,\n",
       "    -0.0249539315700531,\n",
       "    -0.023313453420996666,\n",
       "    -0.09714828431606293,\n",
       "    -0.05270867794752121,\n",
       "    -0.023380139842629433,\n",
       "    -0.0499078631401062,\n",
       "    0.019112231209874153,\n",
       "    -0.029288526624441147,\n",
       "    -0.056923240423202515,\n",
       "    0.029448574408888817,\n",
       "    -0.02483389712870121,\n",
       "    -0.04748049005866051,\n",
       "    -0.06401863694190979,\n",
       "    -0.014777636155486107,\n",
       "    0.024807222187519073,\n",
       "    0.014430868439376354,\n",
       "    0.037424229085445404,\n",
       "    -0.02064601145684719,\n",
       "    -0.00954944733530283,\n",
       "    -0.03131578490138054,\n",
       "    0.014977694489061832,\n",
       "    -0.03769097477197647,\n",
       "    0.03862457722425461,\n",
       "    -0.031929295510053635,\n",
       "    0.054389167577028275,\n",
       "    -0.008782557211816311,\n",
       "    -0.02249988354742527,\n",
       "    0.024380432441830635,\n",
       "    -0.032756201922893524,\n",
       "    0.022846652194857597,\n",
       "    -0.00714874779805541,\n",
       "    -0.056923240423202515,\n",
       "    0.007228771224617958,\n",
       "    0.05396237596869469,\n",
       "    -0.04737379401922226,\n",
       "    0.054309144616127014,\n",
       "    -0.012336925603449345,\n",
       "    -0.01520442683249712,\n",
       "    -0.018245311453938484,\n",
       "    0.054629240185022354,\n",
       "    -0.023860279470682144,\n",
       "    -0.0035243595484644175,\n",
       "    -0.046840302646160126,\n",
       "    0.023420151323080063,\n",
       "    -0.05276202782988548,\n",
       "    0.08797227591276169,\n",
       "    -0.007182091008871794,\n",
       "    0.008802562952041626,\n",
       "    0.0046780286356806755,\n",
       "    0.04107862710952759,\n",
       "    0.007795603014528751,\n",
       "    -0.0009202679502777755,\n",
       "    -0.01289041992276907,\n",
       "    0.05177507549524307,\n",
       "    -0.0032142691779881716,\n",
       "    0.033156320452690125,\n",
       "    -0.026554398238658905,\n",
       "    -0.0016121360240504146,\n",
       "    0.0004167880106251687,\n",
       "    -0.019485672935843468,\n",
       "    -0.04737379401922226,\n",
       "    -0.02420704811811447,\n",
       "    0.01122993603348732,\n",
       "    -0.03486348316073418,\n",
       "    0.03531694784760475,\n",
       "    -0.00490476144477725,\n",
       "    0.059697382152080536,\n",
       "    -0.04033174365758896,\n",
       "    0.029955388978123665,\n",
       "    -0.028995107859373093,\n",
       "    -0.008115695789456367,\n",
       "    -0.01204350683838129,\n",
       "    -0.023993652313947678,\n",
       "    -0.009422743692994118,\n",
       "    -0.04825405031442642,\n",
       "    -0.04630681499838829,\n",
       "    -0.01763180084526539,\n",
       "    -0.018365347757935524,\n",
       "    -0.04457297548651695,\n",
       "    0.032329414039850235,\n",
       "    -0.04526651278138161,\n",
       "    -0.03878462687134743,\n",
       "    -0.007128742057830095,\n",
       "    0.0591638907790184,\n",
       "    0.06844659149646759,\n",
       "    -0.008989283815026283,\n",
       "    -0.004387944005429745,\n",
       "    0.020379265770316124,\n",
       "    0.014590914361178875,\n",
       "    -0.014964357018470764,\n",
       "    -0.01421747263520956,\n",
       "    0.004584668204188347,\n",
       "    -0.0093093765899539,\n",
       "    0.03579708933830261,\n",
       "    0.01047638338059187,\n",
       "    -0.05414909869432449,\n",
       "    0.06556575745344162,\n",
       "    0.03254280984401703,\n",
       "    0.004074519500136375,\n",
       "    0.011876790784299374,\n",
       "    -0.02044595219194889,\n",
       "    -0.026127606630325317,\n",
       "    0.03878462687134743,\n",
       "    0.02752801403403282,\n",
       "    0.010763133876025677,\n",
       "    0.03363645821809769,\n",
       "    -0.057670123875141144,\n",
       "    -0.0002848746080417186,\n",
       "    -0.04972114413976669,\n",
       "    -0.006828655023127794,\n",
       "    -0.019552359357476234,\n",
       "    0.010536400601267815,\n",
       "    0.009782847948372364,\n",
       "    -0.016871578991413116,\n",
       "    0.040651835501194,\n",
       "    0.02664775773882866,\n",
       "    0.004304586444050074,\n",
       "    -0.011323296464979649,\n",
       "    0.021846359595656395,\n",
       "    -0.027474666014313698,\n",
       "    0.007168754003942013,\n",
       "    -0.013297204859554768,\n",
       "    0.013197175227105618,\n",
       "    -0.0027257935144007206,\n",
       "    -0.028808386996388435,\n",
       "    -0.0002880005049519241,\n",
       "    -0.0018555402057245374,\n",
       "    -0.0543358214199543,\n",
       "    -0.04596004635095596,\n",
       "    -0.013670646585524082,\n",
       "    0.0241536982357502,\n",
       "    -0.004071185365319252,\n",
       "    0.004407950211316347,\n",
       "    0.004321258049458265,\n",
       "    -0.015751251950860023,\n",
       "    0.012463628314435482,\n",
       "    -0.014844321645796299,\n",
       "    0.02672778256237507,\n",
       "    -0.01648479886353016,\n",
       "    -0.018965521827340126,\n",
       "    0.03523692488670349,\n",
       "    0.011116569861769676,\n",
       "    -0.018285322934389114,\n",
       "    -0.021926384419202805,\n",
       "    0.07575538754463196,\n",
       "    -0.03475678712129593,\n",
       "    -0.026821142062544823,\n",
       "    -0.005414909683167934,\n",
       "    -0.01763180084526539,\n",
       "    0.02281997725367546,\n",
       "    -0.009396068751811981,\n",
       "    -0.026794467121362686,\n",
       "    0.030755620449781418,\n",
       "    0.0038777957670390606,\n",
       "    0.004324592649936676,\n",
       "    0.034303322434425354,\n",
       "    -0.02314007095992565,\n",
       "    0.0004959777579642832,\n",
       "    -0.029528597369790077,\n",
       "    -0.0006010083016008139,\n",
       "    -0.022673267871141434,\n",
       "    -0.04542655870318413,\n",
       "    0.023580199107527733,\n",
       "    0.012090186588466167,\n",
       "    -0.07186092436313629,\n",
       "    -0.001498769735917449,\n",
       "    -7.76788656366989e-05,\n",
       "    -0.04203890636563301,\n",
       "    0.012283576652407646,\n",
       "    -0.008195719681680202,\n",
       "    0.0380377396941185,\n",
       "    0.01857874169945717,\n",
       "    -0.039451487362384796,\n",
       "    -0.025554107502102852,\n",
       "    0.025527432560920715,\n",
       "    0.013804019428789616,\n",
       "    0.005768346134573221,\n",
       "    0.038331158459186554,\n",
       "    -0.020579325035214424,\n",
       "    0.0030158781446516514,\n",
       "    0.018672103062272072,\n",
       "    -0.03419662266969681,\n",
       "    0.013203844428062439,\n",
       "    -0.040651835501194,\n",
       "    0.03507687896490097,\n",
       "    -0.0384378582239151,\n",
       "    -0.022633256390690804,\n",
       "    0.041265346109867096,\n",
       "    0.011503349058330059,\n",
       "    -0.0021656304597854614,\n",
       "    -0.05009458586573601,\n",
       "    0.013964065350592136,\n",
       "    0.0028858401346951723,\n",
       "    -0.005201514344662428,\n",
       "    -0.048147350549697876,\n",
       "    -0.029235178604722023,\n",
       "    -0.01609802059829235,\n",
       "    0.0256608035415411,\n",
       "    0.036677345633506775,\n",
       "    -0.028835061937570572,\n",
       "    -0.03142248094081879,\n",
       "    0.006095108110457659,\n",
       "    -0.0015787930460646749,\n",
       "    -0.015257775783538818,\n",
       "    0.036090508103370667,\n",
       "    0.010643098503351212,\n",
       "    -0.06737961620092392,\n",
       "    -0.0483073964715004,\n",
       "    0.017738498747348785,\n",
       "    0.018605416640639305,\n",
       "    -0.007442166563123465,\n",
       "    -0.07943645864725113,\n",
       "    0.0035843770019710064,\n",
       "    -0.01057641301304102,\n",
       "    0.025287361815571785,\n",
       "    -0.04345265030860901,\n",
       "    -0.018525393679738045,\n",
       "    -0.0004943105741403997,\n",
       "    0.020846068859100342,\n",
       "    -0.03654397279024124,\n",
       "    0.027848107740283012,\n",
       "    -0.026114270091056824,\n",
       "    0.03571706637740135,\n",
       "    0.018138613551855087,\n",
       "    -0.03206266835331917,\n",
       "    -0.005188177339732647,\n",
       "    -0.0005622470052912831,\n",
       "    0.00519151147454977,\n",
       "    -0.03222271427512169,\n",
       "    0.02000582404434681,\n",
       "    0.02630099095404148,\n",
       "    -0.011223267763853073,\n",
       "    -0.07916971296072006,\n",
       "    -0.025207338854670525,\n",
       "    6.861372821731493e-05,\n",
       "    -0.014337508007884026,\n",
       "    0.030115434899926186,\n",
       "    -0.006008415948599577,\n",
       "    -0.046360164880752563,\n",
       "    0.019525684416294098,\n",
       "    0.02324676886200905,\n",
       "    0.03016878291964531,\n",
       "    0.019152242690324783,\n",
       "    -0.01617804355919361,\n",
       "    0.02630099095404148,\n",
       "    0.0388379730284214,\n",
       "    0.004211226012557745,\n",
       "    0.024180373176932335,\n",
       "    0.03795771673321724,\n",
       "    -0.04121199995279312,\n",
       "    0.010749796405434608,\n",
       "    -0.027181247249245644,\n",
       "    0.0046013398095965385,\n",
       "    -0.01905888132750988,\n",
       "    0.04403948783874512,\n",
       "    0.017565114423632622,\n",
       "    -0.0372375063598156,\n",
       "    0.039051368832588196,\n",
       "    -0.02535404823720455,\n",
       "    -0.019872453063726425,\n",
       "    0.022686604410409927,\n",
       "    -0.0023773587308824062,\n",
       "    -0.027061212807893753,\n",
       "    -0.022833313792943954,\n",
       "    -0.004694700241088867,\n",
       "    0.036837391555309296,\n",
       "    -0.029208503663539886,\n",
       "    -0.0336097851395607,\n",
       "    0.008942604064941406,\n",
       "    0.013950728811323643,\n",
       "    -0.02207309380173683,\n",
       "    -0.0048680840991437435,\n",
       "    -0.022046418860554695,\n",
       "    0.023233430460095406,\n",
       "    0.015497845597565174,\n",
       "    -0.052922073751688004,\n",
       "    0.03811776265501976,\n",
       "    0.05777681991457939,\n",
       "    -0.034143272787332535,\n",
       "    -0.009155998937785625,\n",
       "    0.03291625156998634,\n",
       "    0.00901595875620842,\n",
       "    0.00479472940787673,\n",
       "    0.006111779250204563,\n",
       "    0.0008144037565216422,\n",
       "    0.022006407380104065,\n",
       "    0.03392987698316574,\n",
       "    0.023700233548879623,\n",
       "    0.02779475972056389,\n",
       "    0.04491974413394928,\n",
       "    0.040358416736125946,\n",
       "    -0.026461036875844002,\n",
       "    -0.03363645821809769,\n",
       "    -0.013790681958198547,\n",
       "    0.008542487397789955,\n",
       "    -0.023113396018743515,\n",
       "    0.004608008079230785,\n",
       "    0.010823151096701622,\n",
       "    -0.0410519503057003,\n",
       "    0.026701107621192932,\n",
       "    0.05236191302537918,\n",
       "    0.010589749552309513,\n",
       "    0.017138322815299034,\n",
       "    -0.01625806652009487,\n",
       "    0.014724287204444408,\n",
       "    -0.013844030909240246,\n",
       "    -0.007822277024388313,\n",
       "    -0.026594409719109535,\n",
       "    0.031849272549152374,\n",
       "    0.011876790784299374,\n",
       "    0.04630681499838829,\n",
       "    0.02590087428689003,\n",
       "    -0.006438541226089001,\n",
       "    0.0015296119963750243,\n",
       "    0.045373208820819855,\n",
       "    -0.07047384977340698,\n",
       "    0.06433872878551483,\n",
       "    -0.011036546900868416,\n",
       "    -0.005224854685366154,\n",
       "    -0.030115434899926186,\n",
       "    -0.021339546889066696,\n",
       "    -0.03657064586877823,\n",
       "    -0.04812067747116089,\n",
       "    0.06231147423386574,\n",
       "    0.0179118812084198,\n",
       "    -0.013090478256344795,\n",
       "    -0.05622970312833786,\n",
       "    -0.009069306775927544,\n",
       "    -0.026714444160461426,\n",
       "    -0.00792897492647171,\n",
       "    0.02664775773882866,\n",
       "    -0.06284496188163757,\n",
       "    0.01752510294318199,\n",
       "    0.02091275528073311,\n",
       "    -0.017685148864984512,\n",
       "    -0.00014452124014496803,\n",
       "    -0.031342457979917526,\n",
       "    -0.0032309407833963633,\n",
       "    -0.086531862616539,\n",
       "    0.03072894737124443,\n",
       "    -0.006455212831497192,\n",
       "    -0.0495077483355999,\n",
       "    -0.028621666133403778,\n",
       "    -0.018485382199287415,\n",
       "    0.054389167577028275,\n",
       "    0.004257906228303909,\n",
       "    -0.027848107740283012,\n",
       "    -0.03894467279314995,\n",
       "    0.03384985402226448,\n",
       "    -0.07191427052021027,\n",
       "    0.005074810702353716,\n",
       "    -0.035130228847265244,\n",
       "    0.0011636720737442374,\n",
       "    -0.027501340955495834,\n",
       "    0.005835032090544701,\n",
       "    -0.011463337577879429,\n",
       "    0.03366313502192497,\n",
       "    -0.03571706637740135,\n",
       "    0.007822277024388313,\n",
       "    0.010676441714167595,\n",
       "    0.013590623624622822,\n",
       "    0.012917093932628632,\n",
       "    0.04089190438389778,\n",
       "    0.022513221949338913,\n",
       "    0.004711371846497059,\n",
       "    -0.025527432560920715,\n",
       "    -0.021099476143717766,\n",
       "    -0.006895340979099274,\n",
       "    -0.0015596207231283188,\n",
       "    -0.044946420937776566,\n",
       "    -0.010809813626110554,\n",
       "    0.0011861786479130387,\n",
       "    0.011750088073313236,\n",
       "    0.06140454486012459,\n",
       "    0.004304586444050074,\n",
       "    0.012977111153304577,\n",
       "    -0.036197204142808914,\n",
       "    0.01132996566593647,\n",
       "    -0.035610366612672806,\n",
       "    -0.012130198068916798,\n",
       "    0.00913599319756031,\n",
       "    -0.00447797030210495,\n",
       "    -0.06962026655673981,\n",
       "    -0.02265993133187294,\n",
       "    0.03336971625685692,\n",
       "    -0.014951019547879696,\n",
       "    -0.015151077881455421,\n",
       "    -0.0005234857671894133,\n",
       "    -0.05305544659495354,\n",
       "    -0.07602213323116302,\n",
       "    -0.020712697878479958,\n",
       "    -0.002327344147488475,\n",
       "    0.016858242452144623,\n",
       "    0.015964647755026817,\n",
       "    -0.04606674611568451,\n",
       "    0.018832148984074593,\n",
       "    0.010783139616250992,\n",
       "    0.047907281666994095,\n",
       "    -0.038971345871686935,\n",
       "    0.010169627144932747,\n",
       "    0.018792137503623962,\n",
       "    -0.010983197949826717,\n",
       "    -0.00927603431046009,\n",
       "    0.01597798615694046,\n",
       "    0.04270576685667038,\n",
       "    0.025794176384806633,\n",
       "    0.02384694293141365,\n",
       "    0.033076297491788864,\n",
       "    0.008989283815026283,\n",
       "    -0.006655271165072918,\n",
       "    -0.0258608628064394,\n",
       "    0.019152242690324783,\n",
       "    0.0058783781714737415,\n",
       "    -0.012743710540235043,\n",
       "    -0.02688782848417759,\n",
       "    -0.003957819193601608,\n",
       "    0.01605800911784172,\n",
       "    -0.006471884436905384,\n",
       "    -0.029235178604722023,\n",
       "    -0.05057472363114357,\n",
       "    -0.030115434899926186,\n",
       "    0.01417746115475893,\n",
       "    0.011169918812811375,\n",
       "    -0.01827198639512062,\n",
       "    -0.006008415948599577,\n",
       "    0.009516104124486446,\n",
       "    -0.02040594071149826,\n",
       "    -0.025834187865257263,\n",
       "    0.01087650004774332,\n",
       "    -0.05369563400745392,\n",
       "    -0.02614094503223896,\n",
       "    -0.02163296565413475,\n",
       "    0.003677737433463335,\n",
       "    0.043986137956380844,\n",
       "    0.014644263312220573,\n",
       "    0.00864251609891653,\n",
       "    -0.027501340955495834,\n",
       "    -0.004898092709481716,\n",
       "    -0.005444918759167194,\n",
       "    -0.009336051531136036,\n",
       "    -0.0005555784446187317,\n",
       "    0.005951732862740755,\n",
       "    0.05713663622736931,\n",
       "    -0.016271404922008514,\n",
       "    -0.012103524059057236,\n",
       "    0.0045379879884421825,\n",
       "    0.004501310642808676,\n",
       "    -0.028835061937570572,\n",
       "    -0.033476412296295166,\n",
       "    -0.013370559550821781,\n",
       "    -0.03958485648036003,\n",
       "    -0.003347641322761774,\n",
       "    0.008709202520549297,\n",
       "    -0.023620210587978363,\n",
       "    0.010416366159915924,\n",
       "    -0.011103232391178608,\n",
       "    0.00642520422115922,\n",
       "    0.016671519726514816,\n",
       "    -0.005808357615023851,\n",
       "    -0.024487128481268883,\n",
       "    -0.03870460018515587,\n",
       "    0.034783460199832916,\n",
       "    -0.017151661217212677,\n",
       "    0.011443331837654114,\n",
       "    -0.01100987195968628,\n",
       "    -0.011056552641093731,\n",
       "    0.012663686648011208,\n",
       "    -0.011890128254890442,\n",
       "    0.031102389097213745,\n",
       "    0.016084682196378708,\n",
       "    0.018338672816753387,\n",
       "    -0.008042341098189354,\n",
       "    -0.010423034429550171,\n",
       "    -0.029688643291592598,\n",
       "    -0.03363645821809769,\n",
       "    -0.007962318137288094,\n",
       "    -0.04894758388400078,\n",
       "    0.02420704811811447,\n",
       "    0.037664297968149185,\n",
       "    0.01463092677295208,\n",
       "    -0.02080605737864971,\n",
       "    0.014564240351319313,\n",
       "    0.03096901625394821,\n",
       "    -0.01198348868638277,\n",
       "    -0.002790812635794282,\n",
       "    0.020579325035214424,\n",
       "    0.005068142432719469,\n",
       "    0.0009669481660239398,\n",
       "    0.01996581256389618,\n",
       "    -0.029795341193675995,\n",
       "    0.021406231448054314,\n",
       "    0.017098311334848404,\n",
       "    -0.028648341074585915,\n",
       "    0.010843156836926937,\n",
       "    0.015377810224890709,\n",
       "    -0.010176296345889568,\n",
       "    0.009996243752539158,\n",
       "    0.04617344215512276,\n",
       "    -0.005044802092015743,\n",
       "    -0.015537857078015804,\n",
       "    -0.010623092763125896,\n",
       "    -0.003117574378848076,\n",
       "    -0.015271112322807312,\n",
       "    -0.025380723178386688,\n",
       "    -0.021059464663267136,\n",
       "    -0.05532277375459671,\n",
       "    -0.03230273723602295,\n",
       "    0.02304670959711075,\n",
       "    -0.04099860414862633,\n",
       "    -0.013223850168287754,\n",
       "    0.009776179678738117,\n",
       "    0.030462201684713364,\n",
       "    -0.0354236476123333,\n",
       "    -0.004598005209118128,\n",
       "    -0.03614385798573494,\n",
       "    0.007428829558193684,\n",
       "    0.06412533670663834,\n",
       "    -0.007462172769010067,\n",
       "    -0.013397233560681343,\n",
       "    -0.000261534471064806,\n",
       "    -0.0098095228895545,\n",
       "    0.005691657308489084,\n",
       "    -0.018645428121089935,\n",
       "    -0.03286290168762207,\n",
       "    0.04089190438389778,\n",
       "    0.02163296565413475,\n",
       "    0.03278287872672081,\n",
       "    -0.005775014869868755,\n",
       "    0.013990740291774273,\n",
       "    -0.004564662463963032,\n",
       "    -0.010829819366335869,\n",
       "    -0.010663104243576527,\n",
       "    0.024407105520367622,\n",
       "    0.032409437000751495,\n",
       "    0.011243273504078388,\n",
       "    -0.008315754123032093,\n",
       "    0.030382178723812103,\n",
       "    0.0008248234516941011,\n",
       "    -0.01953902281820774,\n",
       "    -0.019512347877025604,\n",
       "    -0.012670355848968029,\n",
       "    -0.005194845609366894,\n",
       "    -0.04515981301665306,\n",
       "    -0.029715318232774734,\n",
       "    0.011923471465706825,\n",
       "    0.04625346511602402,\n",
       "    0.02716791070997715,\n",
       "    0.03579708933830261,\n",
       "    0.0010994868353009224,\n",
       "    0.013610629364848137,\n",
       "    -0.004731377586722374,\n",
       "    0.032676178961992264,\n",
       "    -0.03555702045559883,\n",
       "    -0.04081188142299652,\n",
       "    0.008075684309005737,\n",
       "    -0.01229691319167614,\n",
       "    -0.0117234131321311,\n",
       "    0.018832148984074593,\n",
       "    0.017845194786787033,\n",
       "    -0.0009644474484957755,\n",
       "    9.330841567134485e-05,\n",
       "    0.052895400673151016,\n",
       "    -0.09138660877943039,\n",
       "    0.020939430221915245,\n",
       "    0.02574082836508751,\n",
       "    0.023620210587978363,\n",
       "    0.01271036732941866,\n",
       "    -0.01863209158182144,\n",
       "    -0.006598587613552809,\n",
       "    -0.02225981466472149,\n",
       "    -0.0016563155222684145,\n",
       "    0.005721665918827057,\n",
       "    0.0031042371410876513,\n",
       "    0.02316674403846264,\n",
       "    -0.046013396233320236,\n",
       "    -0.06951357424259186,\n",
       "    -0.0509214922785759,\n",
       "    -0.019512347877025604,\n",
       "    0.023673558607697487,\n",
       "    -0.005898383911699057,\n",
       "    -0.0005935061490163207,\n",
       "    0.02316674403846264,\n",
       "    -0.013884042389690876,\n",
       "    -0.018658766523003578,\n",
       "    0.021939720958471298,\n",
       "    -0.05006790906190872,\n",
       "    0.008542487397789955,\n",
       "    -0.013197175227105618,\n",
       "    0.004638017155230045,\n",
       "    0.022673267871141434,\n",
       "    0.010016249492764473,\n",
       "    -0.03830448538064957,\n",
       "    0.027554688975214958,\n",
       "    -0.022966686636209488,\n",
       "    -0.01425748411566019,\n",
       "    0.0014604252064600587,\n",
       "    0.015604543499648571,\n",
       "    -0.008842574432492256,\n",
       "    -0.005074810702353716,\n",
       "    -0.031182412058115005,\n",
       "    0.022486547008156776,\n",
       "    0.006061764899641275,\n",
       "    -0.0032992938067764044,\n",
       "    0.0007518855854868889,\n",
       "    0.023020034655928612,\n",
       "    0.012550320476293564,\n",
       "    -0.028968434780836105,\n",
       "    -0.011083226650953293,\n",
       "    -0.031182412058115005,\n",
       "    -0.00948942918330431,\n",
       "    -0.010769802145659924,\n",
       "    -0.0033142983447760344,\n",
       "    -0.019472336396574974,\n",
       "    0.01512440387159586,\n",
       "    -0.028968434780836105,\n",
       "    -0.030142109841108322,\n",
       "    0.00617846567183733,\n",
       "    0.012370267882943153,\n",
       "    0.031689226627349854,\n",
       "    0.032596156001091,\n",
       "    -0.014470879919826984,\n",
       "    -0.016871578991413116,\n",
       "    -0.03136913478374481,\n",
       "    0.013023791834712029,\n",
       "    0.03296959772706032,\n",
       "    0.002990870736539364,\n",
       "    0.012270239181816578,\n",
       "    -0.01869877800345421,\n",
       "    -0.018885498866438866,\n",
       "    -0.019285615533590317,\n",
       "    -0.020325917750597,\n",
       "    -0.013103814795613289,\n",
       "    -0.003375982865691185,\n",
       "    -0.012263570912182331,\n",
       "    -0.028648341074585915,\n",
       "    0.05638975277543068,\n",
       "    0.05236191302537918,\n",
       "    0.016938265413045883,\n",
       "    -1.276413331652293e-05,\n",
       "    0.014244147576391697,\n",
       "    0.011776762083172798,\n",
       "    0.01724502071738243,\n",
       "    -0.035930462181568146,\n",
       "    0.011243273504078388,\n",
       "    -0.0011620050063356757,\n",
       "    -0.013490593992173672,\n",
       "    -0.006631930824369192,\n",
       "    -0.035130228847265244,\n",
       "    0.010049592703580856,\n",
       "    0.012316918931901455,\n",
       "    -0.014911008067429066,\n",
       "    0.004774723667651415,\n",
       "    0.04566662758588791,\n",
       "    0.07036715745925903,\n",
       "    -0.014657600782811642,\n",
       "    -0.025874199345707893,\n",
       "    0.00335597712546587,\n",
       "    0.020846068859100342,\n",
       "    -0.01385736744850874,\n",
       "    -0.02483389712870121,\n",
       "    0.013083809055387974,\n",
       "    -0.023260105401277542,\n",
       "    -0.031689226627349854,\n",
       "    0.0019188920268788934,\n",
       "    -0.0008156541152857244,\n",
       "    0.014604251831769943,\n",
       "    -0.01204350683838129,\n",
       "    0.02467385120689869,\n",
       "    0.026781130582094193,\n",
       "    -0.018565405160188675,\n",
       "    0.017418405041098595,\n",
       "    -0.004524650517851114,\n",
       "    0.008902591653168201,\n",
       "    -0.023553524166345596,\n",
       "    -0.012096854858100414,\n",
       "    -0.05975072830915451,\n",
       "    -0.023580199107527733,\n",
       "    -0.02072603441774845,\n",
       "    -0.04369271919131279,\n",
       "    -0.0436660461127758,\n",
       "    -0.014617589302361012,\n",
       "    -0.016191380098462105,\n",
       "    -0.0033976559061557055,\n",
       "    0.0236468855291605,\n",
       "    0.0009402737487107515,\n",
       "    0.005558284930884838,\n",
       "    -0.014911008067429066,\n",
       "    0.041345369070768356,\n",
       "    0.004247903358191252,\n",
       "    -0.08253069221973419,\n",
       "    0.005568287800997496,\n",
       "    0.03182259947061539,\n",
       "    -0.02249988354742527,\n",
       "    0.03526360169053078,\n",
       "    -0.008509144186973572,\n",
       "    0.02376691997051239,\n",
       "    -0.013550612144172192,\n",
       "    0.03910471871495247,\n",
       "    -0.04163878783583641,\n",
       "    0.009429411962628365,\n",
       "    -0.017578450962901115,\n",
       "    -0.005108153913170099,\n",
       "    -9.330841567134485e-05,\n",
       "    0.002847495721653104,\n",
       "    -0.0521218404173851,\n",
       "    -0.021886372938752174,\n",
       "    -0.01621805503964424,\n",
       "    -0.006345180794596672,\n",
       "    -0.005698325578123331,\n",
       "    -0.049881190061569214,\n",
       "    -0.012323588132858276,\n",
       "    -0.045373208820819855,\n",
       "    0.01095652300864458,\n",
       "    0.04377274587750435,\n",
       "    -0.0077089108526706696,\n",
       "    0.021819686517119408,\n",
       "    0.015631217509508133,\n",
       "    0.005014793481677771,\n",
       "    0.013950728811323643,\n",
       "    -0.013610629364848137,\n",
       "    0.018405359238386154,\n",
       "    -0.0004738879797514528,\n",
       "    0.022953350096940994,\n",
       "    0.01827198639512062,\n",
       "    -0.021806348115205765,\n",
       "    0.009155998937785625,\n",
       "    -0.007828946225345135,\n",
       "    -0.031689226627349854,\n",
       "    -0.03795771673321724,\n",
       "    0.0029775334987789392,\n",
       "    0.015297787263989449,\n",
       "    0.01988578960299492,\n",
       "    -0.012737041339278221,\n",
       "    -0.01516441535204649,\n",
       "    -0.03150250390172005,\n",
       "    -0.009155998937785625,\n",
       "    0.013457251712679863,\n",
       "    -0.02008584700524807,\n",
       "    -0.02368689700961113,\n",
       "    -0.03243611007928848,\n",
       "    -0.024700524285435677,\n",
       "    0.026194293051958084,\n",
       "    0.0033326370175927877,\n",
       "    -0.0005472426419146359,\n",
       "    -0.015604543499648571,\n",
       "    0.002314007142558694,\n",
       "    0.0013779011787846684,\n",
       "    0.004577999468892813,\n",
       "    0.007502184249460697,\n",
       "    0.02155294083058834,\n",
       "    -0.0040811882354319096,\n",
       "    -0.009922889061272144,\n",
       "    -0.011790099553763866,\n",
       "    -0.009922889061272144,\n",
       "    0.006111779250204563,\n",
       "    -0.003272619564086199,\n",
       "    -0.011510017327964306,\n",
       "    -0.035850439220666885,\n",
       "    0.0295819453895092,\n",
       "    -0.03136913478374481,\n",
       "    -0.004204557277262211,\n",
       "    -0.0062718261033296585,\n",
       "    0.04318590834736824,\n",
       "    0.05612300708889961,\n",
       "    0.027554688975214958,\n",
       "    0.03563704341650009,\n",
       "    0.062258124351501465,\n",
       "    0.03326301649212837,\n",
       "    -0.018712114542722702,\n",
       "    0.03563704341650009,\n",
       "    0.00653523625805974,\n",
       "    -0.0068053146824240685,\n",
       "    0.019205590710043907,\n",
       "    0.025007281452417374,\n",
       "    -0.0036477285902947187,\n",
       "    0.014044089242815971,\n",
       "    -0.02261991985142231,\n",
       "    0.018258649855852127,\n",
       "    0.01696493849158287,\n",
       "    0.03278287872672081,\n",
       "    0.027421316131949425,\n",
       "    0.007962318137288094,\n",
       "    0.003781100967898965,\n",
       "    -0.002035592682659626,\n",
       "    -0.010689779184758663,\n",
       "    -0.0015637886244803667,\n",
       "    0.007008707150816917,\n",
       "    0.022873325273394585,\n",
       "    -0.0052615320309996605,\n",
       "    -0.0027491336222738028,\n",
       "    -0.008489138446748257,\n",
       "    0.024687187746167183,\n",
       "    -0.028995107859373093,\n",
       "    0.0001479597412981093,\n",
       "    -0.00014400025247596204,\n",
       "    0.0003949066449422389,\n",
       "    0.029635295271873474,\n",
       "    0.010849825106561184,\n",
       "    0.029635295271873474,\n",
       "    0.03158252686262131,\n",
       "    0.0060817706398665905,\n",
       "    0.00221897941082716,\n",
       "    0.026207629591226578,\n",
       "    0.01425748411566019,\n",
       "    0.03358311206102371,\n",
       "    0.03854455426335335,\n",
       "    0.0020939430687576532,\n",
       "    0.04868083819746971,\n",
       "    0.002153960522264242,\n",
       "    0.03990495204925537,\n",
       "    0.002804149640724063,\n",
       "    -0.02634100243449211,\n",
       "    -0.0077089108526706696,\n",
       "    -0.029315201565623283,\n",
       "    -0.013090478256344795,\n",
       "    0.005418244283646345,\n",
       "    -0.013017123565077782,\n",
       "    0.05895049497485161,\n",
       "    -0.012923763133585453,\n",
       "    0.012103524059057236,\n",
       "    0.012690361589193344,\n",
       "    0.002425706246867776,\n",
       "    0.0008552490035071969,\n",
       "    -0.03891799598932266,\n",
       "    -0.024727199226617813,\n",
       "    0.027194583788514137,\n",
       "    0.04758718982338905,\n",
       "    -0.022726615890860558,\n",
       "    0.051508329808712006,\n",
       "    0.019152242690324783,\n",
       "    -0.007382149342447519,\n",
       "    0.006375189404934645,\n",
       "    -0.022793302312493324,\n",
       "    0.02720792219042778,\n",
       "    0.013790681958198547,\n",
       "    -0.004304586444050074,\n",
       "    0.005354892462491989,\n",
       "    0.023033373057842255,\n",
       "    0.0011578371049836278,\n",
       "    0.0370241142809391,\n",
       "    -0.009822859428822994,\n",
       "    -0.009976238012313843,\n",
       "    0.02992871403694153,\n",
       "    -0.011183256283402443,\n",
       "    -0.010649767704308033,\n",
       "    -0.009422743692994118,\n",
       "    -0.023953640833497047,\n",
       "    0.06476552039384842,\n",
       "    -0.0010728123597800732,\n",
       "    0.03755760192871094,\n",
       "    0.005911721382290125,\n",
       "    -0.019445661455392838,\n",
       "    -0.025327373296022415,\n",
       "    -0.014457542449235916,\n",
       "    -0.01728503219783306,\n",
       "    0.026314327493309975,\n",
       "    -0.0428658127784729,\n",
       "    0.007715579587966204,\n",
       "    -0.004057847894728184,\n",
       "    0.009049301035702229,\n",
       "    -0.01929895207285881,\n",
       "    -0.0022489880211651325,\n",
       "    0.005718331318348646,\n",
       "    0.04270576685667038,\n",
       "    0.005474927369505167,\n",
       "    0.014270821586251259,\n",
       "    0.03272952884435654,\n",
       "    0.0282748993486166,\n",
       "    0.019445661455392838,\n",
       "    -0.002994205104187131,\n",
       "    0.021046128123998642,\n",
       "    -0.033903203904628754,\n",
       "    0.024420443922281265,\n",
       "    0.037664297968149185,\n",
       "    0.028008153662085533,\n",
       "    -0.0125769954174757,\n",
       "    0.0017838526982814074,\n",
       "    0.013870704919099808,\n",
       "    0.041185323148965836,\n",
       "    -0.011156581342220306,\n",
       "    0.02934187650680542,\n",
       "    0.04558660462498665,\n",
       "    0.054789286106824875,\n",
       "    -0.005714997183531523,\n",
       "    -0.024687187746167183,\n",
       "    -0.003022546647116542,\n",
       "    0.0022889997344464064,\n",
       "    -0.01645812578499317,\n",
       "    -0.02819487638771534,\n",
       "    -0.060977753251791,\n",
       "    0.012096854858100414,\n",
       "    -0.005518273450434208,\n",
       "    ...]},\n",
       "  {'embedding': [0.02482173591852188,\n",
       "    0.037629298865795135,\n",
       "    0.046639930456876755,\n",
       "    -0.00745927169919014,\n",
       "    0.020684780552983284,\n",
       "    0.008536013774573803,\n",
       "    -0.01858796738088131,\n",
       "    -0.0010351245291531086,\n",
       "    0.014932711608707905,\n",
       "    0.03131052479147911,\n",
       "    0.010519485920667648,\n",
       "    0.0366375632584095,\n",
       "    -0.029185375198721886,\n",
       "    0.035107456147670746,\n",
       "    -0.017879584804177284,\n",
       "    -0.003793390467762947,\n",
       "    0.02731524407863617,\n",
       "    0.00130519550293684,\n",
       "    -0.02057143859565258,\n",
       "    0.044089749455451965,\n",
       "    0.0014955734368413687,\n",
       "    0.015017717145383358,\n",
       "    0.04261631518602371,\n",
       "    -0.002899056999012828,\n",
       "    -0.03779930993914604,\n",
       "    -0.019650541245937347,\n",
       "    0.0012131057446822524,\n",
       "    -0.01581110619008541,\n",
       "    0.03859269991517067,\n",
       "    -0.03700592368841171,\n",
       "    0.07548528164625168,\n",
       "    -0.028887854889035225,\n",
       "    -0.04366471990942955,\n",
       "    0.026649365201592445,\n",
       "    -0.03417238965630531,\n",
       "    0.057889051735401154,\n",
       "    -0.00528453616425395,\n",
       "    0.06437783688306808,\n",
       "    0.011079108342528343,\n",
       "    0.00858560111373663,\n",
       "    -0.015428579412400723,\n",
       "    -0.02955373376607895,\n",
       "    -0.018233776092529297,\n",
       "    -0.006237311288714409,\n",
       "    0.01969304494559765,\n",
       "    0.03853603079915047,\n",
       "    0.03357734903693199,\n",
       "    -0.008507678285241127,\n",
       "    -0.015286902897059917,\n",
       "    0.04284299910068512,\n",
       "    0.03811100125312805,\n",
       "    0.0007544278050772846,\n",
       "    0.05499884858727455,\n",
       "    -0.026082657277584076,\n",
       "    -0.02337663620710373,\n",
       "    0.0016620433889329433,\n",
       "    -0.017624566331505775,\n",
       "    0.017865415662527084,\n",
       "    0.03629754111170769,\n",
       "    -0.010923264548182487,\n",
       "    0.02139316312968731,\n",
       "    -0.018120434135198593,\n",
       "    0.016278639435768127,\n",
       "    -0.030885493382811546,\n",
       "    0.00943566020578146,\n",
       "    -0.036779239773750305,\n",
       "    -0.03728927671909332,\n",
       "    0.04442977532744408,\n",
       "    -0.009109804406762123,\n",
       "    0.04955846816301346,\n",
       "    -0.025884311646223068,\n",
       "    -0.0422196201980114,\n",
       "    0.06670133024454117,\n",
       "    -0.004693036433309317,\n",
       "    -0.019820554181933403,\n",
       "    0.031452201306819916,\n",
       "    0.013742628507316113,\n",
       "    -0.008238493464887142,\n",
       "    -0.004622198175638914,\n",
       "    0.018970493227243423,\n",
       "    -0.001939198118634522,\n",
       "    0.03136719390749931,\n",
       "    0.02222905494272709,\n",
       "    -0.00445218663662672,\n",
       "    0.027981124818325043,\n",
       "    -0.007721373345702887,\n",
       "    -0.008812283165752888,\n",
       "    0.030573805794119835,\n",
       "    -0.0563872791826725,\n",
       "    -0.04309801384806633,\n",
       "    0.023149952292442322,\n",
       "    0.03499411419034004,\n",
       "    -0.03247227147221565,\n",
       "    0.06029755249619484,\n",
       "    0.036524221301078796,\n",
       "    -0.0019834721460938454,\n",
       "    0.07673203200101852,\n",
       "    0.006216059904545546,\n",
       "    0.02822197414934635,\n",
       "    0.0478300116956234,\n",
       "    0.01999056525528431,\n",
       "    -0.007650535088032484,\n",
       "    0.04145456850528717,\n",
       "    -0.04703662544488907,\n",
       "    0.06879814714193344,\n",
       "    -0.03765763342380524,\n",
       "    -0.04454311728477478,\n",
       "    -0.025190096348524094,\n",
       "    -0.0003603897930588573,\n",
       "    -0.015116890892386436,\n",
       "    -0.05774737522006035,\n",
       "    -0.03689258173108101,\n",
       "    0.0050720213912427425,\n",
       "    -0.04561985656619072,\n",
       "    0.03241560235619545,\n",
       "    -0.05208031088113785,\n",
       "    -0.033379003405570984,\n",
       "    0.04675327241420746,\n",
       "    -0.03411572054028511,\n",
       "    -0.014989381656050682,\n",
       "    -0.05233532935380936,\n",
       "    0.0057626948691904545,\n",
       "    -0.009974030777812004,\n",
       "    0.0013627515872940421,\n",
       "    0.031707219779491425,\n",
       "    -0.03153720498085022,\n",
       "    0.017114531248807907,\n",
       "    -0.03437073901295662,\n",
       "    -0.00465053366497159,\n",
       "    -0.033067312091588974,\n",
       "    0.04309801384806633,\n",
       "    -0.0169303510338068,\n",
       "    0.0818323940038681,\n",
       "    -0.017950423061847687,\n",
       "    -0.015896111726760864,\n",
       "    0.020188912749290466,\n",
       "    -0.020514769479632378,\n",
       "    0.005546638276427984,\n",
       "    0.01603778824210167,\n",
       "    -0.02224322222173214,\n",
       "    0.02568596415221691,\n",
       "    0.024878406897187233,\n",
       "    -0.02843448892235756,\n",
       "    0.055338870733976364,\n",
       "    -0.018106266856193542,\n",
       "    0.036467552185058594,\n",
       "    -0.0034037798177450895,\n",
       "    0.021534839645028114,\n",
       "    -0.005564347840845585,\n",
       "    -0.026635196059942245,\n",
       "    -0.028122801333665848,\n",
       "    0.010604491457343102,\n",
       "    -0.03972611203789711,\n",
       "    0.0337190255522728,\n",
       "    0.014344753697514534,\n",
       "    -0.016292806714773178,\n",
       "    -0.019933894276618958,\n",
       "    0.05046519637107849,\n",
       "    0.04474146291613579,\n",
       "    -0.023787496611475945,\n",
       "    0.0098677733913064,\n",
       "    0.04731997847557068,\n",
       "    0.00310448813252151,\n",
       "    0.02565762773156166,\n",
       "    -0.003129281336441636,\n",
       "    0.009549001231789589,\n",
       "    -0.01083825808018446,\n",
       "    0.008344750851392746,\n",
       "    -0.03519246354699135,\n",
       "    -0.007066119462251663,\n",
       "    -0.012715472839772701,\n",
       "    -0.04425976425409317,\n",
       "    0.051116909831762314,\n",
       "    -0.018205439671874046,\n",
       "    0.05383709818124771,\n",
       "    -0.04672493413090706,\n",
       "    -0.04811336472630501,\n",
       "    -0.023532480001449585,\n",
       "    0.033435672521591187,\n",
       "    -0.01496104709804058,\n",
       "    -0.027230238541960716,\n",
       "    0.02730107679963112,\n",
       "    -0.02279576100409031,\n",
       "    -0.046299904584884644,\n",
       "    -0.06347110867500305,\n",
       "    -0.017596231773495674,\n",
       "    -0.028576165437698364,\n",
       "    0.04179459065198898,\n",
       "    -0.03828101232647896,\n",
       "    -0.00634002685546875,\n",
       "    -0.001744392910040915,\n",
       "    0.02227155864238739,\n",
       "    0.02452421560883522,\n",
       "    0.024892574176192284,\n",
       "    -0.009520665742456913,\n",
       "    0.012765060178935528,\n",
       "    0.022767426446080208,\n",
       "    0.005900829564779997,\n",
       "    -0.024368371814489365,\n",
       "    0.010944515466690063,\n",
       "    -0.021747354418039322,\n",
       "    0.03711926192045212,\n",
       "    0.030233781784772873,\n",
       "    -0.02815113589167595,\n",
       "    0.05669896677136421,\n",
       "    -0.004969305824488401,\n",
       "    -0.020217247307300568,\n",
       "    0.053695421665906906,\n",
       "    0.004948054440319538,\n",
       "    -0.02341913804411888,\n",
       "    0.03159387782216072,\n",
       "    -0.0016850657993927598,\n",
       "    0.026762705296278,\n",
       "    0.031083840876817703,\n",
       "    -0.03626920282840729,\n",
       "    -0.011050772853195667,\n",
       "    -0.04882174730300903,\n",
       "    0.04936011880636215,\n",
       "    0.0007964880205690861,\n",
       "    0.03179222345352173,\n",
       "    0.01030697114765644,\n",
       "    -0.057917386293411255,\n",
       "    -0.0065808771178126335,\n",
       "    0.005525386426597834,\n",
       "    -0.016193632036447525,\n",
       "    0.02765526808798313,\n",
       "    0.0337473601102829,\n",
       "    -0.025530120357871056,\n",
       "    0.03748762235045433,\n",
       "    -0.030942164361476898,\n",
       "    -0.012616299092769623,\n",
       "    0.004611572716385126,\n",
       "    -0.03527746722102165,\n",
       "    -0.01480520237237215,\n",
       "    -0.020174745470285416,\n",
       "    -0.012998825870454311,\n",
       "    -0.03485243767499924,\n",
       "    0.009754432365298271,\n",
       "    0.04066117852926254,\n",
       "    -0.023830000311136246,\n",
       "    0.027400249615311623,\n",
       "    0.0027219613548368216,\n",
       "    -0.018757978454232216,\n",
       "    -0.03915940597653389,\n",
       "    -0.016732003539800644,\n",
       "    0.01574026793241501,\n",
       "    -0.04063284397125244,\n",
       "    -0.04754665866494179,\n",
       "    -0.013374269008636475,\n",
       "    0.018219608813524246,\n",
       "    -0.050890225917100906,\n",
       "    -0.03216058388352394,\n",
       "    0.04598821699619293,\n",
       "    -0.01888548769056797,\n",
       "    -0.055055517703294754,\n",
       "    -0.004693036433309317,\n",
       "    -0.007317595183849335,\n",
       "    0.010795755311846733,\n",
       "    -0.002066707005724311,\n",
       "    -0.019848888739943504,\n",
       "    0.021591510623693466,\n",
       "    0.04284299910068512,\n",
       "    9.989305544877425e-05,\n",
       "    0.04678160697221756,\n",
       "    -0.0075796968303620815,\n",
       "    0.016519488766789436,\n",
       "    -0.0023341216146945953,\n",
       "    -0.0016133420867845416,\n",
       "    -0.040689513087272644,\n",
       "    -0.009534833952784538,\n",
       "    0.03485243767499924,\n",
       "    -0.010781588032841682,\n",
       "    -0.04431643337011337,\n",
       "    -0.009641091339290142,\n",
       "    -0.02701772376894951,\n",
       "    -0.027782777324318886,\n",
       "    0.036240868270397186,\n",
       "    -0.020373091101646423,\n",
       "    0.01999056525528431,\n",
       "    0.040689513087272644,\n",
       "    -0.04148290306329727,\n",
       "    -0.017610399052500725,\n",
       "    0.029752081260085106,\n",
       "    0.026295172050595284,\n",
       "    0.011369545012712479,\n",
       "    0.0016275097150355577,\n",
       "    -0.012332946062088013,\n",
       "    0.01778041012585163,\n",
       "    0.010937431827187538,\n",
       "    0.017964590340852737,\n",
       "    0.0168595127761364,\n",
       "    -0.02870367467403412,\n",
       "    0.024595053866505623,\n",
       "    -0.03289730101823807,\n",
       "    -0.007345930673182011,\n",
       "    0.025005916133522987,\n",
       "    -0.05414878949522972,\n",
       "    -0.008911456912755966,\n",
       "    -0.024113353341817856,\n",
       "    0.035589154809713364,\n",
       "    0.010172378271818161,\n",
       "    0.0648312047123909,\n",
       "    -0.0478300116956234,\n",
       "    -0.0365808941423893,\n",
       "    -0.005337665323168039,\n",
       "    0.029808752238750458,\n",
       "    0.04032115265727043,\n",
       "    -0.04176625609397888,\n",
       "    -0.029128704220056534,\n",
       "    0.010746168904006481,\n",
       "    0.005925622768700123,\n",
       "    0.005302245728671551,\n",
       "    -0.009187726303935051,\n",
       "    0.03437073901295662,\n",
       "    -0.06080758571624756,\n",
       "    -0.048056695610284805,\n",
       "    -0.009541917592287064,\n",
       "    -0.018417954444885254,\n",
       "    0.023844167590141296,\n",
       "    -0.032018907368183136,\n",
       "    0.029638739302754402,\n",
       "    -0.006587960757315159,\n",
       "    0.012552544474601746,\n",
       "    -0.057634033262729645,\n",
       "    0.025416778400540352,\n",
       "    0.005783946253359318,\n",
       "    0.0038713125977665186,\n",
       "    -0.01806376315653324,\n",
       "    0.0031186556443572044,\n",
       "    -0.033407337963581085,\n",
       "    0.016250303015112877,\n",
       "    0.03247227147221565,\n",
       "    -0.023574981838464737,\n",
       "    -0.004338845144957304,\n",
       "    -0.014606854878365993,\n",
       "    -0.010179461911320686,\n",
       "    -0.030630476772785187,\n",
       "    -0.00871310941874981,\n",
       "    0.05040852725505829,\n",
       "    0.004547818098217249,\n",
       "    -0.06449117511510849,\n",
       "    0.007501774933189154,\n",
       "    0.027442753314971924,\n",
       "    -0.015315238386392593,\n",
       "    0.02139316312968731,\n",
       "    0.010427395813167095,\n",
       "    -0.00971901323646307,\n",
       "    0.025161759927868843,\n",
       "    0.05465882271528244,\n",
       "    0.012651718221604824,\n",
       "    -0.0023199538700282574,\n",
       "    -0.012148766778409481,\n",
       "    0.021237319335341454,\n",
       "    0.05516885966062546,\n",
       "    0.01298465859144926,\n",
       "    0.03346400707960129,\n",
       "    0.0423612967133522,\n",
       "    -0.08857619762420654,\n",
       "    -0.010668246075510979,\n",
       "    -0.028377817943692207,\n",
       "    0.026082657277584076,\n",
       "    0.03074381686747074,\n",
       "    0.02849115990102291,\n",
       "    0.03326566144824028,\n",
       "    -0.0295820701867342,\n",
       "    0.046101558953523636,\n",
       "    -0.014153489843010902,\n",
       "    0.010073204524815083,\n",
       "    0.033067312091588974,\n",
       "    0.012871317565441132,\n",
       "    0.03136719390749931,\n",
       "    0.05273202434182167,\n",
       "    0.006828811019659042,\n",
       "    0.025190096348524094,\n",
       "    -0.0225124079734087,\n",
       "    -0.009350653737783432,\n",
       "    0.024368371814489365,\n",
       "    0.04561985656619072,\n",
       "    0.0015123974299058318,\n",
       "    0.0071121640503406525,\n",
       "    -0.01228335965424776,\n",
       "    0.0051003568805754185,\n",
       "    0.01854546368122101,\n",
       "    -0.04190793260931969,\n",
       "    0.020642276853322983,\n",
       "    0.05797405540943146,\n",
       "    -0.0197355467826128,\n",
       "    0.013877220451831818,\n",
       "    0.029157040640711784,\n",
       "    -0.0025979941710829735,\n",
       "    -0.002332350704818964,\n",
       "    0.028023626655340195,\n",
       "    -0.0050720213912427425,\n",
       "    0.05208031088113785,\n",
       "    0.02564346045255661,\n",
       "    0.010172378271818161,\n",
       "    0.0535820834338665,\n",
       "    0.03233059495687485,\n",
       "    0.031140511855483055,\n",
       "    -0.0703282505273819,\n",
       "    -0.03272728994488716,\n",
       "    -0.005273910705000162,\n",
       "    0.010370725765824318,\n",
       "    -0.020174745470285416,\n",
       "    0.01722787134349346,\n",
       "    -0.013629286549985409,\n",
       "    -0.021208982914686203,\n",
       "    -0.012595048174262047,\n",
       "    0.04057617112994194,\n",
       "    -0.003352422034367919,\n",
       "    0.007430936675518751,\n",
       "    -0.005174736957997084,\n",
       "    0.032557278871536255,\n",
       "    0.0042432136833667755,\n",
       "    0.009145223535597324,\n",
       "    -0.022484073415398598,\n",
       "    0.03153720498085022,\n",
       "    0.030800487846136093,\n",
       "    0.002922079525887966,\n",
       "    0.021251486614346504,\n",
       "    -0.020500600337982178,\n",
       "    0.0028158219065517187,\n",
       "    0.01200000662356615,\n",
       "    -0.03122551739215851,\n",
       "    0.050040166825056076,\n",
       "    -0.0450248159468174,\n",
       "    0.012623382732272148,\n",
       "    -0.05162694677710533,\n",
       "    -0.025289269164204597,\n",
       "    -0.030630476772785187,\n",
       "    -0.029185375198721886,\n",
       "    0.04593154788017273,\n",
       "    0.02081228978931904,\n",
       "    0.014536016620695591,\n",
       "    -0.0451664924621582,\n",
       "    -0.01072491705417633,\n",
       "    -0.03745928779244423,\n",
       "    -0.01836128532886505,\n",
       "    0.01949469745159149,\n",
       "    -0.04811336472630501,\n",
       "    0.005408503580838442,\n",
       "    0.024878406897187233,\n",
       "    -0.008783947676420212,\n",
       "    0.011943335644900799,\n",
       "    -0.022413235157728195,\n",
       "    -0.011794575490057468,\n",
       "    -0.04085952416062355,\n",
       "    0.027726106345653534,\n",
       "    0.03012043982744217,\n",
       "    0.006807559635490179,\n",
       "    -0.010675330646336079,\n",
       "    -0.016774507239460945,\n",
       "    0.02089729532599449,\n",
       "    0.028632836416363716,\n",
       "    -0.01100827008485794,\n",
       "    -0.02537427470088005,\n",
       "    0.039867788553237915,\n",
       "    -0.058427419513463974,\n",
       "    0.013452190905809402,\n",
       "    -0.03448408097028732,\n",
       "    0.04338136687874794,\n",
       "    -0.026110993698239326,\n",
       "    0.028902022168040276,\n",
       "    -0.04281466081738472,\n",
       "    -0.002399647142738104,\n",
       "    -0.010264468379318714,\n",
       "    0.005394335836172104,\n",
       "    -0.025459280237555504,\n",
       "    0.022129882127046585,\n",
       "    0.04423142597079277,\n",
       "    0.05012517422437668,\n",
       "    0.011397880502045155,\n",
       "    0.019098002463579178,\n",
       "    -0.02762693352997303,\n",
       "    -0.015697764232754707,\n",
       "    -0.018446290865540504,\n",
       "    0.017072027549147606,\n",
       "    -0.04428809881210327,\n",
       "    -0.0003486571949906647,\n",
       "    -0.04819837212562561,\n",
       "    -0.011801659129559994,\n",
       "    0.008840618655085564,\n",
       "    -0.006782765965908766,\n",
       "    -0.017369547858834267,\n",
       "    -0.012177102267742157,\n",
       "    0.009131055325269699,\n",
       "    0.0009510040399618447,\n",
       "    0.00051844771951437,\n",
       "    -0.021818192675709724,\n",
       "    0.01507438812404871,\n",
       "    -0.06500121206045151,\n",
       "    0.012318778783082962,\n",
       "    0.06902483105659485,\n",
       "    -0.017029523849487305,\n",
       "    -0.030772153288125992,\n",
       "    -0.022639917209744453,\n",
       "    -0.033775694668293,\n",
       "    -0.07140499353408813,\n",
       "    0.005096815060824156,\n",
       "    -0.007009448949247599,\n",
       "    0.00327804172411561,\n",
       "    -0.02595514990389347,\n",
       "    -0.025799304246902466,\n",
       "    -0.046639930456876755,\n",
       "    0.0029291631653904915,\n",
       "    0.023730827495455742,\n",
       "    -0.046923283487558365,\n",
       "    0.016533656045794487,\n",
       "    0.003547227242961526,\n",
       "    -0.022413235157728195,\n",
       "    -0.013239676132798195,\n",
       "    0.009513582102954388,\n",
       "    0.0423612967133522,\n",
       "    0.043551381677389145,\n",
       "    0.007551361806690693,\n",
       "    0.016576159745454788,\n",
       "    -0.0037119262851774693,\n",
       "    0.011610395275056362,\n",
       "    -0.007863050326704979,\n",
       "    -0.013473442755639553,\n",
       "    0.0011466947617009282,\n",
       "    -0.016391979530453682,\n",
       "    -0.04151123762130737,\n",
       "    -0.0014929169556125998,\n",
       "    0.009194809943437576,\n",
       "    -0.011865413747727871,\n",
       "    -0.04318302124738693,\n",
       "    -0.005840616766363382,\n",
       "    -0.014160574413836002,\n",
       "    -0.004632824100553989,\n",
       "    -0.000707940140273422,\n",
       "    -0.04595988243818283,\n",
       "    -0.025204263627529144,\n",
       "    -0.016462817788124084,\n",
       "    0.0239150058478117,\n",
       "    0.00429988419637084,\n",
       "    0.027669435366988182,\n",
       "    -0.05137192830443382,\n",
       "    -0.027669435366988182,\n",
       "    -0.032528940588235855,\n",
       "    -0.0005600652075372636,\n",
       "    0.04958680272102356,\n",
       "    0.01636364497244358,\n",
       "    -0.011617479845881462,\n",
       "    -0.01666116528213024,\n",
       "    -0.008358918130397797,\n",
       "    0.02142149955034256,\n",
       "    -0.003325857687741518,\n",
       "    0.010377809405326843,\n",
       "    -0.008259744383394718,\n",
       "    0.011929168365895748,\n",
       "    0.0011670608073472977,\n",
       "    0.0008381055085919797,\n",
       "    -0.008961043320596218,\n",
       "    0.0009155849111266434,\n",
       "    -0.019225511699914932,\n",
       "    -0.017270375043153763,\n",
       "    0.011837078258395195,\n",
       "    -0.052533674985170364,\n",
       "    0.014720196835696697,\n",
       "    0.03556082025170326,\n",
       "    -0.0033046063035726547,\n",
       "    0.009641091339290142,\n",
       "    -0.04697995260357857,\n",
       "    -0.02221488766372204,\n",
       "    0.00943566020578146,\n",
       "    -0.02815113589167595,\n",
       "    -0.017851248383522034,\n",
       "    -0.015669429674744606,\n",
       "    0.037629298865795135,\n",
       "    -0.020968133583664894,\n",
       "    0.00899646244943142,\n",
       "    -0.012743808329105377,\n",
       "    -0.014415591955184937,\n",
       "    0.028845351189374924,\n",
       "    -0.006332943215966225,\n",
       "    0.01182999461889267,\n",
       "    -0.00958442036062479,\n",
       "    0.01751122437417507,\n",
       "    -0.015187729150056839,\n",
       "    -0.013438023626804352,\n",
       "    -0.013331766240298748,\n",
       "    -0.028264477849006653,\n",
       "    -0.023291628807783127,\n",
       "    -0.042984675616025925,\n",
       "    -0.0006185068050399423,\n",
       "    0.014203077182173729,\n",
       "    0.010965767316520214,\n",
       "    -0.03048880025744438,\n",
       "    0.03485243767499924,\n",
       "    0.008259744383394718,\n",
       "    -0.0071015385910868645,\n",
       "    0.033350665122270584,\n",
       "    0.015046052634716034,\n",
       "    0.005585599225014448,\n",
       "    -0.003690674901008606,\n",
       "    -0.021832359954714775,\n",
       "    -0.030942164361476898,\n",
       "    0.00383943528868258,\n",
       "    -0.012864232994616032,\n",
       "    -0.02870367467403412,\n",
       "    0.032840631902217865,\n",
       "    0.012162934057414532,\n",
       "    -0.023844167590141296,\n",
       "    -0.015655262395739555,\n",
       "    -0.0020507685840129852,\n",
       "    -0.013310514390468597,\n",
       "    -0.0020029526203870773,\n",
       "    0.022937437519431114,\n",
       "    0.0031806391198188066,\n",
       "    -0.006510038860142231,\n",
       "    -0.041001200675964355,\n",
       "    -0.004997641313821077,\n",
       "    -0.06704135984182358,\n",
       "    -0.046866610646247864,\n",
       "    -0.01226210780441761,\n",
       "    -0.023263294249773026,\n",
       "    -0.006857146508991718,\n",
       "    0.0017452783649787307,\n",
       "    0.006297524087131023,\n",
       "    -0.006046047899872065,\n",
       "    0.0013361872406676412,\n",
       "    -0.033435672521591187,\n",
       "    -0.0028441573958843946,\n",
       "    0.04037782549858093,\n",
       "    -0.00970484595745802,\n",
       "    -0.0019179467344656587,\n",
       "    -0.005695398431271315,\n",
       "    -0.043013010174036026,\n",
       "    0.02785361558198929,\n",
       "    -0.020217247307300568,\n",
       "    -0.02227155864238739,\n",
       "    0.028873687610030174,\n",
       "    0.004211336374282837,\n",
       "    0.011978754773736,\n",
       "    0.0049055516719818115,\n",
       "    0.01042031217366457,\n",
       "    -0.0031062590423971415,\n",
       "    -0.0067615145817399025,\n",
       "    -0.002066707005724311,\n",
       "    0.020755618810653687,\n",
       "    0.019848888739943504,\n",
       "    0.005681230686604977,\n",
       "    -0.030630476772785187,\n",
       "    0.015485250391066074,\n",
       "    0.025487616658210754,\n",
       "    -0.014096819795668125,\n",
       "    -0.0017798120388761163,\n",
       "    0.018177105113863945,\n",
       "    -0.01366470567882061,\n",
       "    -0.039584435522556305,\n",
       "    -0.020089738070964813,\n",
       "    0.01298465859144926,\n",
       "    0.047745008021593094,\n",
       "    0.021874863654375076,\n",
       "    0.01608029194176197,\n",
       "    -0.008358918130397797,\n",
       "    -0.025147592648863792,\n",
       "    0.0027874866500496864,\n",
       "    0.028675340116024017,\n",
       "    -0.021194815635681152,\n",
       "    -0.03544747829437256,\n",
       "    -0.007501774933189154,\n",
       "    -0.011348294094204903,\n",
       "    -0.000279589876299724,\n",
       "    0.007402601186186075,\n",
       "    0.01508855540305376,\n",
       "    0.027541926130652428,\n",
       "    -0.002766235265880823,\n",
       "    0.008096816018223763,\n",
       "    -0.039046064019203186,\n",
       "    0.012729640118777752,\n",
       "    0.020132241770625114,\n",
       "    0.001955136889591813,\n",
       "    -0.00903188157826662,\n",
       "    0.0003774352662730962,\n",
       "    -0.03700592368841171,\n",
       "    0.014904376119375229,\n",
       "    -0.013976394198834896,\n",
       "    -0.006021254695951939,\n",
       "    -0.0240000132471323,\n",
       "    0.018998829647898674,\n",
       "    -0.002580284606665373,\n",
       "    -0.059900857508182526,\n",
       "    -0.04842505604028702,\n",
       "    -0.015952782705426216,\n",
       "    0.0337190255522728,\n",
       "    -0.01827627792954445,\n",
       "    0.004172375425696373,\n",
       "    0.01889965496957302,\n",
       "    0.004746165592223406,\n",
       "    -0.01013695914298296,\n",
       "    0.021024804562330246,\n",
       "    -0.057605694979429245,\n",
       "    0.03890438750386238,\n",
       "    -0.021038971841335297,\n",
       "    -0.02900119498372078,\n",
       "    0.02363165281713009,\n",
       "    0.014691861346364021,\n",
       "    -0.01861630193889141,\n",
       "    0.033123984932899475,\n",
       "    -0.011057856492698193,\n",
       "    -0.0023146411404013634,\n",
       "    -0.0030123982578516006,\n",
       "    0.005334123037755489,\n",
       "    -0.010930348187685013,\n",
       "    -0.03754429146647453,\n",
       "    -0.025246765464544296,\n",
       "    0.025983484461903572,\n",
       "    0.011610395275056362,\n",
       "    0.0008783948142081499,\n",
       "    -0.004069659858942032,\n",
       "    0.00522078201174736,\n",
       "    -0.020217247307300568,\n",
       "    -0.03499411419034004,\n",
       "    0.0038040161598473787,\n",
       "    -0.019041331484913826,\n",
       "    -0.03184889629483223,\n",
       "    0.0023358925245702267,\n",
       "    -0.022370731458067894,\n",
       "    0.006017712876200676,\n",
       "    0.02171901986002922,\n",
       "    -0.02059977501630783,\n",
       "    -0.04335303232073784,\n",
       "    0.0024368371814489365,\n",
       "    0.032217252999544144,\n",
       "    0.02898702770471573,\n",
       "    0.028023626655340195,\n",
       "    0.01241795253008604,\n",
       "    0.015981117263436317,\n",
       "    0.006063757464289665,\n",
       "    0.02171901986002922,\n",
       "    0.035362474620342255,\n",
       "    -0.007530109956860542,\n",
       "    0.017879584804177284,\n",
       "    -0.009641091339290142,\n",
       "    -0.019551368430256844,\n",
       "    -0.028972860425710678,\n",
       "    -0.04760333150625229,\n",
       "    -0.01496104709804058,\n",
       "    -0.0029574986547231674,\n",
       "    -0.03709092736244202,\n",
       "    -0.018687140196561813,\n",
       "    0.026550190523266792,\n",
       "    0.04108620807528496,\n",
       "    0.008918540552258492,\n",
       "    0.0015593278221786022,\n",
       "    0.015116890892386436,\n",
       "    -0.01379929855465889,\n",
       "    0.04338136687874794,\n",
       "    -0.07491857558488846,\n",
       "    0.012715472839772701,\n",
       "    -0.006364820059388876,\n",
       "    0.011645814403891563,\n",
       "    -0.018120434135198593,\n",
       "    -0.04145456850528717,\n",
       "    0.03176388889551163,\n",
       "    0.02363165281713009,\n",
       "    -0.01889965496957302,\n",
       "    0.003352422034367919,\n",
       "    -0.0072680083103477955,\n",
       "    0.04119955003261566,\n",
       "    -0.017355380579829216,\n",
       "    -0.04190793260931969,\n",
       "    0.0008757383329793811,\n",
       "    0.017327046021819115,\n",
       "    -0.03320898860692978,\n",
       "    -0.0239291749894619,\n",
       "    0.032217252999544144,\n",
       "    -0.020953966304659843,\n",
       "    -0.010909096337854862,\n",
       "    -0.008103900589048862,\n",
       "    0.016448650509119034,\n",
       "    0.01949469745159149,\n",
       "    -0.017256207764148712,\n",
       "    0.027952788397669792,\n",
       "    0.023263294249773026,\n",
       "    -0.01861630193889141,\n",
       "    0.02086896076798439,\n",
       "    -0.029412057250738144,\n",
       "    0.016391979530453682,\n",
       "    -0.03301064297556877,\n",
       "    -0.02483590506017208,\n",
       "    -0.028335316106677055,\n",
       "    -0.01939552277326584,\n",
       "    -0.007657619193196297,\n",
       "    0.002690084045752883,\n",
       "    -0.019877223297953606,\n",
       "    0.040689513087272644,\n",
       "    -0.017256207764148712,\n",
       "    -0.022115712985396385,\n",
       "    0.012368365190923214,\n",
       "    -0.007211337797343731,\n",
       "    0.01807793229818344,\n",
       "    -0.03527746722102165,\n",
       "    0.03794098645448685,\n",
       "    0.04389140382409096,\n",
       "    -0.05831408128142357,\n",
       "    -0.033690690994262695,\n",
       "    0.033095650374889374,\n",
       "    -0.020911462604999542,\n",
       "    0.018814649432897568,\n",
       "    0.0010112165473401546,\n",
       "    0.04312635213136673,\n",
       "    0.009329402819275856,\n",
       "    0.028391987085342407,\n",
       "    -0.04839671775698662,\n",
       "    0.0042432136833667755,\n",
       "    0.0004617770900949836,\n",
       "    -0.001399056171067059,\n",
       "    0.012977574951946735,\n",
       "    0.02819363959133625,\n",
       "    -0.06041089445352554,\n",
       "    -0.04819837212562561,\n",
       "    0.016703668981790543,\n",
       "    -0.01799292489886284,\n",
       "    -0.031112177297472954,\n",
       "    -0.02334829978644848,\n",
       "    -0.033775694668293,\n",
       "    -0.046384911984205246,\n",
       "    -0.003108029952272773,\n",
       "    0.03247227147221565,\n",
       "    -0.01269422098994255,\n",
       "    0.019650541245937347,\n",
       "    -0.011334125883877277,\n",
       "    -0.00787721760571003,\n",
       "    -0.01237544883042574,\n",
       "    0.005985835567116737,\n",
       "    0.04204960912466049,\n",
       "    0.03332233056426048,\n",
       "    0.020713115110993385,\n",
       "    0.0009306380525231361,\n",
       "    -0.022384898737072945,\n",
       "    0.009237312711775303,\n",
       "    -0.009187726303935051,\n",
       "    -0.017284542322158813,\n",
       "    -0.04063284397125244,\n",
       "    -0.023291628807783127,\n",
       "    -0.0055820574052631855,\n",
       "    0.0038004741072654724,\n",
       "    -0.01353011280298233,\n",
       "    -0.01603778824210167,\n",
       "    -0.01912633888423443,\n",
       "    0.01722787134349346,\n",
       "    -0.004654075484722853,\n",
       "    -0.009308150969445705,\n",
       "    -0.019523032009601593,\n",
       "    -0.028916189447045326,\n",
       "    -0.004090911243110895,\n",
       "    0.024665892124176025,\n",
       "    0.010448647662997246,\n",
       "    0.0069315265864133835,\n",
       "    0.021237319335341454,\n",
       "    -0.028023626655340195,\n",
       "    0.02197403647005558,\n",
       "    0.01184416189789772,\n",
       "    0.0003444511676207185,\n",
       "    0.021634014323353767,\n",
       "    -0.008606852032244205,\n",
       "    -0.010547821410000324,\n",
       "    -0.028944525867700577,\n",
       "    0.01365053839981556,\n",
       "    0.011341210454702377,\n",
       "    -0.014776866883039474,\n",
       "    0.010887845419347286,\n",
       "    -0.009924444369971752,\n",
       "    0.02055727131664753,\n",
       "    -0.005458089988678694,\n",
       "    0.005174736957997084,\n",
       "    -0.006871314253658056,\n",
       "    0.015683596953749657,\n",
       "    0.061034269630908966,\n",
       "    0.0422762893140316,\n",
       "    -0.012481706216931343,\n",
       "    0.05491384118795395,\n",
       "    0.0006100947503000498,\n",
       "    -0.0014822911471128464,\n",
       "    0.059050798416137695,\n",
       "    0.003991737496107817,\n",
       "    -0.01834711618721485,\n",
       "    0.004381348378956318,\n",
       "    0.0394427590072155,\n",
       "    -0.03289730101823807,\n",
       "    0.016760339960455894,\n",
       "    -0.015102723613381386,\n",
       "    0.014040148817002773,\n",
       "    0.030885493382811546,\n",
       "    0.023220790550112724,\n",
       "    0.020146409049630165,\n",
       "    0.025515951216220856,\n",
       "    0.025275101885199547,\n",
       "    0.04615822806954384,\n",
       "    0.0012786311563104391,\n",
       "    -0.021889030933380127,\n",
       "    -0.001939198118634522,\n",
       "    0.00844392366707325,\n",
       "    -0.02731524407863617,\n",
       "    -0.0051109823398292065,\n",
       "    0.003217829391360283,\n",
       "    0.0367225706577301,\n",
       "    -0.028590332716703415,\n",
       "    -0.01307674776762724,\n",
       "    0.002371311653405428,\n",
       "    0.0012511812383309007,\n",
       "    0.0140826515853405,\n",
       "    0.016689501702785492,\n",
       "    0.01946636103093624,\n",
       "    0.030375458300113678,\n",
       "    -0.011468718759715557,\n",
       "    -0.005741443485021591,\n",
       "    0.028576165437698364,\n",
       "    0.03907439857721329,\n",
       "    0.019225511699914932,\n",
       "    0.02565762773156166,\n",
       "    0.01915467344224453,\n",
       "    0.06063757464289665,\n",
       "    0.009549001231789589,\n",
       "    0.024113353341817856,\n",
       "    0.024963412433862686,\n",
       "    -0.030658811330795288,\n",
       "    -0.0197072122246027,\n",
       "    -0.029950428754091263,\n",
       "    0.01496104709804058,\n",
       "    0.0044486443512141705,\n",
       "    -0.016831178218126297,\n",
       "    0.04655492305755615,\n",
       "    -0.01575443521142006,\n",
       "    -0.030658811330795288,\n",
       "    0.032812297344207764,\n",
       "    -0.0024527758359909058,\n",
       "    -0.010696581564843655,\n",
       "    -0.040774520486593246,\n",
       "    -0.03643921762704849,\n",
       "    0.006336485035717487,\n",
       "    0.032557278871536255,\n",
       "    0.0053978776559233665,\n",
       "    0.04343803972005844,\n",
       "    0.025275101885199547,\n",
       "    -0.004707204177975655,\n",
       "    0.010214881040155888,\n",
       "    -0.0049197194166481495,\n",
       "    0.014125155285000801,\n",
       "    0.009761516004800797,\n",
       "    -0.0015097410650923848,\n",
       "    -0.026040155440568924,\n",
       "    0.006722553633153439,\n",
       "    -0.02704605832695961,\n",
       "    0.04394807294011116,\n",
       "    0.008805199526250362,\n",
       "    0.005199530627578497,\n",
       "    0.04029281809926033,\n",
       "    -0.03488077223300934,\n",
       "    -0.0050720213912427425,\n",
       "    0.0028016543947160244,\n",
       "    0.0070625776425004005,\n",
       "    0.06245103478431702,\n",
       "    -0.026422681286931038,\n",
       "    0.02534594014286995,\n",
       "    0.02166234888136387,\n",
       "    -0.007933888584375381,\n",
       "    -0.010887845419347286,\n",
       "    0.0024456919636577368,\n",
       "    0.0015805793227627873,\n",
       "    -0.0028937440365552902,\n",
       "    -0.020217247307300568,\n",
       "    0.037062592804431915,\n",
       "    -0.015711933374404907,\n",
       "    0.0049161771312355995,\n",
       "    -0.04678160697221756,\n",
       "    -0.0033807572908699512,\n",
       "    0.029213709756731987,\n",
       "    0.03357734903693199,\n",
       "    0.0011059627868235111,\n",
       "    0.015471082180738449,\n",
       "    0.029667075723409653,\n",
       "    0.01508855540305376,\n",
       "    0.023291628807783127,\n",
       "    -0.0012591505656018853,\n",
       "    0.014946878887712955,\n",
       "    -0.02618183195590973,\n",
       "    0.04987015575170517,\n",
       "    0.019083835184574127,\n",
       "    0.03694925084710121,\n",
       "    0.011064941063523293,\n",
       "    0.020188912749290466,\n",
       "    0.035589154809713364,\n",
       "    0.050011832267045975,\n",
       "    -0.01799292489886284,\n",
       "    0.01722787134349346,\n",
       "    0.007685954216867685,\n",
       "    0.06052423268556595,\n",
       "    0.017043692991137505,\n",
       "    0.0012325862189754844,\n",
       "    -0.0024580885656177998,\n",
       "    0.01721370406448841,\n",
       "    -0.03349234163761139,\n",
       "    -0.011072024703025818,\n",
       "    -0.04646991938352585,\n",
       "    -0.010958683677017689,\n",
       "    -0.0010023617651313543,\n",
       "    ...]},\n",
       "  {'embedding': [-0.004044405184686184,\n",
       "    0.04927319660782814,\n",
       "    0.09141726791858673,\n",
       "    -0.003979283384978771,\n",
       "    0.01031666062772274,\n",
       "    0.008493251167237759,\n",
       "    0.016794564202427864,\n",
       "    0.0017565743764862418,\n",
       "    -0.02780357375741005,\n",
       "    0.05187806859612465,\n",
       "    0.016122782602906227,\n",
       "    -0.03528914973139763,\n",
       "    -0.031066516414284706,\n",
       "    0.00815736036747694,\n",
       "    -0.053386151790618896,\n",
       "    0.04433764889836311,\n",
       "    0.04609251022338867,\n",
       "    0.0014909460442140698,\n",
       "    -0.020852679386734962,\n",
       "    0.0566490963101387,\n",
       "    0.015437289141118526,\n",
       "    0.0076500955037772655,\n",
       "    0.026555975899100304,\n",
       "    0.020167186856269836,\n",
       "    -0.021867208182811737,\n",
       "    -0.007451302837580442,\n",
       "    -0.026158390566706657,\n",
       "    0.00972713716328144,\n",
       "    -0.0029973157215863466,\n",
       "    -0.03553592786192894,\n",
       "    0.07699450850486755,\n",
       "    -0.022566411644220352,\n",
       "    -0.04250052943825722,\n",
       "    0.05462003871798515,\n",
       "    -0.012866692617535591,\n",
       "    0.04524249956011772,\n",
       "    0.01599939353764057,\n",
       "    0.011091267690062523,\n",
       "    0.039950497448444366,\n",
       "    0.014683247543871403,\n",
       "    0.020551063120365143,\n",
       "    -0.04834092780947685,\n",
       "    -0.002968182321637869,\n",
       "    0.01811070926487446,\n",
       "    -0.031340714544057846,\n",
       "    0.020153477787971497,\n",
       "    -0.01115981675684452,\n",
       "    0.020729290321469307,\n",
       "    -0.0009648305713199079,\n",
       "    0.007656950503587723,\n",
       "    -0.019591374322772026,\n",
       "    -0.034493979066610336,\n",
       "    0.03690691292285919,\n",
       "    -0.015135672874748707,\n",
       "    0.017630865797400475,\n",
       "    -0.005000667180866003,\n",
       "    0.040526311844587326,\n",
       "    0.0006923473556526005,\n",
       "    -0.0002448493323754519,\n",
       "    -0.011365464888513088,\n",
       "    0.033369772136211395,\n",
       "    -0.009932785294950008,\n",
       "    -0.0007728927303105593,\n",
       "    -0.04263762757182121,\n",
       "    0.007129121106117964,\n",
       "    -0.011221511289477348,\n",
       "    -0.04433764889836311,\n",
       "    0.02547289803624153,\n",
       "    -0.04159568250179291,\n",
       "    -0.0028944918885827065,\n",
       "    -0.021030908450484276,\n",
       "    -0.02319706417620182,\n",
       "    0.01896071992814541,\n",
       "    -0.005459947045892477,\n",
       "    -0.012462252750992775,\n",
       "    -0.011831599287688732,\n",
       "    0.01477921660989523,\n",
       "    0.028434226289391518,\n",
       "    0.0051309107802808285,\n",
       "    0.013463071547448635,\n",
       "    -0.005237162113189697,\n",
       "    -0.013387667015194893,\n",
       "    -0.004263762850314379,\n",
       "    0.004106099717319012,\n",
       "    0.011386029422283173,\n",
       "    0.02231963351368904,\n",
       "    -0.009775121696293354,\n",
       "    0.0059740664437413216,\n",
       "    -0.07244283705949783,\n",
       "    -0.043131183832883835,\n",
       "    0.010933604091405869,\n",
       "    0.06860408186912537,\n",
       "    -0.027351148426532745,\n",
       "    0.019988959655165672,\n",
       "    0.016259880736470222,\n",
       "    -0.02637774869799614,\n",
       "    0.03863435238599777,\n",
       "    -0.02173011004924774,\n",
       "    0.013538475148379803,\n",
       "    0.0034086110536009073,\n",
       "    -0.0223059244453907,\n",
       "    0.0012038961285725236,\n",
       "    0.020838970318436623,\n",
       "    -0.04241827130317688,\n",
       "    0.08768819272518158,\n",
       "    -0.0522071048617363,\n",
       "    -0.014710667543113232,\n",
       "    0.004195213783532381,\n",
       "    -0.019563954323530197,\n",
       "    0.011358609423041344,\n",
       "    -0.10030125081539154,\n",
       "    -0.055168431252241135,\n",
       "    -0.02777615375816822,\n",
       "    -0.04101986810564995,\n",
       "    -0.0011096409289166331,\n",
       "    -0.04406345263123512,\n",
       "    -0.045681215822696686,\n",
       "    0.01811070926487446,\n",
       "    -0.013874366879463196,\n",
       "    -0.058897510170936584,\n",
       "    -0.02803664095699787,\n",
       "    0.010117867961525917,\n",
       "    -0.015642937272787094,\n",
       "    0.011084412224590778,\n",
       "    0.04011501744389534,\n",
       "    -0.008698899298906326,\n",
       "    0.006385361775755882,\n",
       "    -0.011502563022077084,\n",
       "    -0.013236858882009983,\n",
       "    -0.02040025405585766,\n",
       "    0.035919804126024246,\n",
       "    -0.005134338513016701,\n",
       "    0.0527280792593956,\n",
       "    -0.00930898729711771,\n",
       "    -0.051549032330513,\n",
       "    -0.005346841178834438,\n",
       "    -0.016972793266177177,\n",
       "    0.03007940761744976,\n",
       "    -0.020537354052066803,\n",
       "    -0.010330370627343655,\n",
       "    0.05067159980535507,\n",
       "    0.0021284539252519608,\n",
       "    -0.03416494280099869,\n",
       "    0.052590981125831604,\n",
       "    -0.007931147702038288,\n",
       "    0.003249234054237604,\n",
       "    -6.565732473973185e-05,\n",
       "    0.03805853798985481,\n",
       "    -0.0041403742507100105,\n",
       "    0.004335739649832249,\n",
       "    -0.023622069507837296,\n",
       "    -0.020989777520298958,\n",
       "    -0.05415390059351921,\n",
       "    0.06975571066141129,\n",
       "    -0.01835748739540577,\n",
       "    -0.012414268217980862,\n",
       "    0.018439745530486107,\n",
       "    0.012715884484350681,\n",
       "    0.027419697493314743,\n",
       "    -0.01573890633881092,\n",
       "    -0.010714246891438961,\n",
       "    0.058897510170936584,\n",
       "    -0.007752919569611549,\n",
       "    0.039155326783657074,\n",
       "    -0.04949255287647247,\n",
       "    -0.019701052457094193,\n",
       "    -0.024883374571800232,\n",
       "    0.008822287432849407,\n",
       "    -0.04694252088665962,\n",
       "    -0.018522005528211594,\n",
       "    -0.007691225036978722,\n",
       "    -0.00559704564511776,\n",
       "    0.0383327379822731,\n",
       "    -0.025363219901919365,\n",
       "    0.040608569979667664,\n",
       "    -0.050260305404663086,\n",
       "    0.021634140983223915,\n",
       "    -0.06322982162237167,\n",
       "    -0.009226728230714798,\n",
       "    -0.02519870176911354,\n",
       "    -0.03635852038860321,\n",
       "    0.0035885528195649385,\n",
       "    -0.05234420299530029,\n",
       "    -0.0315326526761055,\n",
       "    -0.041403744369745255,\n",
       "    0.010330370627343655,\n",
       "    -0.058074917644262314,\n",
       "    0.05270065739750862,\n",
       "    -0.04477636516094208,\n",
       "    0.008781158365309238,\n",
       "    -0.0016374700935557485,\n",
       "    0.07112669199705124,\n",
       "    0.042802147567272186,\n",
       "    0.009178743697702885,\n",
       "    0.0060014864429831505,\n",
       "    0.023087384179234505,\n",
       "    0.021634140983223915,\n",
       "    -0.014285662211477757,\n",
       "    0.008479541167616844,\n",
       "    -0.015286481007933617,\n",
       "    0.00026584253646433353,\n",
       "    0.022484151646494865,\n",
       "    0.04091018810868263,\n",
       "    -0.05906202644109726,\n",
       "    0.03068264201283455,\n",
       "    0.031039096415042877,\n",
       "    -0.03232782334089279,\n",
       "    0.03866177424788475,\n",
       "    -0.029010038822889328,\n",
       "    -0.01579374633729458,\n",
       "    0.020222026854753494,\n",
       "    -0.006495040841400623,\n",
       "    0.0027008401229977608,\n",
       "    0.03199878707528114,\n",
       "    -0.023101095110177994,\n",
       "    -0.01748005673289299,\n",
       "    0.0039552911184728146,\n",
       "    0.0149163156747818,\n",
       "    0.004332312382757664,\n",
       "    0.0007951712468639016,\n",
       "    0.016780855134129524,\n",
       "    -0.029448755085468292,\n",
       "    0.03668755665421486,\n",
       "    0.025843065232038498,\n",
       "    0.047957051545381546,\n",
       "    0.026281779631972313,\n",
       "    0.03814079985022545,\n",
       "    -0.06109108403325081,\n",
       "    0.06125560402870178,\n",
       "    -0.03526173159480095,\n",
       "    0.01945427432656288,\n",
       "    -0.04494088515639305,\n",
       "    -0.030189085751771927,\n",
       "    -0.004688768181949854,\n",
       "    -0.011701355688273907,\n",
       "    -0.057307168841362,\n",
       "    -0.04378925636410713,\n",
       "    -0.033973004668951035,\n",
       "    0.04587315395474434,\n",
       "    -0.02295028604567051,\n",
       "    0.032821375876665115,\n",
       "    0.0025997301563620567,\n",
       "    -0.0052440171130001545,\n",
       "    -0.026528557762503624,\n",
       "    -0.007074282038956881,\n",
       "    0.01403203047811985,\n",
       "    -0.013209438882768154,\n",
       "    -0.026816463097929955,\n",
       "    -0.010350936092436314,\n",
       "    -0.00383190275169909,\n",
       "    -0.013051776215434074,\n",
       "    -0.009877946227788925,\n",
       "    0.06498467922210693,\n",
       "    0.00931584183126688,\n",
       "    -0.04669574275612831,\n",
       "    -0.015697777271270752,\n",
       "    -0.033616550266742706,\n",
       "    -0.007053717039525509,\n",
       "    -0.0038970245514065027,\n",
       "    -0.03572786599397659,\n",
       "    -0.016163911670446396,\n",
       "    -0.011392883956432343,\n",
       "    -0.027721313759684563,\n",
       "    0.003055582521483302,\n",
       "    -0.024801116436719894,\n",
       "    0.04041663557291031,\n",
       "    -0.02641887776553631,\n",
       "    -0.010254967026412487,\n",
       "    -0.020838970318436623,\n",
       "    -0.022484151646494865,\n",
       "    0.017836512997746468,\n",
       "    -0.004291182849556208,\n",
       "    -0.041129544377326965,\n",
       "    -0.03068264201283455,\n",
       "    0.03581012412905693,\n",
       "    -0.037126269191503525,\n",
       "    0.020290575921535492,\n",
       "    0.013003791682422161,\n",
       "    0.01634213887155056,\n",
       "    -0.027145499363541603,\n",
       "    -0.03128587454557419,\n",
       "    -0.037400465458631516,\n",
       "    0.013682428747415543,\n",
       "    0.021606720983982086,\n",
       "    -0.0065738726407289505,\n",
       "    0.051439352333545685,\n",
       "    -0.023430131375789642,\n",
       "    0.0023426704574376345,\n",
       "    -0.012880402617156506,\n",
       "    0.0010839349124580622,\n",
       "    0.007773484103381634,\n",
       "    -0.0019656496588140726,\n",
       "    -0.007718644570559263,\n",
       "    -0.018768783658742905,\n",
       "    0.020797841250896454,\n",
       "    0.03345203027129173,\n",
       "    0.014710667543113232,\n",
       "    -0.03668755665421486,\n",
       "    -0.029393915086984634,\n",
       "    0.010056174360215664,\n",
       "    -0.005922654643654823,\n",
       "    0.05840395390987396,\n",
       "    -0.06284594535827637,\n",
       "    -0.057745881378650665,\n",
       "    -0.03010682761669159,\n",
       "    0.009802541695535183,\n",
       "    0.027625344693660736,\n",
       "    -0.029695531353354454,\n",
       "    -0.01837119646370411,\n",
       "    1.483448431827128e-05,\n",
       "    -0.006399071775376797,\n",
       "    -0.05692329257726669,\n",
       "    -0.0037976279854774475,\n",
       "    0.014614698477089405,\n",
       "    -0.049986109137535095,\n",
       "    -0.03320525214076042,\n",
       "    0.0020496223587542772,\n",
       "    0.04351506009697914,\n",
       "    0.004150656517595053,\n",
       "    -0.03644077852368355,\n",
       "    -0.0010033895960077643,\n",
       "    -0.021675270050764084,\n",
       "    -0.026871303096413612,\n",
       "    -0.04414571076631546,\n",
       "    -0.007670660503208637,\n",
       "    -0.019934119656682014,\n",
       "    0.008123084902763367,\n",
       "    -0.05933622643351555,\n",
       "    0.04420055076479912,\n",
       "    -0.004980102647095919,\n",
       "    0.013833236880600452,\n",
       "    0.0103029515594244,\n",
       "    -0.02522612176835537,\n",
       "    -0.010124723426997662,\n",
       "    0.0043288846500217915,\n",
       "    -0.008911401964724064,\n",
       "    -0.008438412100076675,\n",
       "    0.0073347692377865314,\n",
       "    0.05503133311867714,\n",
       "    -0.01377839781343937,\n",
       "    -0.09816251695156097,\n",
       "    -0.01896071992814541,\n",
       "    0.014792926609516144,\n",
       "    -0.005045224446803331,\n",
       "    0.017425216734409332,\n",
       "    -0.01359331514686346,\n",
       "    -0.06290078908205032,\n",
       "    0.015684066340327263,\n",
       "    0.03468591719865799,\n",
       "    0.013401377014815807,\n",
       "    0.022209955379366875,\n",
       "    -0.008486396633088589,\n",
       "    0.007053717039525509,\n",
       "    -0.0033075010869652033,\n",
       "    -0.004644211381673813,\n",
       "    0.0508909597992897,\n",
       "    0.03010682761669159,\n",
       "    -0.061200764030218124,\n",
       "    -0.005261154379695654,\n",
       "    -0.015615517273545265,\n",
       "    -0.010631987825036049,\n",
       "    0.016726015135645866,\n",
       "    0.027968091890215874,\n",
       "    0.024184172973036766,\n",
       "    -0.03145039454102516,\n",
       "    0.042856987565755844,\n",
       "    -0.010035608895123005,\n",
       "    0.01004931889474392,\n",
       "    0.036495618522167206,\n",
       "    0.008671479299664497,\n",
       "    -0.006183141842484474,\n",
       "    0.017205860465765,\n",
       "    -0.0038079104851931334,\n",
       "    0.048916738480329514,\n",
       "    -0.05239904299378395,\n",
       "    0.013435651548206806,\n",
       "    0.023896265774965286,\n",
       "    0.013545330613851547,\n",
       "    -0.008986805565655231,\n",
       "    0.03013424761593342,\n",
       "    0.014258242212235928,\n",
       "    0.037126269191503525,\n",
       "    0.0017205859767273068,\n",
       "    -0.018549425527453423,\n",
       "    0.03764724358916283,\n",
       "    0.061694320291280746,\n",
       "    -0.025911614298820496,\n",
       "    -0.02174381911754608,\n",
       "    0.007009160239249468,\n",
       "    0.01857684552669525,\n",
       "    -0.01547841913998127,\n",
       "    0.0006160862976685166,\n",
       "    -0.0041438015177845955,\n",
       "    0.025582578033208847,\n",
       "    0.015245351940393448,\n",
       "    0.001195327378809452,\n",
       "    0.0070331525057554245,\n",
       "    0.03419236093759537,\n",
       "    0.07540416717529297,\n",
       "    -0.019714761525392532,\n",
       "    0.003941581584513187,\n",
       "    -0.02034541592001915,\n",
       "    -0.005528496578335762,\n",
       "    0.01133804488927126,\n",
       "    0.008081955835223198,\n",
       "    0.022593829780817032,\n",
       "    0.003663957118988037,\n",
       "    0.04307634383440018,\n",
       "    0.022237375378608704,\n",
       "    0.030490703880786896,\n",
       "    0.012263459153473377,\n",
       "    -0.027639055624604225,\n",
       "    0.04729897901415825,\n",
       "    0.00041108124423772097,\n",
       "    -0.002651141956448555,\n",
       "    -0.03092941828072071,\n",
       "    0.03931984677910805,\n",
       "    0.02748824656009674,\n",
       "    0.05736200511455536,\n",
       "    0.029722951352596283,\n",
       "    0.0034600230865180492,\n",
       "    0.02201801724731922,\n",
       "    0.04236343130469322,\n",
       "    -0.08363007754087448,\n",
       "    0.010618277825415134,\n",
       "    -0.024430951103568077,\n",
       "    0.0005646743811666965,\n",
       "    -0.012798143550753593,\n",
       "    -0.024759987369179726,\n",
       "    -0.042253755033016205,\n",
       "    -0.03095683827996254,\n",
       "    0.042884405702352524,\n",
       "    0.034493979066610336,\n",
       "    -0.016575206071138382,\n",
       "    -0.03183426707983017,\n",
       "    0.002805377822369337,\n",
       "    -0.017562316730618477,\n",
       "    -0.028818102553486824,\n",
       "    0.03098425827920437,\n",
       "    -0.05642973631620407,\n",
       "    0.020112348720431328,\n",
       "    0.008767448365688324,\n",
       "    -0.024239012971520424,\n",
       "    0.021072037518024445,\n",
       "    -0.035947222262620926,\n",
       "    0.00845212209969759,\n",
       "    -0.07134605199098587,\n",
       "    -0.015272771008312702,\n",
       "    -0.024746276438236237,\n",
       "    -0.007670660503208637,\n",
       "    -0.020235735923051834,\n",
       "    -0.007245655171573162,\n",
       "    0.05845879390835762,\n",
       "    -0.0019656496588140726,\n",
       "    -0.0014121143613010645,\n",
       "    -0.03707142919301987,\n",
       "    0.054565198719501495,\n",
       "    -0.04176019877195358,\n",
       "    0.06838472187519073,\n",
       "    -0.014313082210719585,\n",
       "    0.034850433468818665,\n",
       "    0.01721956953406334,\n",
       "    0.02179865911602974,\n",
       "    -3.007062514370773e-05,\n",
       "    0.0025860201567411423,\n",
       "    -0.008774302899837494,\n",
       "    0.0018525433260947466,\n",
       "    -0.009103339165449142,\n",
       "    0.006261973176151514,\n",
       "    -0.030545542016625404,\n",
       "    0.05327647179365158,\n",
       "    0.026268070563673973,\n",
       "    -0.027364857494831085,\n",
       "    -0.02692614309489727,\n",
       "    -0.010967878624796867,\n",
       "    -0.016383269801735878,\n",
       "    -0.0017128741601482034,\n",
       "    -0.018974430859088898,\n",
       "    -0.012695319950580597,\n",
       "    -0.03210846707224846,\n",
       "    -0.007211380172520876,\n",
       "    0.020784130319952965,\n",
       "    0.007231945171952248,\n",
       "    -0.0020050653256475925,\n",
       "    0.007656950503587723,\n",
       "    0.06547823548316956,\n",
       "    -0.003958718851208687,\n",
       "    0.03726336732506752,\n",
       "    -0.01493002474308014,\n",
       "    0.0698653906583786,\n",
       "    -0.05725232884287834,\n",
       "    0.00014866616402287036,\n",
       "    0.011002153158187866,\n",
       "    -0.016191331669688225,\n",
       "    -0.03435688093304634,\n",
       "    -0.017918772995471954,\n",
       "    -0.03241008147597313,\n",
       "    -0.04620219022035599,\n",
       "    -0.012565076351165771,\n",
       "    0.009980769827961922,\n",
       "    -0.012427978217601776,\n",
       "    0.04074567183852196,\n",
       "    -0.026761624962091446,\n",
       "    0.012242894619703293,\n",
       "    0.039100486785173416,\n",
       "    -0.01743892766535282,\n",
       "    -0.03268427774310112,\n",
       "    0.048807062208652496,\n",
       "    0.04184245690703392,\n",
       "    -0.029174556955695152,\n",
       "    0.0051274835132062435,\n",
       "    0.01809700019657612,\n",
       "    0.04938287287950516,\n",
       "    -0.004082107450813055,\n",
       "    0.010940459556877613,\n",
       "    0.03411010280251503,\n",
       "    -0.0383327379822731,\n",
       "    -0.000787031021900475,\n",
       "    -0.019056688994169235,\n",
       "    0.041074708104133606,\n",
       "    -0.0013872652780264616,\n",
       "    0.0006272255559451878,\n",
       "    -0.02868100255727768,\n",
       "    0.014217113144695759,\n",
       "    0.00430146511644125,\n",
       "    -0.02607613243162632,\n",
       "    -0.04477636516094208,\n",
       "    -0.007848888635635376,\n",
       "    -0.02122284658253193,\n",
       "    0.016424398869276047,\n",
       "    0.012167491018772125,\n",
       "    -0.021305104717612267,\n",
       "    -0.028242288157343864,\n",
       "    -0.019097819924354553,\n",
       "    -0.019701052457094193,\n",
       "    0.005614182911813259,\n",
       "    0.01086505502462387,\n",
       "    -0.04666832461953163,\n",
       "    -0.026199521496891975,\n",
       "    -0.011715065687894821,\n",
       "    0.009370681829750538,\n",
       "    0.02437611110508442,\n",
       "    0.050315145403146744,\n",
       "    0.028434226289391518,\n",
       "    -0.022292213514447212,\n",
       "    -0.027666473761200905,\n",
       "    0.02463659830391407,\n",
       "    0.010707391425967216,\n",
       "    0.007835178636014462,\n",
       "    0.029914889484643936,\n",
       "    0.031121356412768364,\n",
       "    -0.02408820390701294,\n",
       "    -0.009733992628753185,\n",
       "    0.0383327379822731,\n",
       "    0.023978525772690773,\n",
       "    -0.010035608895123005,\n",
       "    -0.03232782334089279,\n",
       "    -0.0031943947542458773,\n",
       "    -0.013846946880221367,\n",
       "    0.016780855134129524,\n",
       "    -0.0034617367200553417,\n",
       "    -0.025939034298062325,\n",
       "    0.014217113144695759,\n",
       "    -0.02870842255651951,\n",
       "    -0.02433498203754425,\n",
       "    0.01604052260518074,\n",
       "    -0.002860217122361064,\n",
       "    -0.036495618522167206,\n",
       "    -0.016109071671962738,\n",
       "    0.025130152702331543,\n",
       "    -0.037427887320518494,\n",
       "    -0.012859838083386421,\n",
       "    -0.012948951683938503,\n",
       "    -0.023416422307491302,\n",
       "    0.026528557762503624,\n",
       "    0.02410191297531128,\n",
       "    0.0038147654850035906,\n",
       "    -0.005422245245426893,\n",
       "    0.04647638648748398,\n",
       "    -0.01019327249377966,\n",
       "    -0.003770208451896906,\n",
       "    -0.01606794260442257,\n",
       "    -0.053495828062295914,\n",
       "    -0.006015195976942778,\n",
       "    -0.04622960835695267,\n",
       "    0.013655009679496288,\n",
       "    -0.008177924901247025,\n",
       "    0.004664775915443897,\n",
       "    -0.012112651020288467,\n",
       "    0.00817106943577528,\n",
       "    0.044008612632751465,\n",
       "    0.0036605296190828085,\n",
       "    -0.016273589804768562,\n",
       "    0.028790682554244995,\n",
       "    0.0017291546100750566,\n",
       "    -0.00905535463243723,\n",
       "    0.013106615282595158,\n",
       "    -0.010967878624796867,\n",
       "    0.013977190479636192,\n",
       "    -0.0012604491785168648,\n",
       "    -0.010419485159218311,\n",
       "    0.01116667129099369,\n",
       "    -0.0032201006542891264,\n",
       "    0.0026597105897963047,\n",
       "    0.0137372687458992,\n",
       "    0.03504237160086632,\n",
       "    -0.012551366351544857,\n",
       "    0.01375097781419754,\n",
       "    -0.00901422556489706,\n",
       "    0.01658891700208187,\n",
       "    -0.011954988352954388,\n",
       "    -0.014148564077913761,\n",
       "    0.005045224446803331,\n",
       "    -0.0520700067281723,\n",
       "    -0.03435688093304634,\n",
       "    0.011865873821079731,\n",
       "    -0.015656646341085434,\n",
       "    -0.0010462328791618347,\n",
       "    0.025431768968701363,\n",
       "    0.014532439410686493,\n",
       "    -0.03386332467198372,\n",
       "    0.008685189299285412,\n",
       "    -0.0034051835536956787,\n",
       "    0.00816421490162611,\n",
       "    0.04321344196796417,\n",
       "    -0.005607327912002802,\n",
       "    0.005535351578146219,\n",
       "    -6.115208179835463e-06,\n",
       "    -0.01835748739540577,\n",
       "    0.012852982617914677,\n",
       "    -0.02781728282570839,\n",
       "    -0.0062756831757724285,\n",
       "    0.04261020943522453,\n",
       "    -0.006364797241985798,\n",
       "    0.009877946227788925,\n",
       "    -0.00860978476703167,\n",
       "    -0.0007960280636325479,\n",
       "    -0.013463071547448635,\n",
       "    -0.0011370605789124966,\n",
       "    -0.025363219901919365,\n",
       "    -0.0007878878386691213,\n",
       "    0.019303467124700546,\n",
       "    0.0019930691923946142,\n",
       "    -0.013373957015573978,\n",
       "    0.0371536910533905,\n",
       "    -0.021414782851934433,\n",
       "    0.003209818387404084,\n",
       "    0.04850544407963753,\n",
       "    -0.004798447247594595,\n",
       "    -0.002963040955364704,\n",
       "    -0.02724146842956543,\n",
       "    -0.09004628658294678,\n",
       "    0.005298856645822525,\n",
       "    0.03441172093153,\n",
       "    0.02898262068629265,\n",
       "    0.018974430859088898,\n",
       "    0.014175983145833015,\n",
       "    0.0058609601110219955,\n",
       "    0.027625344693660736,\n",
       "    0.05708780884742737,\n",
       "    -0.039648883044719696,\n",
       "    -0.010837635025382042,\n",
       "    -0.007951712235808372,\n",
       "    0.015025993809103966,\n",
       "    -0.013367102481424809,\n",
       "    0.01578003540635109,\n",
       "    0.025966452434659004,\n",
       "    0.0034977251198142767,\n",
       "    0.03704401105642319,\n",
       "    -0.013147745281457901,\n",
       "    -0.0634491816163063,\n",
       "    0.0012835846282541752,\n",
       "    0.02492450550198555,\n",
       "    -0.002702553989365697,\n",
       "    0.03145039454102516,\n",
       "    0.004195213783532381,\n",
       "    0.006063180509954691,\n",
       "    -0.031943947076797485,\n",
       "    -0.00608031777665019,\n",
       "    0.0010693682124838233,\n",
       "    -0.027693893760442734,\n",
       "    -0.005559343844652176,\n",
       "    -0.058349113911390305,\n",
       "    -0.03243750333786011,\n",
       "    -0.010097303427755833,\n",
       "    -0.027721313759684563,\n",
       "    0.02061961218714714,\n",
       "    -0.005117200780659914,\n",
       "    -0.006381934508681297,\n",
       "    0.006323667708784342,\n",
       "    0.01519051194190979,\n",
       "    -0.03517946973443031,\n",
       "    -0.021442202851176262,\n",
       "    -0.044282812625169754,\n",
       "    0.027159210294485092,\n",
       "    0.014463890343904495,\n",
       "    -0.013360247015953064,\n",
       "    0.008267038501799107,\n",
       "    0.006649276707321405,\n",
       "    -0.003732506185770035,\n",
       "    0.004500257782638073,\n",
       "    -0.016712306067347527,\n",
       "    -0.027735022827982903,\n",
       "    0.015053413808345795,\n",
       "    -0.024472080171108246,\n",
       "    -0.017987322062253952,\n",
       "    -0.0055901906453073025,\n",
       "    -0.012105796486139297,\n",
       "    0.05217968299984932,\n",
       "    -0.007732354570180178,\n",
       "    0.008123084902763367,\n",
       "    -0.006419636774808168,\n",
       "    0.002068473491817713,\n",
       "    -0.001169621478766203,\n",
       "    -0.04241827130317688,\n",
       "    0.006981740240007639,\n",
       "    0.015670357272028923,\n",
       "    0.008842851966619492,\n",
       "    -0.0021370225585997105,\n",
       "    -0.0280640609562397,\n",
       "    -0.03158749267458916,\n",
       "    -0.004986957646906376,\n",
       "    -0.015944553539156914,\n",
       "    -0.037702083587646484,\n",
       "    0.017329249531030655,\n",
       "    0.009528344497084618,\n",
       "    0.005922654643654823,\n",
       "    0.060871727764606476,\n",
       "    0.006491613108664751,\n",
       "    -0.03298589587211609,\n",
       "    -0.02092122845351696,\n",
       "    0.04066340997815132,\n",
       "    0.03534398972988129,\n",
       "    0.0052885739132761955,\n",
       "    0.014422760345041752,\n",
       "    -0.00375992595218122,\n",
       "    -0.03260201960802078,\n",
       "    -0.018439745530486107,\n",
       "    -0.032848797738552094,\n",
       "    0.011790470220148563,\n",
       "    -0.00681379484012723,\n",
       "    -0.006567017640918493,\n",
       "    -0.046860262751579285,\n",
       "    0.024828536435961723,\n",
       "    0.0371536910533905,\n",
       "    -0.0015723482938483357,\n",
       "    0.04365215823054314,\n",
       "    0.0338907465338707,\n",
       "    0.02603500336408615,\n",
       "    0.02517128176987171,\n",
       "    -0.017685703933238983,\n",
       "    0.01231144368648529,\n",
       "    0.001364129944704473,\n",
       "    0.0065738726407289505,\n",
       "    -0.011872729286551476,\n",
       "    -0.017288118600845337,\n",
       "    0.02009863778948784,\n",
       "    0.006666413974016905,\n",
       "    0.002496906090527773,\n",
       "    -0.0028499348554760218,\n",
       "    0.015272771008312702,\n",
       "    0.034219782799482346,\n",
       "    -0.0012330295285210013,\n",
       "    0.011763050220906734,\n",
       "    -0.01863168366253376,\n",
       "    0.005980921443551779,\n",
       "    -0.015930844470858574,\n",
       "    -0.013264278881251812,\n",
       "    0.03548108786344528,\n",
       "    -0.01580745540559292,\n",
       "    -0.018426036462187767,\n",
       "    0.007471867371350527,\n",
       "    -0.019824441522359848,\n",
       "    0.025637416169047356,\n",
       "    0.004990384913980961,\n",
       "    -0.009377536363899708,\n",
       "    0.020208317786455154,\n",
       "    -6.517533620353788e-05,\n",
       "    0.0055867633782327175,\n",
       "    -0.012709029950201511,\n",
       "    0.013271133415400982,\n",
       "    -0.005202887579798698,\n",
       "    -0.0011464861454442143,\n",
       "    -0.047161880880594254,\n",
       "    -0.023430131375789642,\n",
       "    0.020578483119606972,\n",
       "    -0.026309199631214142,\n",
       "    -0.004119809716939926,\n",
       "    -0.03147781267762184,\n",
       "    0.0046682036481797695,\n",
       "    0.00857551023364067,\n",
       "    0.010522308759391308,\n",
       "    -0.015259061940014362,\n",
       "    0.006649276707321405,\n",
       "    -0.01288725808262825,\n",
       "    0.017617154866456985,\n",
       "    0.03199878707528114,\n",
       "    -0.07754290103912354,\n",
       "    -0.0035748430527746677,\n",
       "    0.01630100980401039,\n",
       "    0.0076089659705758095,\n",
       "    0.03098425827920437,\n",
       "    0.008081955835223198,\n",
       "    0.02404707483947277,\n",
       "    -0.00537426071241498,\n",
       "    0.052590981125831604,\n",
       "    -0.003396614920347929,\n",
       "    -0.01630100980401039,\n",
       "    -0.01922120712697506,\n",
       "    -0.015409870073199272,\n",
       "    0.012681609950959682,\n",
       "    0.0251849927008152,\n",
       "    -0.032793957740068436,\n",
       "    -0.016520367935299873,\n",
       "    0.007547271437942982,\n",
       "    0.004298037383705378,\n",
       "    -0.002574024023488164,\n",
       "    -0.030189085751771927,\n",
       "    -0.020208317786455154,\n",
       "    -0.02316964417695999,\n",
       "    0.03789402171969414,\n",
       "    0.017644574865698814,\n",
       "    0.0045962268486619,\n",
       "    -0.007588401436805725,\n",
       "    0.009171889163553715,\n",
       "    0.021126877516508102,\n",
       "    0.031395554542541504,\n",
       "    -0.028434226289391518,\n",
       "    0.012688464485108852,\n",
       "    0.009295277297496796,\n",
       "    -0.007780339103192091,\n",
       "    0.013607025146484375,\n",
       "    -0.017987322062253952,\n",
       "    0.0025363219901919365,\n",
       "    0.018151840195059776,\n",
       "    -0.02890036068856716,\n",
       "    -0.008088810369372368,\n",
       "    -0.02234705351293087,\n",
       "    -0.023265613242983818,\n",
       "    0.02093493938446045,\n",
       "    -0.0008311595884151757,\n",
       "    0.022799478843808174,\n",
       "    -0.021044617518782616,\n",
       "    -0.01059771329164505,\n",
       "    -0.013175164349377155,\n",
       "    -0.008150504902005196,\n",
       "    -0.02322448417544365,\n",
       "    -0.02691243216395378,\n",
       "    -0.004866996314376593,\n",
       "    0.0137372687458992,\n",
       "    -0.008973095566034317,\n",
       "    0.010069883428514004,\n",
       "    -0.003866177285090089,\n",
       "    -0.02088009938597679,\n",
       "    0.001915951375849545,\n",
       "    -0.011488853022456169,\n",
       "    0.010947314091026783,\n",
       "    0.001292153261601925,\n",
       "    -0.016259880736470222,\n",
       "    -0.04346022009849548,\n",
       "    -0.008788012899458408,\n",
       "    -0.022278504446148872,\n",
       "    0.004692195914685726,\n",
       "    -0.017260698601603508,\n",
       "    0.02323819324374199,\n",
       "    -0.0503699854016304,\n",
       "    0.045187659561634064,\n",
       "    -0.006484758574515581,\n",
       "    0.0019296611426398158,\n",
       "    0.0077940491028130054,\n",
       "    0.030847160145640373,\n",
       "    0.0009922502795234323,\n",
       "    0.005768418777734041,\n",
       "    -0.007279929704964161,\n",
       "    0.03814079985022545,\n",
       "    0.02092122845351696,\n",
       "    -0.00871946383267641,\n",
       "    0.029942309483885765,\n",
       "    0.020413964986801147,\n",
       "    -0.01116667129099369,\n",
       "    0.02040025405585766,\n",
       "    0.0177268348634243,\n",
       "    0.007855743169784546,\n",
       "    0.025020474568009377,\n",
       "    -0.027282599359750748,\n",
       "    0.021976888179779053,\n",
       "    0.020551063120365143,\n",
       "    0.05525068938732147,\n",
       "    0.01131062489002943,\n",
       "    -0.02035912498831749,\n",
       "    0.018686523661017418,\n",
       "    0.020688161253929138,\n",
       "    0.0027556796558201313,\n",
       "    -0.021620431914925575,\n",
       "    0.02153817191720009,\n",
       "    -0.0063887895084917545,\n",
       "    0.0025568867567926645,\n",
       "    0.020167186856269836,\n",
       "    0.009713428094983101,\n",
       "    0.016273589804768562,\n",
       "    -0.004993812181055546,\n",
       "    0.032766539603471756,\n",
       "    0.004051260184496641,\n",
       "    0.03613916039466858,\n",
       "    -0.003089857054874301,\n",
       "    -0.013812672346830368,\n",
       "    0.026802754029631615,\n",
       "    0.03065522201359272,\n",
       "    -0.027296308428049088,\n",
       "    0.010344080626964569,\n",
       "    0.05196032673120499,\n",
       "    0.0035679880529642105,\n",
       "    0.042829565703868866,\n",
       "    0.022182535380125046,\n",
       "    0.05152161046862602,\n",
       "    0.06487500667572021,\n",
       "    0.006988595239818096,\n",
       "    0.03556334599852562,\n",
       "    0.007355333771556616,\n",
       "    -0.046558644622564316,\n",
       "    0.013367102481424809,\n",
       "    -0.03759240359067917,\n",
       "    0.0032355242874473333,\n",
       "    0.012503381818532944,\n",
       "    4.793091648025438e-06,\n",
       "    0.014217113144695759,\n",
       "    0.006686978507786989,\n",
       "    0.036495618522167206,\n",
       "    0.029174556955695152,\n",
       "    -0.02689872309565544,\n",
       "    0.017397798597812653,\n",
       "    -0.03698917105793953,\n",
       "    -0.02807777002453804,\n",
       "    0.02832454815506935,\n",
       "    0.0637233778834343,\n",
       "    0.009494069963693619,\n",
       "    0.02955843321979046,\n",
       "    0.009733992628753185,\n",
       "    -0.016479238867759705,\n",
       "    -9.619654520065524e-06,\n",
       "    -0.03693433105945587,\n",
       "    0.04546185955405235,\n",
       "    0.015876004472374916,\n",
       "    -0.006556734908372164,\n",
       "    -0.020126057788729668,\n",
       "    -0.002651141956448555,\n",
       "    -0.005004094913601875,\n",
       "    0.05423616245388985,\n",
       "    -0.0011481997789815068,\n",
       "    -0.005110346246510744,\n",
       "    0.02892778068780899,\n",
       "    -0.013559040613472462,\n",
       "    -0.02496563456952572,\n",
       "    -0.02033170498907566,\n",
       "    -0.029585853219032288,\n",
       "    0.05209742486476898,\n",
       "    0.00041815039003267884,\n",
       "    0.01948169432580471,\n",
       "    0.023909976705908775,\n",
       "    -0.043981194496154785,\n",
       "    -0.020180897787213326,\n",
       "    0.009007370099425316,\n",
       "    -0.023622069507837296,\n",
       "    0.027639055624604225,\n",
       "    -0.024266432970762253,\n",
       "    0.012688464485108852,\n",
       "    0.0055867633782327175,\n",
       "    0.020112348720431328,\n",
       "    -0.010172707960009575,\n",
       "    -0.005768418777734041,\n",
       "    0.007780339103192091,\n",
       "    0.05138451233506203,\n",
       "    0.019070399925112724,\n",
       "    0.03213588520884514,\n",
       "    0.026555975899100304,\n",
       "    0.0624072290956974,\n",
       "    0.000211967111681588,\n",
       "    0.008506961166858673,\n",
       "    0.030490703880786896,\n",
       "    -0.02922939695417881,\n",
       "    0.02434869110584259,\n",
       "    0.01743892766535282,\n",
       "    0.04088276997208595,\n",
       "    -0.013970335945487022,\n",
       "    0.004267190583050251,\n",
       "    0.0012852982617914677,\n",
       "    0.039374686777591705,\n",
       "    0.007807758636772633,\n",
       "    0.04614735022187233,\n",
       "    -0.004198641050606966,\n",
       "    0.052316781133413315,\n",
       "    -0.0030949984211474657,\n",
       "    -0.038250476121902466,\n",
       "    0.005322848912328482,\n",
       "    0.03487785533070564,\n",
       "    0.033643968403339386,\n",
       "    -0.03265685960650444,\n",
       "    -0.05105547606945038,\n",
       "    0.02089380845427513,\n",
       "    0.01330540794879198,\n",
       "    ...]}],\n",
       " 'retrieval_model': [{'contexts': [\"Q: What steps do you take to ensure the transparency and explainability of your AI systems' decisions?\\nA: We prioritize transparency by incorporating explainability features into our AI models, providing detailed documentation on the decision-making processes, and ensuring that stakeholders can understand and trust the outputs of our AI systems. To achieve this, we integrate explainability tools like feature importance scores and decision trees that clearly outline how and why decisions are made by our AI. We supplement these technical tools with comprehensive documentation that describes the algorithms' functions in accessible language. This approach is designed to demystify the AI's operations for non-technical stakeholders, fostering a higher level of trust and acceptance. By ensuring that our AI systems are transparent and their workings understandable, we maintain open communication and build confidence among users and regulators alike.\\n\",\n",
       "    'Q: How do you maintain your AI applications with the newest AI technologies and advancements?\\nA: We maintain a dedicated R&D team focused on integrating the latest AI advancements into our applications. This includes regular updates and feature enhancements based on cutting-edge technologies such as GPT (Generative Pre-trained Transformer) for natural language understanding, CNNs (Convolutional Neural Networks) for advanced image recognition tasks, and DQN (Deep Q-Networks) for decision-making processes in complex environments. Our commitment to these AI methodologies ensures that our applications remain innovative, with capabilities that adapt to evolving market demands and client needs. This approach has enabled us to enhance the predictive accuracy of our financial forecasting tools by 25% and improve the efficiency of our educational content personalization by 40%\\n',\n",
       "    'Q: What considerations do you take into account for user interface and user experience design in your AI applications?\\nA: Our design philosophy centers on simplicity and intuitiveness. We conduct extensive user research and testing to inform our UI/UX designs, ensuring that our AI-based apps are accessible and engaging for all users, regardless of their technical expertise. This includes applying principles from human-centered design, utilizing accessibility guidelines such as WCAG 2.1, and conducting iterative testing with diverse user groups. Our commitment to inclusivity and usability leads to higher user adoption rates and satisfaction. For instance, feedback-driven enhancements in our visual design have improved user engagement by over 30% across our applications.\\n',\n",
       "    'Q: What support and maintenance do you provide following the deployment of AI applications?\\nA: Post-launch, we offer comprehensive support and maintenance services, including regular updates, bug fixes, and performance optimization. Our support team is available 24/7 to assist with any issues or questions. We utilize a ticketing system that ensures swift response times, with an average initial response time of under 2 hours. Additionally, we provide monthly performance reports and hold quarterly reviews with clients to discuss system status and potential improvements. Our proactive approach includes using automated monitoring tools to detect and resolve issues before they impact users, ensuring that our applications perform optimally at all times. This service structure has been instrumental in maintaining a client satisfaction rate above 98%.\\n',\n",
       "    'Q: How does your AI strategy align with the NIST AI RMF for trustworthy and responsible use of AI?\\nA: Our AI solution is meticulously designed to align with the NIST AI Risk Management Framework (RMF) guidelines, ensuring adherence to principles of trustworthiness and responsibility. We have implemented comprehensive governance structures that oversee the ethical development and deployment of our AI technologies. This includes risk identification and assessment processes where potential risks are analyzed and categorized at each stage of the AI lifecycle. To manage these risks, we have instituted robust risk management controls that are deeply integrated into our development and operational processes. These controls are based on the NIST frameworks best practices, ensuring that our AI solutions are not only effective but also secure and ethical, maintaining transparency and accountability at all times.\\n',\n",
       "    'Q: Can your AI applications be tailored to meet unique organizational or user-specific requirements?\\nA: Absolutely, customization is a core aspect of our offering. We work closely with clients to understand their specific needs and tailor our AI algorithms and app functionalities accordingly, using technologies such as TensorFlow for machine learning models, React for responsive UI/UX designs, and Kubernetes for scalable cloud deployment. This personalized approach allows us to optimize AI functionalities to match unique business processes, enhancing user experience and operational efficiency for each client. For example, for a retail client, we customized our recommendation engine to increase customer retention by 20% through more accurate and personalized product suggestions.\\n',\n",
       "    'Q: Can you discuss your governance framework for managing AI risks in accordance with the NIST AI RMF?\\nA: We have established an AI Risk Council that plays a pivotal role in overseeing AI risk management across our organization. This council is tasked with defining clear roles and responsibilities for AI governance, ensuring that there is a structured approach to managing AI risks. It also integrates AI risk management into our existing governance frameworks to enhance coherence and alignment with broader corporate policies and objectives. Additionally, the AI Risk Council promotes robust collaboration between various business units and our IT department. This collaboration is crucial for sharing insights, aligning strategies, and implementing comprehensive risk management practices effectively across the entire organization. This framework not only supports proactive risk management but also fosters an environment where AI technologies are used responsibly and ethically.\\n',\n",
       "    \"Q: How do you evaluate the success of your AI applications in fulfilling client goals?\\nA: Success measurement is tailored to each project's objectives. We establish key performance indicators (KPIs) in collaboration with our clients, such as user engagement rates, efficiency improvements, or return on investment (ROI). We then regularly review these metrics using advanced analytics platforms and business intelligence tools to assess the apps impact. Our approach includes monthly performance analysis meetings where we provide detailed reports and insights on metrics like session duration, user retention rates, and cost savings achieved through automation. We also implement A/B testing to continuously refine and optimize the application based on real-world usage data, ensuring that we make data-driven improvements that align closely with our clients' strategic goals.\\n\",\n",
       "    'Q: Please share your experience with developing AI-enabled applications and provide examples of notable projects.\\nA: Our company has 15 years of experience in developing AI-based applications, with a strong portfolio in sectors such as healthcare, finance, and education. For instance, our project MediAI Insight for the healthcare industry demonstrated significant achievements in patient data analysis, resulting in a 30% reduction in diagnostic errors and a 40% improvement in treatment personalization. Our platform has engaged over 200 healthcare facilities, achieving a user satisfaction rate of 95%.\\n',\n",
       "    'Q: How are ethical issues like bias detection and data privacy managed in your LLM applications?\\nA: We adhere to ethical AI practices by implementing bias detection and mitigation techniques during the training of our Large Language Models (LLMs). This involves using diverse datasets to prevent skewed results and deploying algorithms specifically designed to identify and correct bias in AI outputs. For data privacy, we employ data anonymization and secure data handling protocols, ensuring compliance with GDPR, CCPA, and other relevant regulations. Our systems use state-of-the-art encryption methods for data at rest and in transit, and our data governance policies are rigorously audited by third-party security firms to maintain high standards of data integrity and confidentiality. This commitment extends to regular training for our staff on the latest privacy laws and ethical AI use to ensure that our practices are up-to-date and effective.\\n']},\n",
       "  {'contexts': [\"Q: Please outline the training process for your LLMs, including choices regarding data, models, and validation approaches.\\nA: Our LLM training process begins with the meticulous sourcing of diverse and comprehensive datasets from global sources, ensuring a rich variety that includes various languages, dialects, and cultural contexts. This diversity is critical for building models that perform equitably across different demographics. We leverage cutting-edge tools like Apache Kafka for real-time data streaming and Apache Hadoop for handling large datasets efficiently during preprocessing stages. For model architecture selection, we utilize TensorFlow and PyTorch frameworks to design and iterate on neural network structures that best suit each application's unique requirements, whether it's for predictive analytics in finance or customer service chatbots. Depending on the use case, we might choose from a variety of architectures such as Transformer models for their robust handling of sequential data or GANs (Generative Adversarial Networks) for generating new, synthetic data samples for training.\\n\",\n",
       "    \"Q: What measures do you employ to ensure your LLMs' decisions are clear and explicable?\\nA: We prioritize transparency and explainability in our AI models by incorporating advanced features such as model interpretability layers and providing comprehensive documentation on how model decisions are made. This approach ensures that users can understand and trust the outputs of our Large Language Models (LLMs). To achieve this, we integrate tools like LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) into our models. These tools allow us to break down and communicate the reasoning behind each model decision, fostering trust and facilitating easier audits by stakeholders.\\n\",\n",
       "    \"Q: Describe your strategy for integrating LLMs into existing organizational frameworks and systems.\\nA: Our approach involves conducting a thorough analysis of the existing systems and workflows, designing integration plans that minimize disruption, and using APIs and custom connectors to ensure seamless integration of our LLM-based applications. We start by meticulously mapping the client's current infrastructure and operational flows to identify the most efficient points of integration. This is followed by the development of tailored integration plans that prioritize operational continuity and minimize downtime. To achieve seamless integration, we utilize robust APIs and develop custom connectors where necessary, ensuring compatibility with existing software platforms and databases. These tools allow for the smooth transfer of data and maintain the integrity and security of the system, ensuring that the new AI capabilities enhance functionality without compromising existing processes.\\n\",\n",
       "    'Q: How are ethical issues like bias detection and data privacy managed in your LLM applications?\\nA: We adhere to ethical AI practices by implementing bias detection and mitigation techniques during the training of our Large Language Models (LLMs). This involves using diverse datasets to prevent skewed results and deploying algorithms specifically designed to identify and correct bias in AI outputs. For data privacy, we employ data anonymization and secure data handling protocols, ensuring compliance with GDPR, CCPA, and other relevant regulations. Our systems use state-of-the-art encryption methods for data at rest and in transit, and our data governance policies are rigorously audited by third-party security firms to maintain high standards of data integrity and confidentiality. This commitment extends to regular training for our staff on the latest privacy laws and ethical AI use to ensure that our practices are up-to-date and effective.\\n',\n",
       "    \"Q: Could you provide case studies on successful LLM-based application deployments, focusing on challenges and solutions?\\nA: We can share case studies of successful LLM-based application deployments, highlighting specific challenges such as data scarcity or model interpretability, and detailing the strategies and solutions we implemented to overcome these challenges. For example, in a project involving natural language processing for a legal firm, we faced significant data scarcity. To address this, we employed techniques like synthetic data generation and transfer learning from related domains to enrich our training datasets. Additionally, the issue of model interpretability was critical for our clients trust and regulatory compliance. We tackled this by integrating SHAP (SHapley Additive exPlanations) to provide clear, understandable insights into how our model's decisions were made, ensuring transparency and boosting user confidence in the AI system.\\n\",\n",
       "    'Q: How do you maintain your AI applications with the newest AI technologies and advancements?\\nA: We maintain a dedicated R&D team focused on integrating the latest AI advancements into our applications. This includes regular updates and feature enhancements based on cutting-edge technologies such as GPT (Generative Pre-trained Transformer) for natural language understanding, CNNs (Convolutional Neural Networks) for advanced image recognition tasks, and DQN (Deep Q-Networks) for decision-making processes in complex environments. Our commitment to these AI methodologies ensures that our applications remain innovative, with capabilities that adapt to evolving market demands and client needs. This approach has enabled us to enhance the predictive accuracy of our financial forecasting tools by 25% and improve the efficiency of our educational content personalization by 40%\\n',\n",
       "    'Q: Can your AI applications be tailored to meet unique organizational or user-specific requirements?\\nA: Absolutely, customization is a core aspect of our offering. We work closely with clients to understand their specific needs and tailor our AI algorithms and app functionalities accordingly, using technologies such as TensorFlow for machine learning models, React for responsive UI/UX designs, and Kubernetes for scalable cloud deployment. This personalized approach allows us to optimize AI functionalities to match unique business processes, enhancing user experience and operational efficiency for each client. For example, for a retail client, we customized our recommendation engine to increase customer retention by 20% through more accurate and personalized product suggestions.\\n',\n",
       "    'Q: What considerations do you take into account for user interface and user experience design in your AI applications?\\nA: Our design philosophy centers on simplicity and intuitiveness. We conduct extensive user research and testing to inform our UI/UX designs, ensuring that our AI-based apps are accessible and engaging for all users, regardless of their technical expertise. This includes applying principles from human-centered design, utilizing accessibility guidelines such as WCAG 2.1, and conducting iterative testing with diverse user groups. Our commitment to inclusivity and usability leads to higher user adoption rates and satisfaction. For instance, feedback-driven enhancements in our visual design have improved user engagement by over 30% across our applications.\\n',\n",
       "    \"Q: How do you evaluate the success of your AI applications in fulfilling client goals?\\nA: Success measurement is tailored to each project's objectives. We establish key performance indicators (KPIs) in collaboration with our clients, such as user engagement rates, efficiency improvements, or return on investment (ROI). We then regularly review these metrics using advanced analytics platforms and business intelligence tools to assess the apps impact. Our approach includes monthly performance analysis meetings where we provide detailed reports and insights on metrics like session duration, user retention rates, and cost savings achieved through automation. We also implement A/B testing to continuously refine and optimize the application based on real-world usage data, ensuring that we make data-driven improvements that align closely with our clients' strategic goals.\\n\",\n",
       "    \"Q: What steps do you take to ensure the transparency and explainability of your AI systems' decisions?\\nA: We prioritize transparency by incorporating explainability features into our AI models, providing detailed documentation on the decision-making processes, and ensuring that stakeholders can understand and trust the outputs of our AI systems. To achieve this, we integrate explainability tools like feature importance scores and decision trees that clearly outline how and why decisions are made by our AI. We supplement these technical tools with comprehensive documentation that describes the algorithms' functions in accessible language. This approach is designed to demystify the AI's operations for non-technical stakeholders, fostering a higher level of trust and acceptance. By ensuring that our AI systems are transparent and their workings understandable, we maintain open communication and build confidence among users and regulators alike.\\n\"]},\n",
       "  {'contexts': [\"Q: Describe your strategy for integrating LLMs into existing organizational frameworks and systems.\\nA: Our approach involves conducting a thorough analysis of the existing systems and workflows, designing integration plans that minimize disruption, and using APIs and custom connectors to ensure seamless integration of our LLM-based applications. We start by meticulously mapping the client's current infrastructure and operational flows to identify the most efficient points of integration. This is followed by the development of tailored integration plans that prioritize operational continuity and minimize downtime. To achieve seamless integration, we utilize robust APIs and develop custom connectors where necessary, ensuring compatibility with existing software platforms and databases. These tools allow for the smooth transfer of data and maintain the integrity and security of the system, ensuring that the new AI capabilities enhance functionality without compromising existing processes.\\n\",\n",
       "    \"Q: What measures do you employ to ensure your LLMs' decisions are clear and explicable?\\nA: We prioritize transparency and explainability in our AI models by incorporating advanced features such as model interpretability layers and providing comprehensive documentation on how model decisions are made. This approach ensures that users can understand and trust the outputs of our Large Language Models (LLMs). To achieve this, we integrate tools like LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) into our models. These tools allow us to break down and communicate the reasoning behind each model decision, fostering trust and facilitating easier audits by stakeholders.\\n\",\n",
       "    \"Q: Could you provide case studies on successful LLM-based application deployments, focusing on challenges and solutions?\\nA: We can share case studies of successful LLM-based application deployments, highlighting specific challenges such as data scarcity or model interpretability, and detailing the strategies and solutions we implemented to overcome these challenges. For example, in a project involving natural language processing for a legal firm, we faced significant data scarcity. To address this, we employed techniques like synthetic data generation and transfer learning from related domains to enrich our training datasets. Additionally, the issue of model interpretability was critical for our clients trust and regulatory compliance. We tackled this by integrating SHAP (SHapley Additive exPlanations) to provide clear, understandable insights into how our model's decisions were made, ensuring transparency and boosting user confidence in the AI system.\\n\",\n",
       "    \"Q: Please outline the training process for your LLMs, including choices regarding data, models, and validation approaches.\\nA: Our LLM training process begins with the meticulous sourcing of diverse and comprehensive datasets from global sources, ensuring a rich variety that includes various languages, dialects, and cultural contexts. This diversity is critical for building models that perform equitably across different demographics. We leverage cutting-edge tools like Apache Kafka for real-time data streaming and Apache Hadoop for handling large datasets efficiently during preprocessing stages. For model architecture selection, we utilize TensorFlow and PyTorch frameworks to design and iterate on neural network structures that best suit each application's unique requirements, whether it's for predictive analytics in finance or customer service chatbots. Depending on the use case, we might choose from a variety of architectures such as Transformer models for their robust handling of sequential data or GANs (Generative Adversarial Networks) for generating new, synthetic data samples for training.\\n\",\n",
       "    'Q: How are ethical issues like bias detection and data privacy managed in your LLM applications?\\nA: We adhere to ethical AI practices by implementing bias detection and mitigation techniques during the training of our Large Language Models (LLMs). This involves using diverse datasets to prevent skewed results and deploying algorithms specifically designed to identify and correct bias in AI outputs. For data privacy, we employ data anonymization and secure data handling protocols, ensuring compliance with GDPR, CCPA, and other relevant regulations. Our systems use state-of-the-art encryption methods for data at rest and in transit, and our data governance policies are rigorously audited by third-party security firms to maintain high standards of data integrity and confidentiality. This commitment extends to regular training for our staff on the latest privacy laws and ethical AI use to ensure that our practices are up-to-date and effective.\\n',\n",
       "    'Q: How do you maintain your AI applications with the newest AI technologies and advancements?\\nA: We maintain a dedicated R&D team focused on integrating the latest AI advancements into our applications. This includes regular updates and feature enhancements based on cutting-edge technologies such as GPT (Generative Pre-trained Transformer) for natural language understanding, CNNs (Convolutional Neural Networks) for advanced image recognition tasks, and DQN (Deep Q-Networks) for decision-making processes in complex environments. Our commitment to these AI methodologies ensures that our applications remain innovative, with capabilities that adapt to evolving market demands and client needs. This approach has enabled us to enhance the predictive accuracy of our financial forecasting tools by 25% and improve the efficiency of our educational content personalization by 40%\\n',\n",
       "    'Q: Can your AI applications be tailored to meet unique organizational or user-specific requirements?\\nA: Absolutely, customization is a core aspect of our offering. We work closely with clients to understand their specific needs and tailor our AI algorithms and app functionalities accordingly, using technologies such as TensorFlow for machine learning models, React for responsive UI/UX designs, and Kubernetes for scalable cloud deployment. This personalized approach allows us to optimize AI functionalities to match unique business processes, enhancing user experience and operational efficiency for each client. For example, for a retail client, we customized our recommendation engine to increase customer retention by 20% through more accurate and personalized product suggestions.\\n',\n",
       "    \"Q: How do you evaluate the success of your AI applications in fulfilling client goals?\\nA: Success measurement is tailored to each project's objectives. We establish key performance indicators (KPIs) in collaboration with our clients, such as user engagement rates, efficiency improvements, or return on investment (ROI). We then regularly review these metrics using advanced analytics platforms and business intelligence tools to assess the apps impact. Our approach includes monthly performance analysis meetings where we provide detailed reports and insights on metrics like session duration, user retention rates, and cost savings achieved through automation. We also implement A/B testing to continuously refine and optimize the application based on real-world usage data, ensuring that we make data-driven improvements that align closely with our clients' strategic goals.\\n\",\n",
       "    'Q: What support and maintenance do you provide following the deployment of AI applications?\\nA: Post-launch, we offer comprehensive support and maintenance services, including regular updates, bug fixes, and performance optimization. Our support team is available 24/7 to assist with any issues or questions. We utilize a ticketing system that ensures swift response times, with an average initial response time of under 2 hours. Additionally, we provide monthly performance reports and hold quarterly reviews with clients to discuss system status and potential improvements. Our proactive approach includes using automated monitoring tools to detect and resolve issues before they impact users, ensuring that our applications perform optimally at all times. This service structure has been instrumental in maintaining a client satisfaction rate above 98%.\\n',\n",
       "    \"Q: What steps do you take to ensure the transparency and explainability of your AI systems' decisions?\\nA: We prioritize transparency by incorporating explainability features into our AI models, providing detailed documentation on the decision-making processes, and ensuring that stakeholders can understand and trust the outputs of our AI systems. To achieve this, we integrate explainability tools like feature importance scores and decision trees that clearly outline how and why decisions are made by our AI. We supplement these technical tools with comprehensive documentation that describes the algorithms' functions in accessible language. This approach is designed to demystify the AI's operations for non-technical stakeholders, fostering a higher level of trust and acceptance. By ensuring that our AI systems are transparent and their workings understandable, we maintain open communication and build confidence among users and regulators alike.\\n\"]},\n",
       "  {'contexts': [\"Q: Could you provide case studies on successful LLM-based application deployments, focusing on challenges and solutions?\\nA: We can share case studies of successful LLM-based application deployments, highlighting specific challenges such as data scarcity or model interpretability, and detailing the strategies and solutions we implemented to overcome these challenges. For example, in a project involving natural language processing for a legal firm, we faced significant data scarcity. To address this, we employed techniques like synthetic data generation and transfer learning from related domains to enrich our training datasets. Additionally, the issue of model interpretability was critical for our clients trust and regulatory compliance. We tackled this by integrating SHAP (SHapley Additive exPlanations) to provide clear, understandable insights into how our model's decisions were made, ensuring transparency and boosting user confidence in the AI system.\\n\",\n",
       "    \"Q: Describe your strategy for integrating LLMs into existing organizational frameworks and systems.\\nA: Our approach involves conducting a thorough analysis of the existing systems and workflows, designing integration plans that minimize disruption, and using APIs and custom connectors to ensure seamless integration of our LLM-based applications. We start by meticulously mapping the client's current infrastructure and operational flows to identify the most efficient points of integration. This is followed by the development of tailored integration plans that prioritize operational continuity and minimize downtime. To achieve seamless integration, we utilize robust APIs and develop custom connectors where necessary, ensuring compatibility with existing software platforms and databases. These tools allow for the smooth transfer of data and maintain the integrity and security of the system, ensuring that the new AI capabilities enhance functionality without compromising existing processes.\\n\",\n",
       "    \"Q: Please outline the training process for your LLMs, including choices regarding data, models, and validation approaches.\\nA: Our LLM training process begins with the meticulous sourcing of diverse and comprehensive datasets from global sources, ensuring a rich variety that includes various languages, dialects, and cultural contexts. This diversity is critical for building models that perform equitably across different demographics. We leverage cutting-edge tools like Apache Kafka for real-time data streaming and Apache Hadoop for handling large datasets efficiently during preprocessing stages. For model architecture selection, we utilize TensorFlow and PyTorch frameworks to design and iterate on neural network structures that best suit each application's unique requirements, whether it's for predictive analytics in finance or customer service chatbots. Depending on the use case, we might choose from a variety of architectures such as Transformer models for their robust handling of sequential data or GANs (Generative Adversarial Networks) for generating new, synthetic data samples for training.\\n\",\n",
       "    'Q: What support and maintenance do you provide following the deployment of AI applications?\\nA: Post-launch, we offer comprehensive support and maintenance services, including regular updates, bug fixes, and performance optimization. Our support team is available 24/7 to assist with any issues or questions. We utilize a ticketing system that ensures swift response times, with an average initial response time of under 2 hours. Additionally, we provide monthly performance reports and hold quarterly reviews with clients to discuss system status and potential improvements. Our proactive approach includes using automated monitoring tools to detect and resolve issues before they impact users, ensuring that our applications perform optimally at all times. This service structure has been instrumental in maintaining a client satisfaction rate above 98%.\\n',\n",
       "    \"Q: What measures do you employ to ensure your LLMs' decisions are clear and explicable?\\nA: We prioritize transparency and explainability in our AI models by incorporating advanced features such as model interpretability layers and providing comprehensive documentation on how model decisions are made. This approach ensures that users can understand and trust the outputs of our Large Language Models (LLMs). To achieve this, we integrate tools like LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) into our models. These tools allow us to break down and communicate the reasoning behind each model decision, fostering trust and facilitating easier audits by stakeholders.\\n\",\n",
       "    'Q: How do you maintain your AI applications with the newest AI technologies and advancements?\\nA: We maintain a dedicated R&D team focused on integrating the latest AI advancements into our applications. This includes regular updates and feature enhancements based on cutting-edge technologies such as GPT (Generative Pre-trained Transformer) for natural language understanding, CNNs (Convolutional Neural Networks) for advanced image recognition tasks, and DQN (Deep Q-Networks) for decision-making processes in complex environments. Our commitment to these AI methodologies ensures that our applications remain innovative, with capabilities that adapt to evolving market demands and client needs. This approach has enabled us to enhance the predictive accuracy of our financial forecasting tools by 25% and improve the efficiency of our educational content personalization by 40%\\n',\n",
       "    'Q: How are ethical issues like bias detection and data privacy managed in your LLM applications?\\nA: We adhere to ethical AI practices by implementing bias detection and mitigation techniques during the training of our Large Language Models (LLMs). This involves using diverse datasets to prevent skewed results and deploying algorithms specifically designed to identify and correct bias in AI outputs. For data privacy, we employ data anonymization and secure data handling protocols, ensuring compliance with GDPR, CCPA, and other relevant regulations. Our systems use state-of-the-art encryption methods for data at rest and in transit, and our data governance policies are rigorously audited by third-party security firms to maintain high standards of data integrity and confidentiality. This commitment extends to regular training for our staff on the latest privacy laws and ethical AI use to ensure that our practices are up-to-date and effective.\\n',\n",
       "    'Q: Can your AI applications be tailored to meet unique organizational or user-specific requirements?\\nA: Absolutely, customization is a core aspect of our offering. We work closely with clients to understand their specific needs and tailor our AI algorithms and app functionalities accordingly, using technologies such as TensorFlow for machine learning models, React for responsive UI/UX designs, and Kubernetes for scalable cloud deployment. This personalized approach allows us to optimize AI functionalities to match unique business processes, enhancing user experience and operational efficiency for each client. For example, for a retail client, we customized our recommendation engine to increase customer retention by 20% through more accurate and personalized product suggestions.\\n',\n",
       "    \"Q: How do you evaluate the success of your AI applications in fulfilling client goals?\\nA: Success measurement is tailored to each project's objectives. We establish key performance indicators (KPIs) in collaboration with our clients, such as user engagement rates, efficiency improvements, or return on investment (ROI). We then regularly review these metrics using advanced analytics platforms and business intelligence tools to assess the apps impact. Our approach includes monthly performance analysis meetings where we provide detailed reports and insights on metrics like session duration, user retention rates, and cost savings achieved through automation. We also implement A/B testing to continuously refine and optimize the application based on real-world usage data, ensuring that we make data-driven improvements that align closely with our clients' strategic goals.\\n\",\n",
       "    'Q: Please share your experience with developing AI-enabled applications and provide examples of notable projects.\\nA: Our company has 15 years of experience in developing AI-based applications, with a strong portfolio in sectors such as healthcare, finance, and education. For instance, our project MediAI Insight for the healthcare industry demonstrated significant achievements in patient data analysis, resulting in a 30% reduction in diagnostic errors and a 40% improvement in treatment personalization. Our platform has engaged over 200 healthcare facilities, achieving a user satisfaction rate of 95%.\\n']}],\n",
       " 'generation_model': [{'answer': 'We prioritize data security and privacy within our AI platforms by implementing robust measures such as encryption for data at rest and in transit, data anonymization techniques, and adherence to regulations like GDPR and CCPA. Our systems undergo rigorous audits by third-party security firms to ensure compliance and maintain high standards of data integrity and confidentiality. Additionally, we provide regular staff training on privacy laws and ethical AI practices to keep our protocols up-to-date and effective in safeguarding user data.'},\n",
       "  {'answer': 'Our LLMs utilize a process called continual learning to adapt and improve based on new data and evolving user expectations. Continual learning enables our models to incorporate fresh data streams and feedback iteratively, allowing them to stay current and relevant. This approach involves updating model parameters dynamically as new data becomes available, refining predictions, and enhancing performance over time. By engaging in continual learning, our LLMs can adapt to changing user preferences, adjust to emerging trends, and continuously refine their decision-making capabilities to better meet user expectations.'},\n",
       "  {'answer': 'Our strategy for ensuring that our Large Language Models (LLMs) can efficiently handle increased loads and demand involves implementing scalable infrastructure and resource management techniques. We leverage cloud computing services such as AWS or Google Cloud to dynamically allocate resources based on workload requirements. Additionally, we utilize techniques like load balancing and auto-scaling to distribute incoming requests evenly across multiple instances and automatically adjust resources in real-time to accommodate fluctuating demand. By employing these practices, we can ensure that our LLMs maintain optimal performance even during periods of high loads, providing reliable and responsive AI services to our users.'},\n",
       "  {'answer': 'We offer comprehensive support and maintenance services for LLM-based applications post-deployment, which includes regular updates to ensure optimal performance and address any potential model drift. Our team provides 24/7 support to handle any issues or questions that may arise, with an average initial response time of under 2 hours using a ticketing system for efficient communication. Additionally, we conduct monthly performance reports and quarterly reviews with clients to discuss system status and possible enhancements. To proactively manage model drift, we employ automated monitoring tools that enable us to detect and resolve issues before they affect users, ensuring that the applications continue to operate effectively and meet evolving business needs.'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with some RAGAS Metrics\n",
    "\n",
    "Below I am just experimenting to see how the RAGAS metrics can work with the `RAGModel` instance. This is not a full implementation of the RAGAS metrics but just a poc. We'll want to make this work in a more general way so that the columns can be properly mapped from the user-provided `predict_col` or the default `predict_col` to the column names that RAGAS expects i.e. `question`, `contexts`, `answer`, `ground_truth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm_ragas_ds = vm.init_dataset(result_df, __log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextEntityRecall\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextPrecision\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRelevancy\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-mI3jzOkk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
