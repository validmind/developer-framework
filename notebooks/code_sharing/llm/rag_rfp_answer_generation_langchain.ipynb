{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG for RFP Answer Generation using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise Exception(\"OPENAI_API_KEY not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from IPython.display import HTML, display\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def _format_cell_text(text, width=50):\n",
    "    \"\"\"Private function to format a cell's text.\"\"\"\n",
    "    return \"\\n\".join([textwrap.fill(line, width=width) for line in text.split(\"\\n\")])\n",
    "\n",
    "\n",
    "def _format_dataframe_for_tabulate(df):\n",
    "    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Format all string columns\n",
    "    for column in df_out.columns:\n",
    "        # Check if column is of type object (likely strings)\n",
    "        if df_out[column].dtype == object:\n",
    "            df_out[column] = df_out[column].apply(_format_cell_text)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def _dataframe_to_html_table(df):\n",
    "    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n",
    "    headers = df.columns.tolist()\n",
    "    table_data = df.values.tolist()\n",
    "    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n",
    "\n",
    "\n",
    "def display_nice(df, num_rows=None):\n",
    "    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n",
    "    if num_rows is not None:\n",
    "        df = df.head(num_rows)\n",
    "    formatted_df = _format_dataframe_for_tabulate(df)\n",
    "    html_table = _dataframe_to_html_table(formatted_df)\n",
    "    display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_keys(data, indent=0):\n",
    "    for key, value in data.items():\n",
    "        print(' ' * indent + str(key))\n",
    "        if isinstance(value, dict):  # if the value is another dictionary, recurse\n",
    "            print_dict_keys(value, indent + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Existing RFPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV file paths\n",
    "existing_rfp_paths = [\n",
    "    \"datasets/rag/rfp_existing_questions_client_1.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_2.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_3.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_4.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_5.csv\",\n",
    "]\n",
    "\n",
    "existing_rfp_df = [pd.read_csv(file_path) for file_path in existing_rfp_paths]\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "existing_rfp_df = pd.concat(existing_rfp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rfp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Questions and Answers to Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique identifier to each row in the rfp df\n",
    "existing_rfp_df[\"unique_id\"] = existing_rfp_df.index.astype(str)\n",
    "existing_rfp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Embed a single text item and return the embedding\n",
    "def get_embedding(text):\n",
    "    return embeddings_model.embed_query(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "if run:\n",
    "    # Apply the function to each question and answer and create new columns\n",
    "    existing_rfp_df['Question_Embeddings_LC'] = existing_rfp_df['RFP_Question'].apply(get_embedding)\n",
    "    existing_rfp_df['Answer_Embeddings_LC'] = existing_rfp_df['RFP_Answer'].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rfp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store RFP Questions and Answers in the Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\n",
    "    name = \"rfp_qa_collection\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data for batch addition\n",
    "all_embeddings = []\n",
    "all_metadatas = []\n",
    "all_documents = []\n",
    "all_ids = []\n",
    "\n",
    "# Loop through the DataFrame rows\n",
    "for index, row in existing_rfp_df.iterrows():\n",
    "    # Append each piece of data to its respective list\n",
    "    all_embeddings.append(row['Question_Embeddings_LC'])\n",
    "    all_metadatas.append({\n",
    "        'Project_Title': row['Project_Title'],\n",
    "        'RFP_Question_ID': row['RFP_Question_ID'],\n",
    "        'RFP_Question': row['RFP_Question'],\n",
    "        'RFP_Answer': row['RFP_Answer'],\n",
    "        'Area': row['Area'],\n",
    "        'Last_Accessed_At': row['Last_Accessed_At'],\n",
    "        'Requester': row['Requester'],\n",
    "        'Status': row['Status'],\n",
    "        'hnsw:space': 'cosine'\n",
    "    })\n",
    "    all_documents.append(row['RFP_Question'])\n",
    "    all_ids.append(row['unique_id'])\n",
    "\n",
    "# Add all data to the collection in a single operation\n",
    "collection.add(\n",
    "    ids=all_ids, \n",
    "    documents=all_documents,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadatas,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chroma = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"rfp_qa_collection\",\n",
    ")\n",
    "\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"documents in the collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the retrieval step by inputting a new RFP question and checking the top k most similar questions, including similarity scores for each retrieved question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = existing_rfp_df['RFP_Question'][0]\n",
    "documents = langchain_chroma.similarity_search_by_vector_with_relevance_scores(\n",
    "    get_embedding(query), k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 10\n",
    "\n",
    "print(f\"New RFP Question:\\n{query}\")\n",
    "print()\n",
    "print(f\"Top {number_of_documents} most similar existing RFP questions:\")\n",
    "print()\n",
    "\n",
    "context = \"\"\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    page_content = document[0].page_content  # This is where the content of the page is stored.\n",
    "    metadata = document[0].metadata  # This is where the metadata of the document is stored.\n",
    "    score = document[1]  # This is the score at the end of the tuple.\n",
    "\n",
    "    # Extracting the metadata\n",
    "    rfp = metadata['Project_Title']\n",
    "    question = metadata['RFP_Question']\n",
    "    answer = metadata['RFP_Answer']\n",
    "    metric = metadata['hnsw:space']\n",
    "\n",
    "    context += f\"Question: {question}\\nAnswer: {answer}\\n\"\n",
    "\n",
    "    # Print formatted output\n",
    "    print(f\"Document {i + 1}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Score: {1-score} ({metric})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context. \n",
    "If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "gpt4 = \"gpt-4-turbo\"\n",
    "gpt3 = \"gpt-3.5-turbo-16k\"\n",
    "llm = ChatOpenAI(model=gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = existing_rfp_df['RFP_Question'][0]\n",
    "\n",
    "# Generate an answer using the RAG chain\n",
    "response = rag_chain.invoke(\n",
    "    {\"question\" : question,\n",
    "     \"context\": context}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now inspect the `response_metadata` object to understand its contents and identify what could be useful to incorporate in our RAG evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict_keys(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the LLM used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: {response.response_metadata['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we showed earlier, we can also extract some token usage statistics that can help us understand and optimize our interactions with the language model for cost-effectiveness and efficiency.\n",
    "\n",
    "- **Prompt tokens**: tokens that form the input text sent to the language model. This includes all the text provided to the LLM to generate a response.\n",
    "- **Completion tokens**: number of tokens in the generated text or output from the model.\n",
    "- **Total tokens**: total number of tokens processed by the model. It is the sum of both `prompt_tokens` and `completion_tokens`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Completion tokens: {response.response_metadata['token_usage']['completion_tokens']}\")\n",
    "print(f\"Prompt tokens: {response.response_metadata['token_usage']['prompt_tokens']}\")\n",
    "print(f\"Total tokens: {response.response_metadata['token_usage']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset prepared for RAGAS evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new RFP questions \n",
    "rag_evaluation_df = pd.read_csv(\"datasets/rag/rag_evaluation_dataset_03.csv\")\n",
    "\n",
    "# Set the constant variable to the number of rows in the DataFrame\n",
    "NUM_OF_NEW_RFP_QUESTIONS = len(rag_evaluation_df)\n",
    "\n",
    "print(\"Number of New RFP Questions:\", NUM_OF_NEW_RFP_QUESTIONS)\n",
    "\n",
    "rag_evaluation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing our retrieved context, we will now create a simple helper to format the context so it can be directly passed to the RAG chain. This will be useful for creating our RAGAS evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question_embeddings, number_of_documents=10):\n",
    "    # Placeholder for the real similarity search function.\n",
    "    documents = langchain_chroma.similarity_search_by_vector_with_relevance_scores(\n",
    "        question_embeddings, k=number_of_documents\n",
    "    )\n",
    "\n",
    "    context = \"\"\n",
    "    for i, document in enumerate(documents):\n",
    "        metadata = document[0].metadata     \n",
    "\n",
    "        # Extract and compile context from metadata.\n",
    "        rfp_question = metadata.get('RFP_Question', 'No question found')\n",
    "        rfp_answer = metadata.get('RFP_Answer', 'No answer provided')\n",
    "\n",
    "        context += f\"Question: {rfp_question}\\nAnswer: {rfp_answer}\\n\"\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate responses for each question in every row, using the context extracted from the vector store based on question similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Number of questions to process by the RAG model\n",
    "number_of_rows_to_process = NUM_OF_NEW_RFP_QUESTIONS\n",
    "\n",
    "rag_evaluation_df['question_embeddings'] = ''\n",
    "\n",
    "for i, (index, row) in enumerate(rag_evaluation_df.iloc[:number_of_rows_to_process].iterrows()):\n",
    "    print(f\"Processing row {i}...\")\n",
    "\n",
    "    # Check if the 'answer' field is 'None' (as a string) for the current row\n",
    "    if row[\"answer\"] == \"None\":\n",
    "        print(f\"Answer is 'None' for question ID {index}. Invoking RAG model...\")\n",
    "\n",
    "        print(f\"Computing embeddings for question ID {index}...\")\n",
    "        question_embeddings = get_embedding(row[\"question\"])\n",
    "\n",
    "        rag_evaluation_df.at[index, 'question_embeddings'] = question_embeddings\n",
    "        \n",
    "        print(f\"Extracting context for question ID {index}...\")\n",
    "        context = get_context(question_embeddings)\n",
    "        \n",
    "        # Ensure that context is a list        \n",
    "        rag_evaluation_df.at[index, \"contexts\"] = [context]\n",
    "\n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        # Invoke the RAG model with the question from the current row\n",
    "        response = rag_chain.invoke(\n",
    "            {\"question\": row[\"question\"],\n",
    "             \"context\": context}\n",
    "        )\n",
    "\n",
    "        end_time = time.time()  # End timing\n",
    "\n",
    "        # Calculate the response time and store it\n",
    "        rag_evaluation_df.at[index, 'response_time'] = round(end_time - start_time, 1)\n",
    "\n",
    "        # Store whatever response comes from the LLM\n",
    "        rag_evaluation_df.at[index, \"answer\"] = response.content\n",
    "        print(f\"Question ID {index} answer updated with the response from the RAG model.\")\n",
    "        \n",
    "        # Store some metadata such as model name and tokens statistics\n",
    "        rag_evaluation_df.at[index, \"model\"] = response.response_metadata[\"model_name\"]\n",
    "        rag_evaluation_df.at[index, \"completion_tokens\"] = response.response_metadata['token_usage']['completion_tokens']\n",
    "        rag_evaluation_df.at[index, \"prompt_tokens\"] = response.response_metadata['token_usage']['prompt_tokens']\n",
    "        rag_evaluation_df.at[index, \"total_tokens\"] = response.response_metadata['token_usage']['total_tokens']\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'contexts' column from a string to a list of strings for each row\n",
    "# Assume all other data processing has been completed.\n",
    "# Now adjust the 'contexts' column to be a list of strings.\n",
    "\n",
    "rag_evaluation_df['contexts'] = rag_evaluation_df['contexts'].apply(lambda x: [x] if not isinstance(x, list) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ],\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "required_fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "metrics = [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\", \"context_relevancy\", \"answer_correctness\", \"answer_similarity\"]\n",
    "\n",
    "# Set the variable to the number of rows, limited to a maximum of NUM_OF_NEW_RFP_QUESTIONS\n",
    "number_of_rows_to_process = min(len(rag_evaluation_df), NUM_OF_NEW_RFP_QUESTIONS)\n",
    "\n",
    "# Mapping of metric names to their respective functions, assuming these functions are predefined\n",
    "metrics_functions = {\n",
    "    \"context_precision\": context_precision,\n",
    "    \"faithfulness\": faithfulness,\n",
    "    \"answer_relevancy\": answer_relevancy,\n",
    "    \"context_recall\": context_recall,\n",
    "    \"context_relevancy\": context_relevancy,\n",
    "    \"answer_correctness\": answer_correctness,\n",
    "    \"answer_similarity\": answer_similarity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This loop processes each row up to a predefined number of rows, evaluating them with specified metrics and storing the results\n",
    "for i, (index, row) in enumerate(rag_evaluation_df.iloc[:number_of_rows_to_process].iterrows()):\n",
    "    print(f\"Processing RFP question {i+1}...\")\n",
    "    print(f\"Question: {rag_evaluation_df.iloc[i]['question']}\")\n",
    "    print(f\"Answer: {rag_evaluation_df.iloc[i]['answer']}\")\n",
    "\n",
    "    # Create a temporary Dataset for the current row\n",
    "    ragas_dataset = Dataset.from_pandas(rag_evaluation_df.iloc[i: i + 1][required_fields])\n",
    "\n",
    "    # Evaluate using RAGAS metrics\n",
    "    evaluation_result = evaluate(\n",
    "        ragas_dataset, \n",
    "        [metrics_functions[metric] for metric in metrics if metric in metrics_functions])\n",
    "    print(\"Evaluation completed.\")\n",
    "\n",
    "    # Store evaluation results back into the DataFrame\n",
    "    for metric in metrics:\n",
    "        if metric in evaluation_result:\n",
    "            rag_evaluation_df.at[i, metric] = evaluation_result[metric]\n",
    "            print(f\"{metric}: {evaluation_result[metric]}\")\n",
    "\n",
    "print(\"All RFP questions processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
