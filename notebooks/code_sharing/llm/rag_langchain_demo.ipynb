{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Model using Langchain library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q qdrant-client langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"...\",\n",
    "  api_key = \"...\",\n",
    "  api_secret = \"...\",\n",
    "  project = \"...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read openai key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sample dataset from the library\n",
    "from validmind.datasets.llm.rag import rfp\n",
    "\n",
    "raw_df = rfp.load_data()\n",
    "train_df, test_df = rfp.preprocess(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    train_df,\n",
    "    text_column=\"question\",\n",
    "    target_column = \"ground_truth\",\n",
    "    __log=False\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    test_df,\n",
    "    text_column=\"question\",\n",
    "    target_column = \"ground_truth\",\n",
    "    __log=False\n",
    ")\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.Duplicates\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.StopWords\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.Punctuations\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CommonWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.CommonWords\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.LanguageDetection\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.Toxicity\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.PolarityAndSubjectivity\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.Sentiment\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from validmind.models import FunctionModel\n",
    "\n",
    "embedding_client = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "def embed(input):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    return embedding_client.embed_query(input[\"question\"])\n",
    "\n",
    "vm_embedder = FunctionModel(input_id=\"embedding_model\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_ds.assign_predictions(vm_embedder)\n",
    "print(vm_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "    params={\"probability\": 0.3},\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.CosineSimilarityHeatmap\",\n",
    "    inputs = {\"model\": vm_embedder, \"dataset\": vm_test_ds}\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.EuclideanDistanceHeatmap\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.PCAComponentsPairwisePlots\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "    params = {\"n_components\": 3}\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.TSNEComponentsPairwisePlots\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "    params = {\"n_components\": 3, \"perplexity\": 20}\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds.assign_predictions(vm_embedder)\n",
    "print(vm_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "# load documents from dataframe\n",
    "loader = DataFrameLoader(train_df, page_content_column=\"question\")\n",
    "docs = loader.load()\n",
    "# choose model using embedding client\n",
    "embedding_client = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# setup vector datastore\n",
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embedding_client,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"rfp_rag_collection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(input):\n",
    "    contexts = []\n",
    "\n",
    "    for result in qdrant.similarity_search_with_score(input[\"question\"]):\n",
    "        document, score = result\n",
    "        context = f\"Q: {document.page_content}\\n\"\n",
    "        context += f\"A: {document.metadata['ground_truth']}\\n\"\n",
    "\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "\n",
    "vm_retriever = FunctionModel(input_id=\"retrieval_model\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_ds.assign_predictions(model=vm_retriever)\n",
    "print(vm_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert RFP AI assistant.\n",
    "You are tasked with answering new RFP questions based on existing RFP questions and answers.\n",
    "You will be provided with the existing RFP questions and answer pairs that are the most relevant to the new RFP question.\n",
    "After that you will be provided with a new RFP question.\n",
    "You will generate an answer and respond only with the answer.\n",
    "Ignore your pre-existing knowledge and answer the question based on the provided context.\n",
    "\"\"\".strip()\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def generate(input):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\\n\".join(input[\"retrieval_model\"])},\n",
    "            {\"role\": \"user\", \"content\": input[\"question\"]},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "vm_generator = FunctionModel(input_id=\"generation_model\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vm_generator.predict(pd.DataFrame({\"retrieval_model\": [[\"My name is anil\"]], \"question\": [\"what is my name\"]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAG Model (Pipeline of \"Component\" Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import PipelineModel\n",
    "\n",
    "vm_rag_model = PipelineModel(vm_retriever | vm_generator, input_id=\"rag_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_ds.assign_predictions(model=vm_rag_model)\n",
    "print(vm_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_ds.df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    }\n",
    ").log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextEntityRecall\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextPrecision\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRelevancy\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.Faithfulness\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerRelevance\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRecall\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerCorrectness\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AspectCritique\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-pPj8dHa5-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
