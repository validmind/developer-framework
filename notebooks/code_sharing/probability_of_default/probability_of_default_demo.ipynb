{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Probability of Default Demo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use Case"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load API key and secret from environment variables\n",
        "%load_ext dotenv\n",
        "%dotenv .env\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import pickle\n",
        "import os\n",
        "from typing import List\n",
        "from datetime import datetime\n",
        "import re\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.stats import chi2_contingency\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Connect to ValidMind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n",
        "  api_key = \"...\",\n",
        "  api_secret = \"...\",\n",
        "  project = \"...\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def apply_default_probabilities(model_fit, df_scores, point_in_time=None):\n",
        "\n",
        "    # If point-in-time None set to today's date\n",
        "    if point_in_time is None:\n",
        "        point_in_time = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    df_scores['point_in_time'] = point_in_time\n",
        "\n",
        "    # Remove 'const' column if it exists in the dataframe\n",
        "    if 'const' in df_scores.columns:\n",
        "        df_scores = df_scores.drop(columns=['const'])\n",
        "\n",
        "    # Prepare the feature matrix\n",
        "    X = sm.add_constant(df_scores['score'])\n",
        "\n",
        "    # Compute the probabilities\n",
        "    probabilities = model_fit.predict(X)\n",
        "\n",
        "    # Add the probabilities to the dataframe\n",
        "    df_scores['predicted_default_probability'] = probabilities\n",
        "\n",
        "    # Compute predicted default based on the probability\n",
        "    df_scores['predicted_default'] = (df_scores['predicted_default_probability'] >= 0.5).astype(int)\n",
        "\n",
        "    return df_scores\n",
        "\n",
        "\n",
        "def apply_credit_scores(model, X, target_score, target_odds, pdo):\n",
        "    X_copy = X.copy()\n",
        "    beta = model.params.values\n",
        "    alpha = model.params[0]\n",
        "    factor = pdo / np.log(2)\n",
        "    offset = target_score - (factor * np.log(target_odds))\n",
        "\n",
        "    for _, row in X_copy.iterrows():\n",
        "        score_i = 0\n",
        "        for i in range(1, len(beta)):\n",
        "            WoE_i = row[i]\n",
        "            score_i += (beta[i] * WoE_i) * factor\n",
        "\n",
        "        score_i += alpha * factor\n",
        "        score_i += offset\n",
        "        X_copy.loc[row.name, \"score\"] = score_i\n",
        "\n",
        "    return X_copy\n",
        "\n",
        "\n",
        "def get_risk_band(pd_value, pd_risk_bands):\n",
        "    for band, (low, high) in pd_risk_bands.items():\n",
        "        if low <= pd_value < high:\n",
        "            return band\n",
        "    return None\n",
        "\n",
        "def apply_risk_bands(df, default_probabilities_column, pd_risk_bands):\n",
        "    df['risk_band'] = df[default_probabilities_column].apply(lambda x: get_risk_band(x, pd_risk_bands))\n",
        "    return df\n",
        "\n",
        "\n",
        "def compute_point_in_time_pd(model, df, threshold=0.5, point_in_time_date='2023-01-01'):\n",
        "    \"\"\"\n",
        "    Compute point-in-time default probabilities and predicted default values,\n",
        "    and add them as new columns in df.\n",
        "\n",
        "    The 'point_in_time' column is also added to df, with the given date.\n",
        "    \"\"\"\n",
        "    # Copy df to avoid changing the original DataFrame\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Store 'default' and 'const' columns and exclude them from df_copy\n",
        "    default_column = None\n",
        "\n",
        "    if 'default' in df_copy.columns:\n",
        "        default_column = df_copy['default']\n",
        "        df_copy.drop('default', axis=1, inplace=True)\n",
        "\n",
        "    # Calculate probabilities\n",
        "    probabilities = model.predict(df_copy)\n",
        "    df_copy[\"default_probabilities\"] = probabilities\n",
        "\n",
        "    # Drop 'const' column\n",
        "    if 'const' in df_copy.columns:\n",
        "        df_copy.drop('const', axis=1, inplace=True)\n",
        "\n",
        "    # Add 'default' column back into df_copy\n",
        "    if default_column is not None:\n",
        "        df_copy['default'] = default_column\n",
        "\n",
        "    # Compute predicted default\n",
        "    df_copy['predicted_default'] = df_copy['default_probabilities'].apply(lambda x: 1 if x >= threshold else 0)\n",
        "\n",
        "    # Add 'point_in_time' column\n",
        "    df_copy['point_in_time'] = pd.to_datetime(point_in_time_date)\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "\n",
        "\n",
        "def load_model_and_df(file_name):\n",
        "    \"\"\"Load a model and a DataFrame from a pickle file in the current directory\"\"\"\n",
        "    # Get the current working directory\n",
        "    current_path = os.getcwd()\n",
        "\n",
        "    # Construct the full file path\n",
        "    full_file_path = os.path.join(current_path, file_name)\n",
        "\n",
        "    print(f\"The full file path is: {full_file_path}\")\n",
        "\n",
        "    # Load the model and DataFrame\n",
        "    with open(full_file_path, 'rb') as file:\n",
        "        model, df = pickle.load(file)\n",
        "\n",
        "    print(f\"Model and DataFrame loaded from {full_file_path}\")\n",
        "\n",
        "    return model, df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Description"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Credit Risk Scorecard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GLM Credit Risk Scorecard Model\n",
        "file_name = 'model_fit_glm_scorecard_20230725_104439.pkl'\n",
        "model_fit_glm, df = load_model_and_df(file_name)\n",
        "\n",
        "# Compute credit scores\n",
        "target_score = 500  # The score that you want to assign to the target odds\n",
        "target_odds = 50  # The target odds (e.g., odds of being good vs bad)\n",
        "pdo = 20  # Points to double the odds\n",
        "\n",
        "# Compute risk scores from model's coefficients\n",
        "df_scores = apply_credit_scores(model_fit_glm, df, target_score, target_odds, pdo)\n",
        "\n",
        "# Define the scores and default columns\n",
        "scores_column = 'score'\n",
        "default_column = 'default'\n",
        "\n",
        "# Extract the relevant columns from the dataframe\n",
        "X = df_scores[scores_column]\n",
        "y = df_scores[default_column]\n",
        "\n",
        "# Add constant to the features (Statsmodels requires this step)\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the GLM model (using the binomial family to get logistic regression)\n",
        "model_fit_pit_pd = sm.GLM(y, X, family=sm.families.Binomial()).fit()\n",
        "\n",
        "# Print the model summary\n",
        "print(model_fit_pit_pd.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pit_pd = apply_default_probabilities(model_fit_pit_pd, df_scores)\n",
        "df_pit_pd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Validate PiT-PD Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Validate Point-in-Time PD Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.vm_models.test_context import TestContext\n",
        "from validmind.tests.data_validation.PiTPDHistogram import PiTPDHistogram\n",
        "\n",
        "# Define text context\n",
        "test_context_pit_pd = TestContext(dataset=df_pit_pd)\n",
        "\n",
        "# Configure test parameters\n",
        "params = {\n",
        "    \"default_column\": 'default',\n",
        "    \"predicted_default_column\": 'predicted_default',\n",
        "    \"default_probabilities_column\": 'predicted_default_probability',\n",
        "    \"point_in_time_column\": 'point_in_time',\n",
        "    \"title\": \"Histogram of Probability of Default\",\n",
        "}\n",
        "\n",
        "metric = PiTPDHistogram(test_context_pit_pd, params)\n",
        "metric.run()\n",
        "metric.result.log()\n",
        "metric.result.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.tests.data_validation.PiTCreditScoresHistogram import PiTCreditScoresHistogram\n",
        "\n",
        "# Define text context\n",
        "test_context_pit_pd = TestContext(dataset=df_pit_pd)\n",
        "\n",
        "# Configure test parameters\n",
        "params = {\n",
        "    \"default_column\": 'default',\n",
        "    \"predicted_default_column\": 'predicted_default',\n",
        "    \"scores_column\": 'score',\n",
        "    \"point_in_time_column\": 'point_in_time',\n",
        "    \"title\": \"Histogram of Credit Scores\",\n",
        "}\n",
        "\n",
        "metric = PiTCreditScoresHistogram(test_context_pit_pd, params)\n",
        "metric.run()\n",
        "metric.result.log()\n",
        "metric.result.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assing Risk Bands to PDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd_risk_bands = {\n",
        "    \"A\": [0, 0.1],\n",
        "    \"B\": [0.1, 0.4],\n",
        "    \"C\": [0.4, 0.6],\n",
        "    \"D\": [0.6, 1],\n",
        "}\n",
        "\n",
        "\n",
        "df_risk_bands = apply_risk_bands(df=df_pit_pd,\n",
        "                                  default_probabilities_column='predicted_default_probability',\n",
        "                                  pd_risk_bands=pd_risk_bands)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.tests.data_validation.DefaultRatesbyRiskBandPlot import DefaultRatesByRiskBandPlot\n",
        "\n",
        "# Define text context\n",
        "test_context_risk_grade = TestContext(dataset=df_risk_bands)\n",
        "\n",
        "# Configure test parameters\n",
        "params = {\n",
        "    \"default_column\": 'default',\n",
        "    \"risk_band_column\": 'risk_band',\n",
        "    \"title\": \"Bar Plot of Default Rates per Risk Grade\",\n",
        "}\n",
        "\n",
        "metric = DefaultRatesbyRiskBandPlot(test_context_risk_grade, params)\n",
        "metric.run()\n",
        "metric.result.log()\n",
        "metric.result.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "def plot_total_counts_by_risk_band(df, risk_band_column):\n",
        "    # Calculate the count of accounts in each risk band\n",
        "    risk_band_counts = df[risk_band_column].value_counts().sort_index()\n",
        "\n",
        "    # Convert to percentage\n",
        "    total_accounts = len(df)\n",
        "    risk_band_percentages = (risk_band_counts / total_accounts) * 100\n",
        "\n",
        "    # Use 'Dark24' color sequence for more distinguishable colors\n",
        "    colors = px.colors.qualitative.Dark24\n",
        "\n",
        "    # Create the bar plot\n",
        "    fig = go.Figure(data=[go.Bar(x=risk_band_percentages.index,\n",
        "                                 y=risk_band_percentages.values,\n",
        "                                 marker_color=colors)])\n",
        "\n",
        "    # Customize the plot\n",
        "    fig.update_layout(\n",
        "        title_text='Percentage of Total Accounts by Risk Band',\n",
        "        xaxis_title='Risk Band',\n",
        "        yaxis_title='Percentage of Total Accounts'\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "plot_total_counts_by_risk_band(df_risk_bands, risk_band_column='risk_band')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "def plot_default_rates_by_risk_band(df, risk_band_column, default_column):\n",
        "    # Calculate the default rate in each risk band\n",
        "    risk_bands = sorted(df[risk_band_column].unique())\n",
        "    default_rates = []\n",
        "\n",
        "    for band in risk_bands:\n",
        "        band_data = df[df[risk_band_column] == band]\n",
        "        default_rate = band_data[default_column].mean()\n",
        "        default_rates.append(default_rate)\n",
        "\n",
        "    # Use 'Dark24' color sequence for more distinguishable colors\n",
        "    colors = px.colors.qualitative.Dark24\n",
        "\n",
        "    # Create the bar plot\n",
        "    fig = go.Figure(data=[go.Bar(x=risk_bands, y=default_rates, marker_color=colors)])\n",
        "\n",
        "    # Customize the plot\n",
        "    fig.update_layout(\n",
        "        title_text='Default Rates by Risk Band',\n",
        "        xaxis_title='Risk Band',\n",
        "        yaxis_title='Default Rate'\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "plot_default_rates_by_risk_band(df_risk_bands, risk_band_column='risk_band', default_column='default')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Validate PiT-PD Risk Grades Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.tests.data_validation.RiskGradeDefaultRatesBarPlot import RiskGradeDefaultRatesBarPlot\n",
        "\n",
        "# Define text context\n",
        "test_context_risk_grade = TestContext(dataset=df_risk_grade)\n",
        "\n",
        "# Configure test parameters\n",
        "params = {\n",
        "    \"default_column\": 'default',\n",
        "    \"predicted_default_column\": 'predicted_default',\n",
        "    \"risk_band_column\": 'risk_band',\n",
        "    \"point_in_time_column\": 'point_in_time',\n",
        "    \"title\": \"Bar Plot of Default Rates per Risk Grade\",\n",
        "}\n",
        "\n",
        "metric = RiskGradeDefaultRatesBarPlot(test_context_risk_grade, params)\n",
        "metric.run()\n",
        "metric.result.log()\n",
        "metric.result.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Validate Prepared Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Sampling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sampling Method"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dev-framework",
      "language": "python",
      "name": "dev-framework"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
