{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "::: {.callout-tip}\n",
    "### New to ValidMind? \n",
    "For access to all features available in this notebook, create a free ValidMind account. \n",
    "\n",
    "Signing up is FREE — [**Sign up now!**](https://app.prod.validmind.ai)\n",
    ":::\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "Get your code snippet:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
    "\n",
    "3. Enter the model details and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
    "\n",
    "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    project=\"...\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "from IPython.display import HTML\n",
    "from notebooks.probability_of_default.helpers.Developer import Developer\n",
    "from notebooks.probability_of_default.helpers.economic_model import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"DRSFRMACBS\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "developer = Developer()\n",
    "macro_model = developer.load_objects_from_pickle(\n",
    "    \"datasets/macroeconomic_data_and_models.pkl\")\n",
    "\n",
    "df_raw = macro_model[\"df_raw\"]\n",
    "df_preparation = macro_model[\"df_prepared\"]\n",
    "\n",
    "df_train = macro_model[\"df_train\"]\n",
    "df_test = macro_model[\"df_test\"]\n",
    "model_fit = macro_model[\"model_fit\"]\n",
    "\n",
    "df_train_final = macro_model[\"df_train_final\"]\n",
    "df_test_final = macro_model[\"df_test_final\"]\n",
    "model_fit_final = macro_model[\"model_fit_final\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ValidMind Datasets and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "#  Create ValidMind Datasets\n",
    "vm_df_raw = vm.init_dataset(\n",
    "    dataset=df_raw,\n",
    "    target_column=target_column)\n",
    "vm_df_preparation = vm.init_dataset(\n",
    "    dataset=df_preparation,\n",
    "    target_column=target_column)\n",
    "vm_df_train = vm.init_dataset(\n",
    "    dataset=df_train,\n",
    "    target_column=target_column)\n",
    "vm_df_test = vm.init_dataset(\n",
    "    dataset=df_test,\n",
    "    target_column=target_column)\n",
    "vm_df_train_final = vm.init_dataset(\n",
    "    dataset=df_train_final,\n",
    "    target_column=target_column)\n",
    "vm_df_test_final = vm.init_dataset(\n",
    "    dataset=df_test_final,\n",
    "    target_column=target_column)\n",
    "\n",
    "#  Create ValidMind Models\n",
    "vm_model_fit = vm.init_model(\n",
    "    model=model_fit,\n",
    "    train_ds=vm_df_train,\n",
    "    test_ds=vm_df_test)\n",
    "vm_model_fit_final = vm.init_model(\n",
    "    model=model_fit_final,\n",
    "    train_ds=vm_df_train_final,\n",
    "    test_ds=vm_df_test_final)\n",
    "\n",
    "#  Create test contexts\n",
    "test_context_raw = TestContext(dataset=vm_df_raw)\n",
    "test_context_preparation = TestContext(dataset=vm_df_preparation)\n",
    "test_context_train = TestContext(dataset=vm_df_train)\n",
    "test_context_test = TestContext(dataset=vm_df_test)\n",
    "test_context_train_final = TestContext(dataset=vm_df_train_final)\n",
    "test_context_test_final = TestContext(dataset=vm_df_test_final)\n",
    "\n",
    "test_context_model = TestContext(model=vm_model_fit)\n",
    "test_context_models = TestContext(models=[vm_model_fit, vm_model_fit_final])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesMissingValues import TimeSeriesMissingValues\n",
    "\n",
    "params = {\"min_threshold\": 2}\n",
    "\n",
    "metric = TimeSeriesMissingValues(test_context_raw, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesOutliers import TimeSeriesOutliers\n",
    "\n",
    "params = {\"zscore_threshold\": 3}\n",
    "\n",
    "metric = TimeSeriesOutliers(test_context_raw, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesFrequency import TimeSeriesFrequency\n",
    "\n",
    "metric = TimeSeriesFrequency(test_context_raw)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepared Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesMissingValues import TimeSeriesMissingValues\n",
    "\n",
    "params = {\"min_threshold\": 2}\n",
    "\n",
    "metric = TimeSeriesMissingValues(test_context_preparation, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesFrequency import TimeSeriesFrequency\n",
    "\n",
    "metric = TimeSeriesFrequency(test_context_preparation)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesLinePlot import TimeSeriesLinePlot\n",
    "\n",
    "metric = TimeSeriesLinePlot(test_context_train)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = TimeSeriesLinePlot(test_context_test)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.LaggedCorrelationHeatmap import LaggedCorrelationHeatmap\n",
    "\n",
    "metric = LaggedCorrelationHeatmap(test_context_train)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.EngleGrangerCoint import EngleGrangerCoint\n",
    "\n",
    "metric = EngleGrangerCoint(test_context_train)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.DatasetSplit import DatasetSplit\n",
    "\n",
    "metric = DatasetSplit(test_context_model)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.ModelMetadata import ModelMetadata\n",
    "\n",
    "metric = ModelMetadata(test_context_model)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionCoeffsPlot import RegressionCoeffsPlot\n",
    "\n",
    "metric = RegressionCoeffsPlot(test_context_models)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs import RegressionModelsCoeffs\n",
    "\n",
    "metric = RegressionModelsCoeffs(test_context_models)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsPerformance import RegressionModelsPerformance\n",
    "\n",
    "metric = RegressionModelsPerformance(test_context_models)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
