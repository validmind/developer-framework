{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Scorecard Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "::: {.callout-tip}\n",
    "### New to ValidMind? \n",
    "To access the ValidMind Platform UI, you'll need an account.\n",
    "\n",
    "Signing up is FREE â€” **[Create your account](https://app.prod.validmind.ai)**.\n",
    ":::\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "Get your code snippet:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
    "\n",
    "3. Enter the model details, making sure to select **Credit Risk Scorecard** as the template and **Credit Risk - Underwriting - Credit Cards** as the use case, and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
    "\n",
    "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"...\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    project=\"...\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Credit risk Scorecard** model created from the Lending Club dataset is instrumental in computing the Probability of Default (PD), a key factor in ECL calculations. This scorecard assesses several credit characteristics of potential borrowers, like their credit history, income, outstanding debts, and more, each of which is assigned a specific score. By combining these scores, we derive a total score for each borrower, used to estimate the Point-in-Time (PiT) PD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "from IPython.display import HTML\n",
    "from notebooks.probability_of_default.helpers.Developer import Developer\n",
    "from notebooks.probability_of_default.helpers.scorecard_model import *\n",
    "from notebooks.probability_of_default.helpers.model_development import *\n",
    "\n",
    "# Visualization imports\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_column = \"default\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "developer = Developer()\n",
    "scorecard = developer.load_objects_from_pickle(\"datasets/scorecard_data_and_models.pkl\")\n",
    "\n",
    "df_raw = scorecard[\"df_raw\"]\n",
    "df_prepared = scorecard[\"df_prepared\"]\n",
    "df_train = scorecard[\"df_train\"]\n",
    "df_train_feature_selection = scorecard[\"df_train_feature_selection\"]\n",
    "\n",
    "df_train_feature_eng = scorecard[\"df_train_feature_eng\"]\n",
    "df_test_feature_eng = scorecard[\"df_test_feature_eng\"]\n",
    "\n",
    "model_fit_final = scorecard[\"model_fit_final\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ValidMind Datasets and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "vm_df_raw = vm.init_dataset(dataset=df_raw, target_column=default_column)\n",
    "vm_df_prepared = vm.init_dataset(dataset=df_prepared, target_column=default_column)\n",
    "vm_df_train_feature_selection = vm.init_dataset(\n",
    "    dataset=df_train_feature_selection, target_column=default_column)\n",
    "\n",
    "vm_df_train_feature_eng = vm.init_dataset(\n",
    "    dataset=df_train_feature_eng, target_column=default_column)\n",
    "vm_df_test_feature_eng = vm.init_dataset(\n",
    "    dataset=df_test_feature_eng, target_column=default_column)\n",
    "\n",
    "vm_model_fit_final = vm.init_model(\n",
    "    model=model_fit_final,\n",
    "    train_ds=vm_df_train_feature_eng,\n",
    "    test_ds=vm_df_test_feature_eng)\n",
    "\n",
    "\n",
    "test_context_raw = TestContext(dataset=vm_df_raw)\n",
    "test_context_prepared = TestContext(dataset=vm_df_prepared)\n",
    "test_context_train_feature_selection = TestContext(\n",
    "    dataset=vm_df_train_feature_selection)\n",
    "\n",
    "test_context_models_fit_final = TestContext(models=[vm_model_fit_final])\n",
    "test_context_model_fit_final = TestContext(model=[vm_model_fit_final])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.DescriptiveStatistics import DescriptiveStatistics\n",
    "\n",
    "metric = DescriptiveStatistics(test_context_raw, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n",
    "\n",
    "params = {\"threshold\": 80,\n",
    "          \"fig_height\": 1100}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context_raw, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "\n",
    "metric = ClassImbalance(test_context_prepared, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.IQROutliersTable import IQROutliersTable\n",
    "\n",
    "numerical_features = get_numerical_columns(df_prepared)\n",
    "params = {\"num_features\": numerical_features,\n",
    "          \"threshold\": 1.5\n",
    "          }\n",
    "\n",
    "metric = IQROutliersTable(test_context_prepared, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.IQROutliersBarPlot import IQROutliersBarPlot\n",
    "\n",
    "numerical_features = get_numerical_columns(df_prepared)\n",
    "params = {\"num_features\": numerical_features,\n",
    "          \"threshold\": 1.5,\n",
    "          \"fig_width\": 500}\n",
    "\n",
    "metric = IQROutliersBarPlot(test_context_prepared, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n",
    "\n",
    "metric = TabularNumericalHistograms(test_context_train_feature_selection, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context_train_feature_selection, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context_train_feature_selection, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TargetRateBarPlots import TargetRateBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"default_column\": default_column,\n",
    "    \"columns\": None\n",
    "}\n",
    "\n",
    "metric = TargetRateBarPlots(test_context_train_feature_selection, params=params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ChiSquaredFeaturesTable import ChiSquaredFeaturesTable\n",
    "\n",
    "categorical_features = get_categorical_columns(df_train_feature_selection)\n",
    "params = {\"cat_features\": categorical_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ChiSquaredFeaturesTable(test_context_train_feature_selection, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ANOVAOneWayTable import ANOVAOneWayTable\n",
    "\n",
    "numerical_features = get_numerical_columns(df_train_feature_selection)\n",
    "params = {\"num_features\": numerical_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ANOVAOneWayTable(test_context_train_feature_selection, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context_train_feature_selection, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n",
    "\n",
    "params = {\"features\": None}\n",
    "\n",
    "metric = FeatureTargetCorrelationPlot(test_context_train_feature_selection, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WOEBinTable import WOEBinTable\n",
    "\n",
    "metric = WOEBinTable(test_context_train_feature_selection, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"breaks_adj\": {\n",
    "        \"int_rate\": [5, 10, 15]}\n",
    "}\n",
    "\n",
    "metric = WOEBinTable(test_context_train_feature_selection, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WOEBinPlots import WOEBinPlots\n",
    "\n",
    "params = {\n",
    "    \"breaks_adj\": {\"int_rate\": [5, 10, 15]},\n",
    "    \"fig_height\": 500,\n",
    "}\n",
    "\n",
    "metric = WOEBinPlots(test_context_train_feature_selection, params=params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionCoeffsPlot import RegressionCoeffsPlot\n",
    "\n",
    "metric = RegressionCoeffsPlot(test_context_models_fit_final, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs import RegressionModelsCoeffs\n",
    "\n",
    "metric = RegressionModelsCoeffs(test_context_models_fit_final, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.GINITable import GINITable\n",
    "\n",
    "metric = GINITable(test_context_model_fit_final, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogRegressionConfusionMatrix import LogRegressionConfusionMatrix\n",
    "\n",
    "params = {\n",
    "    \"cut_off_threshold\": 0.5\n",
    "}\n",
    "\n",
    "metric = LogRegressionConfusionMatrix(test_context_model_fit_final, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.sklearn.ROCCurve import ROCCurve\n",
    "\n",
    "metric = ROCCurve(test_context_model_fit_final, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.sklearn.PrecisionRecallCurve import PrecisionRecallCurve\n",
    "\n",
    "metric = PrecisionRecallCurve(test_context_model_fit_final, params=None)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogisticRegPredictionHistogram import LogisticRegPredictionHistogram\n",
    "\n",
    "# Â Configure test parameters\n",
    "params = {\n",
    "    \"title\": \"Histogram of Probability of Default\",\n",
    "}\n",
    "\n",
    "metric = LogisticRegPredictionHistogram(test_context_model_fit_final, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogisticRegCumulativeProb import LogisticRegCumulativeProb\n",
    "\n",
    "# Â Configure test parameters\n",
    "params = {\n",
    "    \"title\": \"Cumulative Probability of Default\",\n",
    "}\n",
    "\n",
    "metric = LogisticRegCumulativeProb(test_context_model_fit_final, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.ScorecardHistogram import ScorecardHistogram\n",
    "\n",
    "# Â Configure test parameters\n",
    "params = {\n",
    "    \"target_score\": 600,\n",
    "    \"target_odds\": 50,\n",
    "    \"pdo\": 20,\n",
    "    \"title\": \"Histogram of Credit Scores\",\n",
    "}\n",
    "\n",
    "metric = ScorecardHistogram(test_context_model_fit_final, params)\n",
    "metric.run()\n",
    "metric.result.log()\n",
    "metric.result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-mI3jzOkk-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
