{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deposits Forecast Model with Seasonality using PyMC and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Explore ValidMind developer framework\n",
    "1. Data quality tests\n",
    "2. Seasonality adjustment\n",
    "3. Custom tests\n",
    "4. Random forest model\n",
    "5. Model validation test\n",
    "6. Review model document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_'></a>\n",
    "\n",
    "## About ValidMind\n",
    "\n",
    "ValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\n",
    "\n",
    "You use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
    "\n",
    "<a id='toc2_1_'></a>\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language. \n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n",
    "\n",
    "<a id='toc2_2_'></a>\n",
    "\n",
    "### New to ValidMind?\n",
    "\n",
    "If you haven't already seen our [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/guide/get-started-developer-framework.html), we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n",
    "\n",
    "::: {.callout-tip}\n",
    "\n",
    "For access to all features available in this notebook, create a free ValidMind account.\n",
    "\n",
    "Signing up is FREE — [**Sign up now!**](https://app.prod.validmind.ai)\n",
    "\n",
    ":::\n",
    "\n",
    "<a id='toc2_3_'></a>\n",
    "\n",
    "![Dataset based test architecture](./images/dataset_image.png)\n",
    "![Model based test architecture](./images/model_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "\n",
    "Let's go ahead and install the `validmind` library if its not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4_'></a>\n",
    "\n",
    "## Initialize the client library\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "Get your code snippet:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
    "\n",
    "3. Enter the model details, making sure to select **Time Series Forecasting** as the template and **Credit Risk - Underwriting - Loan** as the use case, and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
    "\n",
    "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "  api_key = \"...\",\n",
    "  api_secret = \"...\",\n",
    "  project = \"...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning how to run tests, let's explore the list of all available tests in the ValidMind Developer Framework. You can see that the documentation template for this model has references to some of the test IDs listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.list_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some data quality assessments by running a few individual tests related to data assessment. You will use the `vm.tests.list_tests()` function introduced above in combination with `vm.tests.list_tags()` and `vm.tests.list_task_types()` to find which prebuilt tests are relevant for data quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of available tags\n",
    "sorted(vm.tests.list_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of available task types\n",
    "sorted(vm.tests.list_task_types())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass `tags` and `task_types` as parameters to the `vm.tests.list_tests()` function to filter the tests based on the tags and task types. For example, to find tests related to tabular data quality for classification models, you can call `list_tests()` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.list_tests(task=\"regression\", tags=[\"time_series_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.datasets.regression import fred_deposits as demo_dataset\n",
    "\n",
    "deposits_df, deposits_seasonality_df, fedfunds_df, tb3ms_df, gs10_df, gs30_df = demo_dataset.load_data()\n",
    "\n",
    "raw_df = deposits_seasonality_df.copy()\n",
    "\n",
    "raw_df[\"FEDFUNDS\"] = fedfunds_df[\"FEDFUNDS\"]\n",
    "raw_df[\"TB3MS\"] = tb3ms_df[\"TB3MS\"]\n",
    "raw_df[\"GS10\"] = gs10_df[\"GS10\"]\n",
    "raw_df[\"GS30\"] = gs30_df[\"GS30\"]\n",
    "\n",
    "target_column = demo_dataset.target_column\n",
    "\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run data validation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=raw_df,\n",
    "    input_id=\"raw_ds\",\n",
    "    target_column=target_column,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.TimeSeriesLinePlot\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.TimeSeriesFrequency\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.SeasonalDecompose\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality adjustment with PyMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit linear PyMC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymc_df = raw_df.copy()\n",
    "pymc_df[\"Month\"] = pymc_df.index\n",
    "\n",
    "t = (pymc_df[\"Month\"]- pd.Timestamp(\"1900-01-01\")).dt.days.to_numpy()\n",
    "t_min = np.min(t)\n",
    "t_max = np.max(t)\n",
    "t = (t - t_min) / (t_max - t_min)\n",
    "\n",
    "y = pymc_df[target_column].to_numpy()\n",
    "y_max = np.max(y)\n",
    "y = y / y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(check_bounds=False) as linear:\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=0.5)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=0.5)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    trend = pm.Deterministic(\"trend\", alpha + beta * t)\n",
    "    pm.Normal(\"likelihood\", mu=trend, sigma=sigma, observed=y)\n",
    "\n",
    "    linear_prior = pm.sample_prior_predictive()\n",
    "\n",
    "with linear:\n",
    "    linear_trace = pm.sample(return_inferencedata=True)\n",
    "    linear_prior = pm.sample_posterior_predictive(trace=linear_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = az.extract(linear_prior, group=\"posterior_predictive\", num_samples=100)[\"likelihood\"] * y_max\n",
    "trend = az.extract(linear_trace, group=\"posterior\", num_samples=100)[\"trend\"] * y_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External test provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import LocalTestProvider\n",
    "\n",
    "tests_folder = \"tests\"\n",
    "# initialize the test provider with the tests folder we created earlier\n",
    "my_test_provider = LocalTestProvider(tests_folder)\n",
    "\n",
    "vm.tests.register_test_provider(\n",
    "    namespace=\"bny_test_provider\",\n",
    "    test_provider=my_test_provider,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run custom tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_pymc_ds = vm.init_dataset(\n",
    "    dataset=pymc_df,\n",
    "    input_id=\"pymc_ds\",\n",
    "    target_column=target_column,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\n",
    "    \"bny_test_provider.PyMCPlot:Posterior_Likelihood\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_pymc_ds,\n",
    "    },\n",
    "    params={\n",
    "        \"pymc_output\": likelihood,\n",
    "        \"month_column\": \"Month\",\n",
    "        \"title\": \"Posterior Predictive\",\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"bny_test_provider.PyMCPlot:Posterior_Trend\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_pymc_ds,\n",
    "    },\n",
    "    params={\n",
    "        \"pymc_output\": trend,\n",
    "        \"month_column\": \"Month\",\n",
    "        \"title\": \"Posterior Trend Lines\",\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Seasonality PyMC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fourier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_order = 10\n",
    "periods = (pymc_df[\"Month\"] - pd.Timestamp(\"1900-01-01\")).dt.days / 365.25\n",
    "\n",
    "fourier_features = pd.DataFrame(\n",
    "    {\n",
    "        f\"{func}_order_{order}\": getattr(np, func)(2 * np.pi * periods * order)\n",
    "        for order in range(1, n_order + 1)\n",
    "        for func in (\"sin\", \"cos\")\n",
    "    }\n",
    ")\n",
    "fourier_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"fourier_features\": np.arange(2 * n_order)}\n",
    "with pm.Model(check_bounds=False, coords=coords) as linear_with_seasonality:\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=0.5)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=0.5)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.1)\n",
    "    beta_fourier = pm.Normal(\"beta_fourier\", mu=0, sigma=0.1, dims=\"fourier_features\")\n",
    "    seasonality = pm.Deterministic(\n",
    "        \"seasonality\", pm.math.dot(beta_fourier, fourier_features.to_numpy().T)\n",
    "    )\n",
    "    trend = pm.Deterministic(\"trend\", alpha + beta * t)\n",
    "    mu = trend + seasonality\n",
    "    pm.Normal(\"likelihood\", mu=mu, sigma=sigma, observed=y)\n",
    "\n",
    "    linear_seasonality_prior = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = az.extract(linear_seasonality_prior, group=\"prior_predictive\", num_samples=100)[\"likelihood\"] * y_max\n",
    "trend = az.extract(linear_seasonality_prior, group=\"prior\", num_samples=100)[\"trend\"] * y_max\n",
    "seasonality = az.extract(linear_seasonality_prior, group=\"prior\", num_samples=100)[\"seasonality\"] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run custom tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"bny_test_provider.PyMCPlot:Prior_Likelihood\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_pymc_ds,\n",
    "    },\n",
    "    params={\n",
    "        \"pymc_output\": likelihood,\n",
    "        \"month_column\": \"Month\",\n",
    "        \"title\": \"Prior Predictive\",\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"bny_test_provider.PyMCPlot:Prior_Trend\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_pymc_ds,\n",
    "    },\n",
    "    params={\n",
    "        \"pymc_output\": trend,\n",
    "        \"month_column\": \"Month\",\n",
    "        \"title\": \"Prior Trend Lines\",\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"bny_test_provider.PyMCSeasonalityPlot:Prior_Seasonality_Lines\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_pymc_ds,\n",
    "    },\n",
    "    params={\n",
    "        \"seasonality\": seasonality,\n",
    "        \"month_column\": \"Month\",\n",
    "        \"title\": \"Prior Seasonality Lines\",\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior seasonality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with linear_with_seasonality:\n",
    "    linear_seasonality_trace = pm.sample(return_inferencedata=True)\n",
    "    linear_seasonality_posterior = pm.sample_posterior_predictive(trace=linear_seasonality_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = az.extract(linear_seasonality_posterior, group=\"posterior_predictive\", num_samples=100)[\"likelihood\"] * y_max\n",
    "trend = az.extract(linear_trace, group=\"posterior\", num_samples=100)[\"trend\"] * y_max\n",
    "seasonality = az.extract(linear_seasonality_trace, group=\"posterior\", num_samples=100)[\"seasonality\"] * 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run custom tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"bny_test_provider.PyMCPlot:Posterior_Predictive_Seasonality\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_pymc_ds,\n",
    "    },\n",
    "    params={\n",
    "        \"pymc_output\": likelihood,\n",
    "        \"month_column\": \"Month\",\n",
    "        \"title\": \"Posterior Predictive Seasonality\",\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"bny_test_provider.PyMCPlot:Posterior_Trend_Seasonality\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_pymc_ds,\n",
    "    },\n",
    "    params={\n",
    "        \"pymc_output\": trend,\n",
    "        \"month_column\": \"Month\",\n",
    "        \"title\": \"Posterior Trend Lines\",\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"bny_test_provider.PyMCSeasonalityPlot:Posterior_Seasonality_Lines\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_pymc_ds,\n",
    "    },\n",
    "    params={\n",
    "        \"seasonality\": seasonality,\n",
    "        \"month_column\": \"Month\",\n",
    "        \"title\": \"Posterior Seasonality Lines\",\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the posterior predictive mean for seasonality\n",
    "seasonality_posterior_mean = seasonality.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = raw_df.copy()\n",
    "\n",
    "# Adjust the target variable by removing the seasonality component\n",
    "preprocessed_df[target_column] = preprocessed_df[target_column] - seasonality_posterior_mean\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(preprocessed_df, test_size=0.20)\n",
    "\n",
    "X_train = train_df.drop(target_column, axis=1)\n",
    "y_train = train_df[target_column]\n",
    "X_test = test_df.drop(target_column, axis=1)\n",
    "y_test = test_df[target_column]\n",
    "\n",
    "model = RandomForestRegressor(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "s1 = model.score(X_train, y_train)\n",
    "s2 = model.score(X_test, y_test)\n",
    "print(\"R² of Support Vector Regressor on training set: {:.3f}\".format(s1))\n",
    "print(\"R² of Support Vector Regressor on test set: {:.3f}\".format(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create VM datasets and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df, input_id=\"train_dataset\", target_column=target_column\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df, input_id=\"test_dataset\", target_column=target_column\n",
    ")\n",
    "\n",
    "vm_model = vm.init_model(\n",
    "    model,\n",
    "    input_id=\"random_forest_regressor\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model validation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.model_validation.ModelMetadata\",\n",
    "    inputs = {\n",
    "        \"model\": vm_model\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.DatasetSplit\",\n",
    "    inputs = {\n",
    "        \"datasets\": [vm_train_ds, vm_test_ds]\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.RegressionErrors\",\n",
    "    inputs = {\n",
    "        \"datasets\": [vm_train_ds, vm_test_ds],\n",
    "        \"model\": vm_model\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.model_validation.RegressionResidualsPlot:train_dataset\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_train_ds,\n",
    "        \"model\": vm_model\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.model_validation.RegressionResidualsPlot:test_dataset\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.model_validation.RegressionResidualsPlot:test_dataset\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.RegressionR2Square\",\n",
    "    inputs = {\n",
    "        \"datasets\": [vm_train_ds, vm_test_ds],\n",
    "        \"model\": vm_model\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.PermutationFeatureImportance:train_dataset\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_train_ds,\n",
    "        \"model\": vm_model\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.PermutationFeatureImportance:test_dataset\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc8_'></a>\n",
    "\n",
    "## Where to go from here\n",
    "\n",
    "In this notebook you have learned the end-to-end process to document a model with the ValidMind Developer Framework, running through some very common scenarios in a typical model development setting:\n",
    "\n",
    "- Running out-of-the-box tests\n",
    "- Documenting your model by adding evidence to model documentation\n",
    "- Extending the capabilities of the Developer Framework by implementing custom tests\n",
    "- Ensuring that the documentation is complete by running all tests in the documentation template\n",
    "\n",
    "As a next step, you can explore the following notebooks to get a deeper understanding on how the developer framework allows you generate model documentation for any use case:\n",
    "\n",
    "<a id='toc8_1_'></a>\n",
    "\n",
    "### Use cases\n",
    "\n",
    "- [Application scorecard demo](../code_samples/credit_risk/application_scorecard_demo.ipynb)\n",
    "- [Linear regression documentation demo](../code_samples/regression/quickstart_regression_full_suite.ipynb)\n",
    "- [LLM model documentation demo](../code_samples/nlp_and_llm/foundation_models_integration_demo.ipynb)\n",
    "\n",
    "<a id='toc8_2_'></a>\n",
    "\n",
    "### More how-to guides and code samples\n",
    "\n",
    "- [Explore available tests in detail](../how_to/explore_tests.ipynb)\n",
    "- [In-depth guide for implementing custom tests](../code_samples/custom_tests/implement_custom_tests.ipynb)\n",
    "- [In-depth guide to external test providers](../code_samples/custom_tests/integrate_external_test_providers.ipynb)\n",
    "- [Configuring dataset features](../how_to/configure_dataset_features.ipynb)\n",
    "- [Introduction to unit and composite metrics](../how_to/run_unit_metrics.ipynb)\n",
    "\n",
    "<a id='toc8_3_'></a>\n",
    "\n",
    "### Discover more learning resources\n",
    "\n",
    "All notebook samples can be found in the following directories of the Developer Framework GitHub repository:\n",
    "\n",
    "- [Code samples](https://github.com/validmind/developer-framework/tree/main/notebooks/code_samples)\n",
    "- [How-to guides](https://github.com/validmind/developer-framework/tree/main/notebooks/how_to)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
