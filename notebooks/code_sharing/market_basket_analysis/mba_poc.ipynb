{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Basket Analysis POC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q mlxtend networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"...\",\n",
    "  api_key = \"...\",\n",
    "  api_secret = \"...\",\n",
    "  model = \"...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "retail = pd.read_csv('./datasets/mba/Online Retail.csv')\n",
    "\n",
    "def preprocess(retail):\n",
    "    # Drop rows where any of the elements is missing\n",
    "    retail = retail.dropna()\n",
    "\n",
    "    # Convert the 'Description' column to a categorical type (similar to factor in R)\n",
    "    retail['Description'] = retail['Description'].astype('category')\n",
    "    # Convert the 'Country' column to a categorical type\n",
    "    retail['Country'] = retail['Country'].astype('category')\n",
    "    # Convert the 'InvoiceDate' to datetime format if not already\n",
    "    retail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'])\n",
    "    # Extract just the date part and store in a new column\n",
    "    retail['Date'] = retail['InvoiceDate'].dt.date\n",
    "    # Extract time from 'InvoiceDate' and store in another variable\n",
    "    TransTime = retail['InvoiceDate'].dt.strftime('%H:%M:%S')\n",
    "    # Convert 'InvoiceNo' to string first, then to numeric, handling errors by coercing invalid parses to NaN\n",
    "    InvoiceNo = pd.to_numeric(retail['InvoiceNo'].astype(str), errors='coerce')\n",
    "    # Add 'TransTime' and 'InvoiceNo' as new columns to the 'retail' DataFrame\n",
    "    retail['TransTime'] = pd.to_datetime(retail['InvoiceDate']).dt.strftime('%H:%M:%S')  # Redefining for clarity\n",
    "    retail['InvoiceNo'] = pd.to_numeric(retail['InvoiceNo'].astype(str), errors='coerce')\n",
    "    retail['InvoiceNo'] = InvoiceNo\n",
    "    return retail\n",
    "\n",
    "retail = preprocess(retail)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(retail.head())\n",
    "# Display data types and non-null counts for each column\n",
    "print(retail.info())\n",
    "\n",
    "# Show summary statistics for numeric columns\n",
    "print(retail.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'InvoiceNo' and 'Date', then apply a custom function to concatenate 'Description'\n",
    "transactionData = retail.groupby(['InvoiceNo', 'Date'])['Description'].apply(lambda x: ', '.join(x.astype(str))).reset_index()\n",
    "\n",
    "# Rename the concatenated descriptions column for clarity\n",
    "transactionData.rename(columns={'Description': 'ConcatenatedDescriptions'}, inplace=True)\n",
    "\n",
    "# Remove the 'InvoiceNo' and 'Date' columns\n",
    "transactionData.drop(['InvoiceNo', 'Date'], axis=1, inplace=True)\n",
    "\n",
    "# Rename the remaining column to 'items'\n",
    "transactionData.columns = ['items']\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(transactionData)\n",
    "# Write the DataFrame to a CSV file without quotes and without row names (indices)\n",
    "transactionData.to_csv('./datasets/mba//market_basket_transactions.csv', index=False, quoting=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('./datasets/mba//market_basket_transactions.csv', header=None)\n",
    "\n",
    "transactions = [row[0].split(',') for row in data.values]\n",
    "\n",
    "# Use TransactionEncoder from mlxtend to encode the list of transactions\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "transaction_df = pd.DataFrame(te_ary, columns=te.columns_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Generate frequent itemsets\n",
    "frequent_itemsets = apriori(transaction_df, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "# Display the rules\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate item frequencies\n",
    "item_frequencies = transaction_df.sum(axis=0).sort_values(ascending=False)[:20]\n",
    "\n",
    "# Create a bar plot for the item frequencies\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=item_frequencies.values, y=item_frequencies.index, palette=\"Pastel2\")\n",
    "plt.title(\"Absolute Item Frequency Plot for the Top 20 Items\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Items\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative item frequencies\n",
    "total_transactions = transaction_df.shape[0]\n",
    "item_frequencies = (transaction_df.sum(axis=0) / total_transactions).sort_values(ascending=False)[:20]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a bar plot for the relative item frequencies\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=item_frequencies.values, y=item_frequencies.index, palette=\"pastel\")\n",
    "plt.title(\"Relative Item Frequency Plot for the Top 20 Items\")\n",
    "plt.xlabel(\"Relative Frequency\")\n",
    "plt.ylabel(\"Items\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Generate frequent itemsets\n",
    "frequent_itemsets = apriori(transaction_df, min_support=0.01, use_colnames=True, max_len=3)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.8)\n",
    "# Display the rules\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subset(df, rule_index):\n",
    "    antecedents = df.loc[rule_index, 'antecedents']\n",
    "    consequents = df.loc[rule_index, 'consequents']\n",
    "    # Check each rule against all other rules\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx != rule_index and antecedents.issubset(row['antecedents']) and consequents.issubset(row['consequents']):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Assuming 'rules' is the DataFrame obtained from association_rules\n",
    "rules['is_subset'] = [is_subset(rules, i) for i in rules.index]\n",
    "# Filter rules to remove those marked as subsets\n",
    "filtered_rules = rules[~rules['is_subset']]\n",
    "print(f\"Number of non-redundant rules: {len(filtered_rules)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'all_rules' is the DataFrame containing the association rules generated earlier\n",
    "\n",
    "# Filter rules where confidence is greater than 0.4\n",
    "subRules = filtered_rules[filtered_rules['confidence'] > 0.4]\n",
    "\n",
    "# Setting the aesthetic style of seaborn plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Creating the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = sns.scatterplot(x='support', y='confidence', data=subRules, size='lift', sizes=(50, 200), hue='lift', palette='viridis', legend='brief')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Association Rules - Confidence vs. Support')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "\n",
    "# Adding a legend with title\n",
    "plt.legend(title='Lift')\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10subRules = subRules.sort_values(by='confidence', ascending=False).head(10)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges from antecedents to consequents for each rule\n",
    "for _, rule in top10subRules.iterrows():\n",
    "    antecedents = ', '.join(list(rule['antecedents']))\n",
    "    consequents = ', '.join(list(rule['consequents']))\n",
    "    G.add_edge(antecedents, consequents, weight=rule['confidence'])\n",
    "\n",
    "# Position nodes using the spring layout\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=20)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2500, edge_color='gray', linewidths=1, font_size=10)\n",
    "\n",
    "# Draw edge labels based on the confidence of each rule\n",
    "edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in G.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')\n",
    "\n",
    "# Display the plot\n",
    "plt.title('Top 10 Association Rules Graph')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-pPj8dHa5-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
