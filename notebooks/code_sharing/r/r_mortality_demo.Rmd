---
editor_options:
  markdown:
    wrap: 72
---

# Introduction

## Executive Summary

Being able to make accurate and timely estimates of future claims is a
fundamental task for actuaries. Questions of profitability, product
competitiveness, and insurer solvency depend on understanding future
claims, with mortality being one of the central issues facing a life
insurer.

In this demo, we show an example of a machine learning application on
mortality assumption setting, a classic life insurance problem. Using
real mortality data collected by the Society of Actuaries, we will walk
you through the process of model building and validation.

## Overview of Mortality Case Study

#### Case Study Data

Our dataset is the composite mortality experience data at policy level
from 2012 to 2016. This dataset is used to published the 2016 Individual
Life Experience Report by SOA's Individual Life Experience Committee
(ILEC).

For the case study, the data was restricted to term life insurance
policies that were within the initial policy term, issued after 1980,
and the issue age was at least 18 years old.

More details on this dataset can be found in Section 2 of the data
report
<https://www.soa.org/49957f/globalassets/assets/files/resources/research-report/2021/2016-individual-life-report.pdf>

#### Case Study Model

For the case study in this paper, we used the `statsmodel`'s
implementation of the GLM family models. Our main model is using Poisson
distribution with log link function that is often used for mortality
prediction.

The <b>response variable</b> used in this case study is the
`number of deaths`. `Policies exposed` was used as a weight in the
model. We also tried to fit the `mortality rate`, which is
`number of deaths`/ `policies exposed` using Gaussian distribution with
log link, that can be found in the Appendix

The <b>features</b> used in the mortality model are:

-   `Attained Age` -- the sum of the policyholder's age at policy issue
    and the number of years they have held the policy.

-   `Duration` -- the number of years (starting with a value of one) the
    policyholder has had the policy.

-   `Smoking Status` -- if the policyholder is considered a smoker or
    not.

-   `Preferred Class` -- an underwriting structure used by insurers to
    classify and price policyholders. Different companies have different
    structures with the number of classes ranging from two to four. The
    lower the class designation, the healthier the policyholders who are
    put into that class. Thus, someone in class 1 of 3 (displayed as 1_3
    in this paper) is considered healthier at time of issue than someone
    in class 3 of 3.

-   `Gender` -- A categorical feature in the model with two levels, male
    and female.

-   `Guaranteed Term Period` -- the length of the policy at issue during
    which the premium will remain constant regardless of policyholder
    behavior or health status. The shortest term period in the data is
    five years with increasing lengths by five years up to 30 years.
    Term period is used as a categorical feature with six levels.

-   `Face_Amount_Band`

-   `Observation Year`

## **Initializing the Python environment**

```{r setup, include=FALSE}
library(reticulate)

python_version <- "<path_to_python_version>"
use_python(python_version)

library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(glue)
library(dplyr)
library(caTools)
library(glmnet)
library(zoo)
library(caret)
library(knitr)
library(kableExtra)
library(purrr)
library(validmind)
library(rmarkdown)
library(htmltools)
library(broom)
library(huxtable)
library(xgboost)
library(plotly)

print_table <- print

custom_print_table_hook <- function(x) {
  cat(x)
}

knitr::knit_hooks$set(print_table = custom_print_table_hook)
```

## **Initializing the ValidMind Client Library**

Log in to the ValidMind platform with your registered email address, and
navigate to the Documentation Projects page.

### **Creating a new Documentation Project**

*(Note: if a documentation project has already been created, you can
skip this section and head directly "Finding Project API key and
secret")*

Clicking on "Create a new project" allows to you to register a new
documentation project for our demo model.

Select "Customer Churn model" from the Model drop-down, and "Initial
Validation" as Type. Finally, click on "Create Project".

### **Finding the project API key and secret**

In the "Client Integration" page of the newly created project, you will
find the initialization code that allows the client library to associate
documentation and tests with the appropriate project. The initialization
code configures the following arguments:

-   api_host: Location of the ValidMind API.

-   api_key: Account API key.

-   api_secret: Account Secret key.

-   project: The project identifier. The \`project\` argument is
    mandatory since it allows the library to associate all data
    collected with a specific account project.

The code snippet can be copied and pasted directly in the cell below to
initialize the ValidMind Developer Framework when run:

```{r}
vm_r <- vm(
  api_key="<your_api_key_here>",
  api_secret="<your_api_secret_here>",
  model="<your_project_id_here>",
  python_version=python_version,
  api_host="https://api.dev.vm.validmind.ai/api/v1/tracking"
)
```

## 3. Load Data

```{r}
full_df <- read.csv('../insurance_mortality/train_df.csv')

df <- full_df %>%
  select(Observation_Year, Gender, Smoker_Status,
         Insurance_Plan, Duration, Attained_Age, SOA_Guaranteed_Level_Term_Period,
         Face_Amount_Band, Preferred_Class,
         Number_Of_Deaths, Policies_Exposed,
         SOA_Anticipated_Level_Term_Period, SOA_Post_level_Term_Indicator,
         Expected_Death_QX2015VBT_by_Policy,
         Issue_Age, Issue_Year)
```

```{r}
df <- df %>%
  mutate(mort = Number_Of_Deaths / Policies_Exposed)

```

```{r}
df
```

```{r}
df <- df %>%
  filter(
    Expected_Death_QX2015VBT_by_Policy != 0,
    Smoker_Status != 'Unknown',
    Insurance_Plan == ' Term',
    !is.na(Preferred_Class),
    Attained_Age >= 18,
    Issue_Year >= 1980,
    SOA_Post_level_Term_Indicator == "Within Level Term",
    SOA_Anticipated_Level_Term_Period != "Unknown",
    mort < 1
  )
```

```{r}
summary(df)
```

## 5. Data Validation

### Validation of Raw Dataset

#### Run the Dat Quality Test Suite

```{r}
vm_dataset <- vm_r$init_dataset(
    dataset = df,
    target_column = "mort",
)
```

```{r}
suite_results <- vm_r$run_test_suite(
    "custom_tabular_dataset",
    dataset=vm_dataset,
    config={},
    fail_fast=TRUE
)
```

```{r}
processed_validation_results <- process_result(suite_results)
```

```{r, echo=FALSE}
pretty_print_all_plotly(processed_validation_results)
```

```{r, echo=FALSE, results='asis'}
pretty_print_all_images(processed_validation_results)
```

```{r, echo=FALSE, results='asis'}
pretty_print_all_tables(processed_validation_results)
```

## 6. Model Training

### Encode Categorical Variables

```{r}
# Categorical variables
cat_vars <- c('Observation_Year',
              'Gender',
              'Smoker_Status',
              'Face_Amount_Band',
              'Preferred_Class',
              'SOA_Anticipated_Level_Term_Period')

# Convert these numerical variables to factors to force one hot encoding
df$Observation_Year <- as.factor(df$Observation_Year)
df$Preferred_Class <- as.factor(df$Preferred_Class)

# Create a formula for dummyVars
formula <- as.formula(paste("~", paste(cat_vars, collapse = "+")))

# Create dummy variable object
dummies <- dummyVars(formula, data = df, fullRank = TRUE)

# Apply dummy variable transformation
df_dummies <- predict(dummies, newdata = df)

# Combine with original data frame
df <- bind_cols(df, as.data.frame(df_dummies))

cat_vars_encoded <- colnames(df_dummies)
```


```{r}
# Target Variable
Y <- c("Number_Of_Deaths")

# Predictors (aka Input Variables)
X <- c(cat_vars_encoded, "Attained_Age", "Duration", "Const")

# Split the dataset into train and test sets
set.seed(42)
splitIndex <- createDataPartition(df$Number_Of_Deaths, p = 0.80, list = FALSE)
train_df <- df[splitIndex, ]
test_df <- df[-splitIndex, ]

policies_exposed <- train_df$Policies_Exposed

# Add constant variable
train_df$Const <- 1
test_df$Const <- 1

# Subset to keep only relevant columns
train_df <- train_df[, c(X, Y)]
test_df <- test_df[, c(X, Y)]

# Print sizes
cat("Train size:", nrow(train_df), ", test size:", nrow(test_df), "\n")
```


```{r}
offset_log <- log(policies_exposed)

model <- glm(Number_Of_Deaths ~ .,
             data=train_df[, c(X, Y)],
             family=poisson(link="log"),
             weights=policies_exposed,
             offset=offset_log)

# Fit the model
summary(model)
```

```{r}
model_path = save_model(model)

vm_train_ds <- vm_r$init_dataset(
  dataset=train_df,
  target_column="Number_Of_Deaths"
)
vm_test_ds <- vm_r$init_dataset(
  dataset=test_df,
  target_column="Number_Of_Deaths"
)

vm_model <- vm_r$init_r_model(
    model_path=model_path,
    train_ds=vm_train_ds,
    test_ds=vm_test_ds
)
```

```{r}
suite_results <- vm_r$run_test_suite(
    "custom_regression_suite",
    model=vm_model,
    models=list(vm_model),
    fail_fast = TRUE,
)
```


```{r}
processed_validation_results <- process_result(suite_results)
```

```{r, echo=FALSE}
pretty_print_all_plotly(processed_validation_results)
```

```{r, echo=FALSE, results='asis'}
pretty_print_all_images(processed_validation_results)
```

```{r, echo=FALSE, results='asis'}
pretty_print_all_tables(processed_validation_results)
```
