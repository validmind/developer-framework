# Copyright Â© 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

import warnings

import plotly.express as px
from datasets import Dataset

from validmind import tags, tasks

from .utils import get_ragas_config, get_renamed_columns

LOWER_IS_BETTER_ASPECTS = ["harmfulness", "maliciousness"]


@tags("ragas", "llm", "qualitative")
@tasks("text_summarization", "text_generation", "text_qa")
def AspectCritique(
    dataset,
    question_column="question",
    answer_column="answer",
    contexts_column="contexts",
    aspects: list = [  # noqa: B006 this is fine as immutable default since it never gets modified
        "coherence",
        "conciseness",
        "correctness",
        "harmfulness",
        "maliciousness",
    ],
    additional_aspects: list = None,
):
    """
    Evaluates generations against the following aspects: harmfulness, maliciousness,
    coherence, correctness, and conciseness.

    ### Overview:

    This is designed to assess submissions against predefined and user-defined "aspects".
    For each aspect, a judge LLM is prompted to critique a piece of generated text based
    on a description of the aspect. The output of this evaluation is a binary (0/1 = yes/no)
    score that indicates whether the submission aligns with the defined aspect or not.

    ### Inputs and Outputs:

    The input to this metric is a dataset containing the input `question` (prompt to the LLM)
    and the `answer` (text generated by the LLM). Any retrieved `contexts` can also be
    included to enhance the evaluation.

    The `question_column`, `answer_column`, and `contexts_column` parameters can be used to
    specify the names or sources for the data that this metric will evaluate if the dataset
    does not contain the required columns `question`, `answer`, and `contexts`.

    By default, the aspects evaluated are harmfulness, maliciousness, coherence,
    correctness, and conciseness. To change the aspects evaluated, the `aspects` parameter
    can be set to a list containing any of these aspects.

    To add custom aspects, the `additional_aspects` parameter can be passed as a list
    of tuples where each tuple contains the aspect name and a description of the aspect
    that the judge LLM will use to critique the submission.

    The output of this metric is a table of scores for each aspect where the aspect score
    is the number of "yes" scores divided by the total number of submissions:
    $$
    \\text{aspect score} = \\frac{\\text{number of "yes" scores}}{\\text{total number of submissions}}
    $$

    ### Examples:

    - **Mapping to Required Columns:** If the dataset does not contain the columns required
    to run this metric (i.e., `question`, `answer`, and `contexts`), the

    ```python
    pred_col = my_vm_dataset.prediction_column(my_vm_model)
    run_test(
        "validmind.model_validation.ragas.AspectCritique",
        inputs={"dataset": my_vm_dataset},
        params={
            "question_column": "input_prompt",
            "answer_column": f"{pred_col}.llm_output",
            "contexts_column": lambda row: [row[pred_col]["context_message"]],
        },
    )
    ```

    - **Custom Aspects:** To evaluate custom aspects, the `additional_aspects` parameter can
    be set to a list of tuples where each tuple contains the aspect name and a description
    of the aspect that the judge LLM will use to critique the submission. For example, to
    evaluate whether the LLM-generated text has a "professional tone", the `additional_aspects`
    parameter can be set like this:

    ```python
    run_test(
        "validmind.model_validation.ragas.AspectCritique",
        inputs={"dataset": my_vm_dataset},
        params={
            "additional_aspects": [
                ("professionalism", "Does the text have a professional tone?"),
            ],
        },
    )
    ```
    """
    try:
        from ragas import evaluate
        from ragas.metrics.critique import AspectCritique as _AspectCritique
        from ragas.metrics.critique import (
            coherence,
            conciseness,
            correctness,
            harmfulness,
            maliciousness,
        )
    except ImportError:
        raise ImportError("Please run `pip install validmind[llm]` to use LLM tests")

    aspect_map = {
        "coherence": coherence,
        "conciseness": conciseness,
        "correctness": correctness,
        "harmfulness": harmfulness,
        "maliciousness": maliciousness,
    }

    warnings.filterwarnings(
        "ignore",
        category=FutureWarning,
        message="promote has been superseded by promote_options='default'.",
    )

    required_columns = {
        "question": question_column,
        "answer": answer_column,
        "contexts": contexts_column,
    }

    df = get_renamed_columns(dataset._df, required_columns)

    built_in_aspects = [aspect_map[aspect] for aspect in aspects]
    custom_aspects = (
        [
            _AspectCritique(name=name, definition=description)
            for name, description in additional_aspects
        ]
        if additional_aspects
        else []
    )
    all_aspects = [*built_in_aspects, *custom_aspects]

    result_df = evaluate(
        Dataset.from_pandas(df), metrics=all_aspects, **get_ragas_config()
    ).to_pandas()

    # reverse the score for aspects where lower is better
    for aspect in LOWER_IS_BETTER_ASPECTS:
        if aspect in result_df.columns:
            result_df[aspect] = 1 - result_df[aspect]

    df_melted = result_df.melt(
        id_vars=["question", "answer", "contexts"],
        value_vars=[aspect.name for aspect in all_aspects],
        var_name="Metric",
        value_name="Result",
    )
    df_counts = df_melted.groupby(["Metric", "Result"]).size().reset_index(name="Count")
    df_counts["Result"] = df_counts["Result"].map({0: "Fail", 1: "Pass"})

    fig = px.bar(
        df_counts,
        x="Metric",
        y="Count",
        color="Result",
        color_discrete_map={"Fail": "red", "Pass": "green"},
        labels={"Count": "Pass vs Fail Count", "Metric": "Aspect Name"},
        barmode="group",
        title="Aspect Critique Results",
    )

    return {
        "Aspect Scores": [
            {"Aspect": aspect, "Score": result_df[aspect].mean()}
            for aspect in aspects + [aspect.name for aspect in custom_aspects]
        ]
    }, fig
