# Copyright Â© 2023 ValidMind Inc. All rights reserved.

import warnings
from dataclasses import dataclass

import matplotlib.pyplot as plt
import shap

from validmind.logging import get_logger
from validmind.vm_models import Figure, Metric

logger = get_logger(__name__)


@dataclass
class SHAPGlobalImportance(Metric):
    """
    Evaluates and visualizes global feature importance using SHAP values for model explanation and risk identification.

    **Purpose:**
    The SHAP (SHapley Additive exPlanations) Global Importance metric aims to elucidate model outcomes by attributing
    them to the contributing features. It assigns a quantifiable global importance to each feature via their respective
    absolute Shapley values, thereby making it suitable for tasks like classification (both binary and multiclass).
    This metric forms an essential part of model risk management.

    **Test Mechanism:**
    The exam begins with the selection of a suitable explainer which aligns with the model's type. For tree-based
    models like XGBClassifier, RandomForestClassifier, CatBoostClassifier, TreeExplainer is used whereas for linear
    models like LogisticRegression, XGBRegressor, LinearRegression, it is the LinearExplainer. Once the explainer
    calculates the Shapley values, these values are visualized using two specific graphical representations:

    1. Mean Importance Plot: This graph portrays the significance of individual features based on their absolute
    Shapley values. It calculates the average of these absolute Shapley values across all instances to highlight the
    global importance of features.

    2. Summary Plot: This visual tool combines the feature importance with their effects. Every dot on this chart
    represents a Shapley value for a certain feature in a specific case. The vertical axis is denoted by the feature
    whereas the horizontal one corresponds to the Shapley value. A color gradient indicates the value of the feature,
    gradually changing from low to high. Features are systematically organized in accordance with their importance.
    These plots are generated by the function `_generate_shap_plot()`.

    **Signs of High Risk:**

    - Overemphasis on certain features in SHAP importance plots, thus hinting at the possibility of model overfitting
    - Anomalies such as unexpected or illogical features showing high importance, which might suggest that the model's
    decisions are rooted in incorrect or undesirable reasoning
    - A SHAP summary plot filled with high variability or scattered data points, indicating a cause for concern

    **Strengths:**

    - SHAP does more than just illustrating global feature significance, it offers a detailed perspective on how
    different features shape the model's decision-making logic for each instance.
    - It provides clear insights into model behavior.

    **Limitations:**

    - High-dimensional data can convolute interpretations.
    - Associating importance with tangible real-world impact still involves a certain degree of subjectivity.
    """

    name = "shap"
    required_inputs = ["model"]
    metadata = {
        "task_types": ["classification", "text_classification"],
        "tags": [
            "sklearn",
            "binary_classification",
            "multiclass_classification",
            "feature_importance",
            "visualization",
        ],
    }

    def _generate_shap_plot(self, type_, shap_values, x_test):
        """
        Plots two types of SHAP global importance (SHAP).
        :params type: mean, summary
        :params shap_values: a matrix
        :params x_test:
        """
        plt.close("all")

        # preserve styles
        # mpl.rcParams["grid.color"] = "#CCC"
        ax = plt.axes()
        ax.set_facecolor("white")

        summary_plot_extra_args = {}
        if type_ == "mean":
            summary_plot_extra_args = {"plot_type": "bar"}

        shap.summary_plot(shap_values, x_test, show=False, **summary_plot_extra_args)
        figure = plt.gcf()
        # avoid displaying on notebooks and clears the canvas for the next plot
        plt.close()

        return Figure(
            for_object=self,
            figure=figure,
            key=f"shap:{type_}",
            metadata={"type": type_},
        )

    def run(self):
        model_library = self.model.model_library()
        if model_library in [
            "statsmodels",
            "pytorch",
            "catboost",
            "transformers",
            "FoundationModel",
            "R",
        ]:
            logger.info(f"Skiping SHAP for {model_library} models")
            return

        trained_model = self.model.model
        model_class = self.model.model_class()

        # the shap library generates a bunch of annoying warnings that we don't care about
        warnings.filterwarnings("ignore", category=UserWarning)

        # Any tree based model can go here
        if (
            model_class == "XGBClassifier"
            or model_class == "RandomForestClassifier"
            or model_class == "CatBoostClassifier"
        ):
            explainer = shap.TreeExplainer(trained_model)
        elif (
            model_class == "LogisticRegression"
            or model_class == "XGBRegressor"
            or model_class == "LinearRegression"
        ):
            explainer = shap.LinearExplainer(trained_model, self.model.test_ds.x)
        else:
            raise ValueError(f"Model {model_class} not supported for SHAP importance.")

        shap_values = explainer.shap_values(self.model.test_ds.x)

        figures = [
            self._generate_shap_plot("mean", shap_values, self.model.test_ds.x),
            self._generate_shap_plot("summary", shap_values, self.model.test_ds.x),
        ]

        # restore warnings
        warnings.filterwarnings("default", category=UserWarning)

        return self.cache_results(figures=figures)
