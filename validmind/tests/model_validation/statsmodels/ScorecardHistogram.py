# Copyright © 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

from dataclasses import dataclass

import plotly.graph_objects as go
from matplotlib import cm

from validmind.vm_models import Figure, Metric


@dataclass
class ScorecardHistogram(Metric):
    """
    Creates histograms of credit scores, from both default and non-default instances, generated by a credit-risk model.

    **Purpose**: The Scorecard Histogram test metric provides a visual interpretation of the credit scores generated by
    a machine learning model for credit-risk classification tasks. It aims to compare the alignment of the model's
    scoring decisions with the actual outcomes of credit loan applications. It helps in identifying potential
    discrepancies between the model's predictions and real-world risk levels.

    **Test Mechanism**: This metric uses logistic regression to generate a histogram of credit scores for both default
    (negative class) and non-default (positive class) instances. Using both training and test datasets, the metric
    calculates the credit score of each instance with a scorecard method, considering the impact of different features
    on the likelihood of default. İncludes the default point to odds (PDO) scaling factor and predefined target score
    and odds settings. Histograms for training and test sets are computed and plotted separately to offer insights into
    the model's generalizability to unseen data.

    **Signs of High Risk**:
    - Discrepancies between the distributions of training and testing data, indicating a model's poor generalisation
    ability
    - Skewed distributions favouring specific scores or classes, representing potential bias

    **Strengths**:
    - Provides a visual interpretation of the model's credit scoring system, enhancing comprehension of model behavior
    - Enables a direct comparison between actual and predicted scores for both training and testing data
    - Its intuitive visualization helps understand the model's ability to differentiate between positive and negative
    classes
    - Can unveil patterns or anomalies not easily discerned through numerical metrics alone

    **Limitations**:
    - Despite its value for visual interpretation, it doesn't quantify the performance of the model, and therefore may
    lack precision for thorough model evaluation
    - The quality of input data can strongly influence the metric, as bias or noise in the data will affect both the
    score calculation and resultant histogram
    - Its specificity to credit scoring models limits its applicability across a wider variety of machine learning
    tasks and models
    - The metric's effectiveness is somewhat tied to the subjective interpretation of the analyst, since it relies on
    the analyst's judgment of the characteristics and implications of the plot.
    """

    name = "scorecard_histogram"
    required_inputs = ["datasets"]
    metadata = {
        "task_types": ["classification"],
        "tags": ["tabular_data", "visualization", "credit_risk"],
    }
    default_params = {
        "title": "Histogram of Scores",
        "score_column": "score",
    }

    @staticmethod
    def plot_score_histogram(dataframes, dataset_titles, score_col, target_col, title):
        figures = []
        # Generate a colormap and convert to Plotly-accepted color format
        # Adjust 'viridis' to any other matplotlib colormap if desired
        colormap = cm.get_cmap("viridis")

        for _, (df, dataset_title) in enumerate(zip(dataframes, dataset_titles)):
            fig = go.Figure()

            # Get unique classes and assign colors
            classes = sorted(df[target_col].unique())
            colors = [
                colormap(i / len(classes))[:3] for i in range(len(classes))
            ]  # RGB
            color_dict = {
                cls: f"rgb({int(rgb[0]*255)}, {int(rgb[1]*255)}, {int(rgb[2]*255)})"
                for cls, rgb in zip(classes, colors)
            }

            for class_value in sorted(df[target_col].unique()):
                scores_class = df[df[target_col] == class_value][score_col]
                fig.add_trace(
                    go.Histogram(
                        x=scores_class,
                        opacity=0.75,
                        name=f"{dataset_title} {target_col} = {class_value}",
                        marker=dict(
                            color=color_dict[class_value],
                        ),
                    )
                )
            fig.update_layout(
                barmode="overlay",
                title_text=f"{title} - {dataset_title}",
                xaxis_title="Score",
                yaxis_title="Frequency",
                legend_title=target_col,
            )
            figures.append(fig)
        return figures

    def run(self):
        title = self.params["title"]
        score_column = self.params["score_column"]
        dataset_titles = [dataset.input_id for dataset in self.inputs.datasets]
        target_column = self.inputs.datasets[0].target_column

        dataframes = []
        metric_value = {"score_histogram": {}}
        for dataset in self.inputs.datasets:
            df = dataset.df.copy()
            # Check if the score_column exists in the DataFrame
            if score_column not in df.columns:
                raise ValueError(
                    f"The required column '{score_column}' is not present in the dataset with input_id {dataset.input_id}"
                )

            df[score_column] = dataset.get_extra_column(score_column)
            dataframes.append(df)
            metric_value["score_histogram"][dataset.input_id] = list(df[score_column])

        figures = self.plot_score_histogram(
            dataframes, dataset_titles, score_column, target_column, title
        )

        figures_list = [
            Figure(
                for_object=self,
                key=f"score_histogram_{title.replace(' ', '_')}_{i+1}",
                figure=fig,
            )
            for i, fig in enumerate(figures)
        ]

        return self.cache_results(metric_value=metric_value, figures=figures_list)
