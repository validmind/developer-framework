# Copyright © 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

import itertools
from dataclasses import dataclass

import pandas as pd
import plotly.graph_objects as go
import torch
from selfcheckgpt.modeling_selfcheck import SelfCheckNLI
from tqdm import tqdm

from validmind.vm_models import Figure, Metric


@dataclass
class SelfCheckNLIScore(Metric):
    """
    Evaluates text generation models' performance by quantifying the level of hallucination in generated texts compared to reference texts.

    **Purpose**: The HallucinationScore metric is designed to assess the factual accuracy and reliability of text generated by models, focusing on the detection and quantification of hallucinations—instances where generated content deviates from factual or expected outputs. By comparing generated texts against reference texts, this metric highlights discrepancies indicative of hallucinations, offering insights into the model's ability to produce contextually and factually coherent content.

    **Test Mechanism**: To compute the HallucinationScore, the metric employs a comparison between the generated texts (model predictions) and the provided reference texts (true values). Using the SelfCheckNLI model, it evaluates each generated text's level of factual congruence with the reference, assigning a hallucination score based on the semantic coherence and factual accuracy. The scores for each text instance are then visualized in a line plot, allowing for the examination of hallucination trends across the dataset.

    **Signs of High Risk**:
    - High hallucination scores across a significant portion of the dataset, indicating a prevalence of factually inaccurate or irrelevant content generation.
    - Patterns of consistent hallucination in specific contexts or subjects, suggesting gaps in the model's understanding or knowledge.
    - Sharp fluctuations in hallucination scores, which may reveal inconsistencies in the model's performance or sensitivity to certain types of input.

    **Strengths**:
    - Directly addresses the critical aspect of factual accuracy in generated text, beyond mere linguistic or stylistic coherence.
    - Provides a granular, instance-by-instance analysis of model performance, allowing for targeted improvements and diagnostics.
    - Facilitates a deeper understanding of a model's capabilities and limitations in producing reliable and accurate content.

    **Limitations**:
    - Reliance on the SelfCheckNLI model means the accuracy and effectiveness of the HallucinationScore are contingent upon the performance and suitability of the underlying NLI model.
    - May not fully capture the subtleties of certain factual inaccuracies or the contextual relevance of reference texts, especially in complex or nuanced domains.
    - Potentially resource-intensive, given the computational demands of running advanced NLI models for large datasets.
    """

    name = "self_check_nli_score"
    required_inputs = ["model", "dataset"]

    def run(self):
        # Assuming the dataset is structured with generated sentences and reference samples
        y_true = list(itertools.chain.from_iterable(self.inputs.dataset.y))
        y_pred = self.inputs.dataset.y_pred(self.inputs.model)

        hallucination_scores = self.compute_hallucination_scores(y_pred, y_true)

        # Visualization of scores
        figures = self.visualize_scores(hallucination_scores)

        return self.cache_results(figures=figures)

    def compute_hallucination_scores(self, predictions, references):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        selfcheck_nli = SelfCheckNLI(device=device)
        hallucination_scores = []

        print("Starting hallucination score computation...")

        for index, (sentences, samples) in enumerate(
            tqdm(zip(predictions, references), total=len(predictions))
        ):
            sent_scores_nli = selfcheck_nli.predict(
                sentences=sentences, sampled_passages=samples
            )

            # Compute the mean of the hallucination scores for this row
            average_score = sent_scores_nli.mean()
            hallucination_scores.append(average_score)

            # Print a progress update for each row
            print(
                f"Row {index + 1}/{len(predictions)}: Average hallucination score: {average_score}"
            )

        print("Completed hallucination score computation.")

        return hallucination_scores

    def visualize_scores(self, scores):
        scores_df = pd.DataFrame(scores, columns=["Hallucination Score"])

        fig = go.Figure()
        fig.add_trace(
            go.Scatter(
                x=scores_df.index,
                y=scores_df["Hallucination Score"],
                mode="lines+markers",
                name="Hallucination Score",
            )
        )

        fig.update_layout(
            title="Hallucination Scores Across Text Instances",
            xaxis_title="Text Instance Index",
            yaxis_title="Hallucination Score",
        )

        # Wrapping the plotly figure for compatibility with your framework might be needed
        figures = [
            Figure(
                for_object=self,
                key=self.key,
                figure=fig,
            )
        ]

        return figures
