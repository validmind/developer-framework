# Copyright Â© 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

from dataclasses import dataclass

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from sklearn.metrics import roc_auc_score

from validmind.errors import SkipTestError
from validmind.logging import get_logger
from validmind.vm_models import Figure, Metric

logger = get_logger(__name__)


@dataclass
class FeaturesAUC(Metric):
    """
    Evaluates the discriminatory power of each individual feature within a binary classification model by calculating
    the Area Under the Curve (AUC) for each feature separately.

    ### Purpose

    The central objective of this metric is to quantify how well each feature on its own can differentiate between the
    two classes in a binary classification problem. It serves as a univariate analysis tool that can help in
    pre-modeling feature selection or post-modeling interpretation.

    ### Test Mechanism

    For each feature, the metric treats the feature values as raw scores to compute the AUC against the actual binary
    outcomes. It provides an AUC value for each feature, offering a simple yet powerful indication of each feature's
    univariate classification strength.

    ### Signs of High Risk

    - A feature with a low AUC score may not be contributing significantly to the differentiation between the two
    classes, which could be a concern if it is expected to be predictive.
    - Conversely, a surprisingly high AUC for a feature not believed to be informative may suggest data leakage or
    other issues with the data.

    ### Strengths

    - By isolating each feature, it highlights the individual contribution of features to the classification task
    without the influence of other variables.
    - Useful for both initial feature evaluation and for providing insights into the model's reliance on individual
    features after model training.

    ### Limitations

    - Does not reflect the combined effects of features or any interaction between them, which can be critical in
    certain models.
    - The AUC values are calculated without considering the model's use of the features, which could lead to different
    interpretations of feature importance when considering the model holistically.
    - This metric is applicable only to binary classification tasks and cannot be directly extended to multiclass
    classification or regression without modifications.
    """

    name = "features_auc"
    required_inputs = ["model", "dataset"]
    default_params = {
        "fontsize": 12,
        "figure_height": 500,
    }
    tasks = ["classification"]
    tags = [
        "feature_importance",
        "AUC",
        "visualization",
    ]

    def run(self):
        dataset = self.inputs.dataset
        x = dataset.x_df()
        y = dataset.y_df()
        n_targets = dataset.df[dataset.target_column].nunique()

        if n_targets != 2:
            raise SkipTestError("FeaturesAUC metric requires a binary target variable.")

        aucs = pd.DataFrame(index=x.columns, columns=["AUC"])

        for column in x.columns:
            feature_values = x[column]
            if feature_values.nunique() > 1:
                auc_score = roc_auc_score(y, feature_values)
                aucs.loc[column, "AUC"] = auc_score
            else:
                aucs.loc[
                    column, "AUC"
                ] = np.nan  # Not enough unique values to calculate AUC

        # Sorting the AUC scores in descending order
        sorted_indices = aucs["AUC"].dropna().sort_values(ascending=False).index

        # Plotting the results
        fig = go.Figure()
        fig.add_trace(
            go.Bar(
                y=[column for column in sorted_indices],
                x=[aucs.loc[column, "AUC"] for column in sorted_indices],
                orientation="h",
            )
        )
        fig.update_layout(
            title_text="Feature AUC Scores",
            yaxis=dict(
                tickmode="linear",
                dtick=1,
                tickfont=dict(size=self.params["fontsize"]),
                title="Features",
                autorange="reversed",  # Ensure that the highest AUC is at the top
            ),
            xaxis=dict(title="AUC"),
            height=self.params["figure_height"],
        )

        return self.cache_results(
            metric_value=aucs.to_dict(),
            figures=[
                Figure(
                    for_object=self,
                    key="features_auc",
                    figure=fig,
                ),
            ],
        )
