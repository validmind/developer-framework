# Copyright Â© 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

import itertools
from dataclasses import dataclass

import evaluate
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from validmind.vm_models import Figure, Metric


@dataclass
class RegardHistogram(Metric):
    """
    **Purpose:**
    The `RegardHistogram` metric offers a histogram-based visualization of regard scores across different text
    samples. As an evolution from the line plot representation, the histogram provides a distributional perspective
    on how often certain regard scores (positive, negative, neutral, or other) are perceived or generated by the model.

    **Test Mechanism:**
    This metric extracts the necessary data from the model's test dataset: the input text, the true regard (target text),
    and the model's predicted regard. After ensuring data consistency, the `evaluate.load("regard")` tool computes the
    regard scores. Histograms are then created for each category of regard against input, target, and predicted texts.
    These histograms illustrate the frequency distribution of scores within each category, shedding light on commonalities
    or outliers in the model's performance.

    **Signs of High Risk:**
    Any noticeable skewness in the histogram, especially when comparing the predicted regard scores with the target regard
    scores, could indicate biases or inconsistencies in the model. For instance, a lack of neutral scores in the model's
    predictions, despite a balanced distribution in the target data, might signal an issue.

    **Strengths:**
    Histogram representations give a quick, intuitive snapshot of score distributions. The immediate visual contrast
    between the model's predictions and target data is useful for stakeholders and researchers aiming to gauge
    model reliability in regard to sentiment assessments.

    **Limitations:**
    The efficacy of `RegardHistogram` hinges upon the precision of underlying tools like `evaluate.load("regard")`, which
    might have their biases or inaccuracies. The metric portrays regard in set categories, but sentiments in real-world scenarios
    can be more nuanced. Assumptions made, like the expectation of consistent regard across texts, may not always hold, given that
    sentiments can be subjective. While histograms showcase distributions, they may not capture the intricate contexts behind
    texts, possibly leading to oversimplifications or misinterpretations.
    """

    name = "regard_histogram"
    required_inputs = ["model", "dataset"]
    metadata = {
        "task_types": ["text_classification", "text_summarization"],
        "tags": ["regard_histogram"],
    }

    def _get_datasets(self):
        if not hasattr(self, "model"):
            raise AttributeError("The 'model' attribute is missing.")

        y_true = list(itertools.chain.from_iterable(self.inputs.dataset.y))
        y_pred = self.inputs.dataset.y_pred(self.inputs.model.input_id)
        input_text = self.inputs.dataset.df[self.inputs.dataset.text_column]

        if not len(y_true) == len(y_pred) == len(input_text):
            raise ValueError(
                "Inconsistent lengths among input text, true summaries, and predicted summaries."
            )

        return input_text, y_true, y_pred

    def regard_histogram(self):
        regard_tool = evaluate.load("regard")
        input_text, y_true, y_pred = self._get_datasets()

        dataframes = {
            "Input Text": input_text,
            "Target Text": y_true,
            "Predicted Summaries": y_pred,
        }

        total_text_columns = len(dataframes)
        total_rows = total_text_columns * 2

        categories_order = ["positive", "negative", "neutral", "other"]
        category_colors = {
            "negative": "#d9534f",
            "neutral": "#5bc0de",
            "other": "#f0ad4e",
            "positive": "#5cb85c",
        }

        fig = make_subplots(
            rows=total_rows,
            cols=2,
            subplot_titles=[
                f"{col_name} {cat}"
                for col_name in dataframes
                for cat in categories_order
            ],
            shared_xaxes=True,
            vertical_spacing=0.1,
        )

        row_offset = 0
        for column_name, column_data in dataframes.items():
            results = regard_tool.compute(data=column_data)["regard"]
            regard_dicts = [
                dict((x["label"], x["score"]) for x in sublist) for sublist in results
            ]

            for idx, category in enumerate(categories_order, start=1):
                row, col = ((idx - 1) // 2 + 1 + row_offset, (idx - 1) % 2 + 1)
                fig.add_trace(
                    go.Histogram(
                        name=f"{category} ({column_name})",
                        x=[res_dict[category] for res_dict in regard_dicts],
                        marker_color=category_colors[category],
                        showlegend=False,  # Disable the legend
                    ),
                    row=row,
                    col=col,
                )
            row_offset += 2  # Move to the next pair of rows for the next text column

        subplot_height = 350
        total_height = (
            total_rows * subplot_height + 200
        )  # 200 for padding, titles, etc.

        fig.update_layout(
            title_text="Regard Score Histogram Distribution", height=total_height
        )

        # Specify x and y titles only for the first subplot
        fig.update_xaxes(title_text="Index", showticklabels=True, row=1, col=1)
        fig.update_yaxes(title_text="Score", showticklabels=True, row=1, col=1)

        # Show tick labels on all subplots
        for row in range(total_rows):
            for col in range(2):  # since you have 2 columns
                fig.update_xaxes(showticklabels=True, row=row + 1, col=col + 1)
                fig.update_yaxes(showticklabels=True, row=row + 1, col=col + 1)

        return fig

    def run(self):
        fig = self.regard_histogram()
        return self.cache_results(
            figures=[Figure(for_object=self, key=self.key, figure=fig)]
        )
