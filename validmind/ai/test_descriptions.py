# Copyright Â© 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

import os
from concurrent.futures import ThreadPoolExecutor
from typing import Union

from validmind.utils import md_to_html

from ..client_config import client_config
from ..logging import get_logger

__executor = ThreadPoolExecutor()

logger = get_logger(__name__)


AI_REVISION_NAME = "Generated by ValidMind AI"
DEFAULT_REVISION_NAME = "Default Description"

SYSTEM_PROMPT = """  # noqa
You are an expert data scientist and MRM specialist.
You are tasked with analyzing the results of a quantitative test run on some model or dataset.
Your goal is to create a test description that will act as part of the model documentation.
You will provide both the developer and other consumers of the documentation with a clear and concise "interpretation" of the results they will see.
The overarching theme to maintain is MRM documentation.

Examine the provided statistical test results and compose a description of the results.
The results are either in the form of serialized tables or images of plots.
Compose a description and interpretation of the result to accompany it in MRM documentation.
It will be read by other data scientists and developers and by validators and stakeholders.

Use valid Markdown syntax to format the response.
Avoid long sentences and complex vocabulary.
Avoid overly verbose explanations - the goal is to explain to a user what they are seeing in the results.
Structure the response clearly and logically.
Respond only with your analysis and insights, not the verbatim test results.
Respond only with the markdown content, no explanation or context for your response is necessary.
Use the Test ID that is provided to form the Test Name e.g. "ClassImbalance" -> "Class Imbalance".

Explain the test, its purpose, its mechanism/formula etc and why it is useful.
If relevant, provide a very brief description of the way this test is used in model/dataset evaluation and how it is interpreted.
Highlight the key insights from the test results. The key insights should be concise and easily understood.
An insight should only be included if it is something not entirely obvious from the test results.
End the response with any closing remarks, summary or additional useful information.

Use the following format for the response (feel free to stray from it if necessary - this is a suggested starting point):

<ResponseFormat>
**<Test Name>** calculates the xyz <continue to explain what it does in detail>...

This test is useful for <explain why and for what this test is useful>...

**Key Insights:**

The following key insights can be identified in the test results:

- **<key insight 1 - title>**: <concise explanation of key insight 1>
- ...<continue with any other key insights using the same format>
</ResponseFormat>
""".strip()


USER_PROMPT = """
Test ID: `{test_name}`

<Test Docstring>
{test_description}
</Test Docstring>

<Test Results Table(s)>
{test_summary}
</Test Results Table(s)>
""".strip()


USER_PROMPT_FIGURES = """
Test ID: `{test_name}`

<Test Docstring>
{test_description}
</Test Docstring>

The attached plots show the results of the test.
""".strip()


class DescriptionFuture:
    """This will be immediately returned from generate_description so that
    the tests can continue to be run in parallel while the description is
    retrieved asynchronously.

    The value will be retrieved later and if its not ready yet, it should
    block until it is.
    """

    def __init__(self, future):
        self._future = future

    def get_description(self):
        if isinstance(self._future, str):
            description = self._future
        else:
            # This will block until the future is completed
            description = self._future.result()

        return md_to_html(description, mathml=True)


def generate_description(
    test_id: str,
    test_description: str,
    test_summary: str,
    metric: Union[float, int] = None,
    figures: list = None,
):
    """Generate the description for the test results"""
    if not test_summary and not figures and not metric:
        raise ValueError("No summary or figures provided - cannot generate description")

    # TODO: fix circular import
    from validmind.ai.utils import get_client_and_model

    client, model = get_client_and_model()

    # get last part of test id
    test_name = test_id.split(".")[-1]
    # truncate the test description to save time
    test_description = (
        f"{test_description[:500]}..."
        if len(test_description) > 500
        else test_description
    )

    if metric:
        metric_summary = f"**Metric Value**: {metric}"
        if test_summary:
            test_summary = metric_summary + "\n" + test_summary
        else:
            test_summary = metric_summary

    if test_summary:
        logger.debug(
            f"Generating description for test {test_name} with stringified summary"
        )
        return (
            client.chat.completions.create(
                model=model,
                temperature=0,
                seed=42,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {
                        "role": "user",
                        "content": USER_PROMPT.format(
                            test_name=test_name,
                            test_description=test_description,
                            test_summary=test_summary,
                        ),
                    },
                ],
            )
            .choices[0]
            .message.content.strip()
        )

    logger.debug(
        f"Generating description for test {test_name} with {len(figures)} figures"
    )
    return (
        client.chat.completions.create(
            model=model,
            temperature=0,
            seed=42,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": USER_PROMPT_FIGURES.format(
                                test_name=test_name,
                                test_description=test_description,
                            ),
                        },
                        *[
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": figure._get_b64_url(),
                                },
                            }
                            for figure in figures
                        ],
                    ],
                },
            ],
        )
        .choices[0]
        .message.content.strip()
    )


def background_generate_description(
    test_id: str,
    test_description: str,
    test_summary: str,
    figures: list = None,
    metric: Union[int, float] = None,
):
    def wrapped():
        try:
            return generate_description(
                test_id=test_id,
                test_description=test_description,
                test_summary=test_summary,
                figures=figures,
                metric=metric,
            )
        except Exception as e:
            logger.error(f"Failed to generate description: {e}")

            return test_description

    return DescriptionFuture(__executor.submit(wrapped))


def get_description_metadata(
    test_id,
    default_description,
    summary=None,
    figures=None,
    metric=None,
    prefix="metric_description",
    should_generate=True,
):
    """Get Metadata Dictionary for a Test or Metric Result

    Generates an LLM interpretation of the test results or uses the default
    description and returns a metadata object that can be logged with the test results.

    By default, the description is generated by an LLM that will interpret the test
    results and provide a human-readable description. If the summary or figures are
    not provided, or the `VALIDMIND_LLM_DESCRIPTIONS_ENABLED` environment variable is
    set to `0` or `false` or no LLM has been configured, the default description will
    be used as the test result description.

    Note: Either the summary or figures must be provided to generate the description.

    Args:
        test_id (str): The test ID
        default_description (str): The default description for the test
        summary (Any): The test summary or results to interpret
        figures (List[Figure]): The figures to attach to the test suite result
        metric (Union[int, float]): Unit metrics attached to the test result
        prefix (str): The prefix to use for the content ID (Default: "metric_description")
        should_generate (bool): Whether to generate the description or not (Default: True)

    Returns:
        dict: The metadata object to be logged with the test results
    """
    # Check the feature flag first, then the environment variable
    llm_descriptions_enabled = (
        client_config.can_generate_llm_test_descriptions()
        and os.getenv("VALIDMIND_LLM_DESCRIPTIONS_ENABLED", "1") not in ["0", "false"]
    )

    # TODO: fix circular import
    from validmind.ai.utils import is_configured

    if (
        should_generate
        and (summary or figures)
        and llm_descriptions_enabled
        and is_configured()
    ):
        revision_name = AI_REVISION_NAME

        # get description future and set it as the description in the metadata
        # this will lazily retrieved so it can run in the background in parallel
        description = background_generate_description(
            test_id=test_id,
            test_description=default_description,
            test_summary=summary,
            figures=figures,
            metric=metric,
        )

    else:
        revision_name = DEFAULT_REVISION_NAME
        description = md_to_html(default_description, mathml=True)

    return {
        "content_id": f"{prefix}:{test_id}::{revision_name}",
        "text": description,
    }
